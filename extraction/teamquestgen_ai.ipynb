{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"teamquestgen_ai.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOkO2U5JFbj6bFI/5jbV77b"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"680d970ab65a4072a053d785dbd9ce88":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_84f6b2bedd4a4346a838eb60f826727c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4b14ca24b1604c229ca165e00047c5ca","IPY_MODEL_44f3d6f219e84fe49ab8f3d75f9895b8"]}},"84f6b2bedd4a4346a838eb60f826727c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4b14ca24b1604c229ca165e00047c5ca":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d83f6b66702947749dd34085e937076a","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":791656,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":791656,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bed5bb0c302a48a9b72f00989a4da6e1"}},"44f3d6f219e84fe49ab8f3d75f9895b8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3306082224144cbdaaafcad3b6538f93","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 792k/792k [01:31&lt;00:00, 8.65kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2d2290aadce6421c8ce27b8c05755a2f"}},"d83f6b66702947749dd34085e937076a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"bed5bb0c302a48a9b72f00989a4da6e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3306082224144cbdaaafcad3b6538f93":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2d2290aadce6421c8ce27b8c05755a2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b54427ddd2034f2faec12da6ee9b61ce":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0284b439f5da4611abc51e9bfadef8de","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_fd7445aae3d94622953632e19f134cd9","IPY_MODEL_9d1f52db7e8a4975b3b347c0b22d1281"]}},"0284b439f5da4611abc51e9bfadef8de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fd7445aae3d94622953632e19f134cd9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ffce8ac2ab7b4c778c2f3cfcffd6c93e","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1208,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1208,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_723daa67cc38473cbf0d60e5156b32b7"}},"9d1f52db7e8a4975b3b347c0b22d1281":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2e0477a37acc42a3ac5fe4e3cb5267b3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.21k/1.21k [01:28&lt;00:00, 13.6B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_04a311f6b94e48efbdc1fdfb64160289"}},"ffce8ac2ab7b4c778c2f3cfcffd6c93e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"723daa67cc38473cbf0d60e5156b32b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2e0477a37acc42a3ac5fe4e3cb5267b3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"04a311f6b94e48efbdc1fdfb64160289":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"56b4f51d03a241d8a180c94d8d21a29b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d1338da2191a46168641719a6f63bcb5","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_055042a6551d46c590c9f1d967c11dcc","IPY_MODEL_7a0b73a29fe04225b2f1fb6d691d6b6b"]}},"d1338da2191a46168641719a6f63bcb5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"055042a6551d46c590c9f1d967c11dcc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e41adac70a794ab695038836cd2cd21b","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":891691413,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":891691413,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e4f09997fe924d829748b4432f2ff44b"}},"7a0b73a29fe04225b2f1fb6d691d6b6b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_66b35c87f2934018ab8b8b5d81977b7b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 892M/892M [01:21&lt;00:00, 10.9MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d881c2c34ea64b5e94a1f2501ad16370"}},"e41adac70a794ab695038836cd2cd21b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e4f09997fe924d829748b4432f2ff44b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"66b35c87f2934018ab8b8b5d81977b7b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d881c2c34ea64b5e94a1f2501ad16370":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"79b64b35fb3f42afb3dbd351f09a720d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a0c74ee474854d2abd10e4db76d234dc","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3989abdfc2e64029a63cf0d80bbc1dfe","IPY_MODEL_b816a62596884003a6485cdb8aadfe56"]}},"a0c74ee474854d2abd10e4db76d234dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3989abdfc2e64029a63cf0d80bbc1dfe":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1a12cd45673f44c99c41f3713e3d7447","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1208,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1208,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3a7db845376641c986caf6b254569a4d"}},"b816a62596884003a6485cdb8aadfe56":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_89b17a4cf2514ab080c32bd14a34f98a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.21k/1.21k [00:02&lt;00:00, 598B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2386914382434d899c36365c4c2ca48b"}},"1a12cd45673f44c99c41f3713e3d7447":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3a7db845376641c986caf6b254569a4d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"89b17a4cf2514ab080c32bd14a34f98a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2386914382434d899c36365c4c2ca48b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"faa9bfbd055a4112a68c485e2c4a7597":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8d4ca21ef3f8471183148c1a25892fb7","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c72b2ac6ea3f4b109b39d5f2aa1fc6b6","IPY_MODEL_0f10a4889aea4f60b34d8d66be69f48e"]}},"8d4ca21ef3f8471183148c1a25892fb7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c72b2ac6ea3f4b109b39d5f2aa1fc6b6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5e1620fca9fa486e80eb751712be1890","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":891695056,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":891695056,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_97b280f092d74da89ba4c97b4cb3d084"}},"0f10a4889aea4f60b34d8d66be69f48e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_44ebad76c8ed4377a7f996362db805b1","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 892M/892M [01:17&lt;00:00, 11.4MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b44abc45ac2c4451846ab420cfa53393"}},"5e1620fca9fa486e80eb751712be1890":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"97b280f092d74da89ba4c97b4cb3d084":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"44ebad76c8ed4377a7f996362db805b1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b44abc45ac2c4451846ab420cfa53393":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"__y7VCzCmWyw"},"source":["https://towardsdatascience.com/questgen-an-open-source-nlp-library-for-question-generation-algorithms-1e18067fcdc6"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"jX0CJjM29-Ce","executionInfo":{"status":"ok","timestamp":1628580327977,"user_tz":-330,"elapsed":39786,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}},"outputId":"5d02466d-af5c-4559-8ee2-cd29529eb6d0"},"source":["!pip install git+https://github.com/ramsrigouthamg/Questgen.ai\n","!pip install sense2vec==1.0.2\n","!pip install git+https://github.com/boudinfl/pke.git"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/ramsrigouthamg/Questgen.ai\n","  Cloning https://github.com/ramsrigouthamg/Questgen.ai to /tmp/pip-req-build-mmaxfp7s\n","  Running command git clone -q https://github.com/ramsrigouthamg/Questgen.ai /tmp/pip-req-build-mmaxfp7s\n","Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from Questgen==1.0.0) (1.9.0+cu102)\n","Collecting transformers==3.0.2\n","  Downloading transformers-3.0.2-py3-none-any.whl (769 kB)\n","\u001b[K     |████████████████████████████████| 769 kB 3.7 MB/s \n","\u001b[?25hCollecting pytorch_lightning==0.8.1\n","  Downloading pytorch_lightning-0.8.1-py3-none-any.whl (293 kB)\n","\u001b[K     |████████████████████████████████| 293 kB 39.8 MB/s \n","\u001b[?25hCollecting sense2vec==1.0.3\n","  Downloading sense2vec-1.0.3-py2.py3-none-any.whl (35 kB)\n","Collecting strsim==0.0.3\n","  Downloading strsim-0.0.3-py3-none-any.whl (42 kB)\n","\u001b[K     |████████████████████████████████| 42 kB 826 kB/s \n","\u001b[?25hRequirement already satisfied: six==1.15.0 in /usr/local/lib/python3.7/dist-packages (from Questgen==1.0.0) (1.15.0)\n","Collecting networkx==2.4.0\n","  Downloading networkx-2.4-py3-none-any.whl (1.6 MB)\n","\u001b[K     |████████████████████████████████| 1.6 MB 40.3 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from Questgen==1.0.0) (1.19.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from Questgen==1.0.0) (1.4.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from Questgen==1.0.0) (0.22.2.post1)\n","Collecting unidecode==1.1.1\n","  Downloading Unidecode-1.1.1-py2.py3-none-any.whl (238 kB)\n","\u001b[K     |████████████████████████████████| 238 kB 58.7 MB/s \n","\u001b[?25hCollecting future==0.18.2\n","  Downloading future-0.18.2.tar.gz (829 kB)\n","\u001b[K     |████████████████████████████████| 829 kB 56.3 MB/s \n","\u001b[?25hCollecting joblib==0.14.1\n","  Downloading joblib-0.14.1-py2.py3-none-any.whl (294 kB)\n","\u001b[K     |████████████████████████████████| 294 kB 54.9 MB/s \n","\u001b[?25hRequirement already satisfied: spacy==2.2.4 in /usr/local/lib/python3.7/dist-packages (from Questgen==1.0.0) (2.2.4)\n","Collecting pytz==2020.1\n","  Downloading pytz-2020.1-py2.py3-none-any.whl (510 kB)\n","\u001b[K     |████████████████████████████████| 510 kB 69.5 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil==2.8.1 in /usr/local/lib/python3.7/dist-packages (from Questgen==1.0.0) (2.8.1)\n","Collecting boto3==1.14.40\n","  Downloading boto3-1.14.40-py2.py3-none-any.whl (129 kB)\n","\u001b[K     |████████████████████████████████| 129 kB 53.0 MB/s \n","\u001b[?25hCollecting flashtext==2.7\n","  Downloading flashtext-2.7.tar.gz (14 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from Questgen==1.0.0) (1.1.5)\n","Collecting jmespath<1.0.0,>=0.7.1\n","  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n","Collecting s3transfer<0.4.0,>=0.3.0\n","  Downloading s3transfer-0.3.7-py2.py3-none-any.whl (73 kB)\n","\u001b[K     |████████████████████████████████| 73 kB 1.7 MB/s \n","\u001b[?25hCollecting botocore<1.18.0,>=1.17.40\n","  Downloading botocore-1.17.63-py2.py3-none-any.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 47.0 MB/s \n","\u001b[?25hRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx==2.4.0->Questgen==1.0.0) (4.4.2)\n","Collecting PyYAML>=5.1\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","\u001b[K     |████████████████████████████████| 636 kB 48.3 MB/s \n","\u001b[?25hRequirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==0.8.1->Questgen==1.0.0) (2.5.0)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==0.8.1->Questgen==1.0.0) (4.41.1)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from sense2vec==1.0.3->Questgen==1.0.0) (4.6.1)\n","Requirement already satisfied: catalogue>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from sense2vec==1.0.3->Questgen==1.0.0) (1.0.0)\n","Requirement already satisfied: srsly>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from sense2vec==1.0.3->Questgen==1.0.0) (1.0.5)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from sense2vec==1.0.3->Questgen==1.0.0) (0.8.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4->Questgen==1.0.0) (57.2.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4->Questgen==1.0.0) (2.23.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4->Questgen==1.0.0) (3.0.5)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4->Questgen==1.0.0) (2.0.5)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4->Questgen==1.0.0) (1.1.3)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4->Questgen==1.0.0) (0.4.1)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4->Questgen==1.0.0) (7.4.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4->Questgen==1.0.0) (1.0.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->Questgen==1.0.0) (3.7.4.3)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 31.8 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->Questgen==1.0.0) (3.0.12)\n","Collecting sentencepiece!=0.1.92\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 26.2 MB/s \n","\u001b[?25hCollecting tokenizers==0.8.1.rc1\n","  Downloading tokenizers-0.8.1rc1-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n","\u001b[K     |████████████████████████████████| 3.0 MB 28.2 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->Questgen==1.0.0) (2019.12.20)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2->Questgen==1.0.0) (21.0)\n","Requirement already satisfied: urllib3<1.26,>=1.20 in /usr/local/lib/python3.7/dist-packages (from botocore<1.18.0,>=1.17.40->boto3==1.14.40->Questgen==1.0.0) (1.24.3)\n","Collecting docutils<0.16,>=0.10\n","  Downloading docutils-0.15.2-py3-none-any.whl (547 kB)\n","\u001b[K     |████████████████████████████████| 547 kB 50.2 MB/s \n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->sense2vec==1.0.3->Questgen==1.0.0) (3.5.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4->Questgen==1.0.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4->Questgen==1.0.0) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4->Questgen==1.0.0) (3.0.4)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->Questgen==1.0.0) (0.4.4)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->Questgen==1.0.0) (3.3.4)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->Questgen==1.0.0) (1.8.0)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->Questgen==1.0.0) (1.32.1)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->Questgen==1.0.0) (3.17.3)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->Questgen==1.0.0) (1.0.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->Questgen==1.0.0) (0.36.2)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->Questgen==1.0.0) (0.12.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->Questgen==1.0.0) (0.6.1)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->Questgen==1.0.0) (1.34.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.8.1->Questgen==1.0.0) (4.7.2)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.8.1->Questgen==1.0.0) (4.2.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.8.1->Questgen==1.0.0) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch_lightning==0.8.1->Questgen==1.0.0) (1.3.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.8.1->Questgen==1.0.0) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch_lightning==0.8.1->Questgen==1.0.0) (3.1.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.2->Questgen==1.0.0) (2.4.7)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2->Questgen==1.0.0) (7.1.2)\n","Building wheels for collected packages: Questgen, flashtext, future\n","  Building wheel for Questgen (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for Questgen: filename=Questgen-1.0.0-py3-none-any.whl size=8652 sha256=afa7df947b4af7d8ec00bb777a58b367ec90250ee7b9d6ef48422fc75bf37629\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-p90jgwqg/wheels/40/0a/eb/4da53249b8be090573b5775ae929efbbf2b2b58fc3b9f14cec\n","  Building wheel for flashtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for flashtext: filename=flashtext-2.7-py2.py3-none-any.whl size=9309 sha256=906eb8f94ffcb67e5c48a94dce6ccab9f803d053475101990478e9d03ec9c02a\n","  Stored in directory: /root/.cache/pip/wheels/cb/19/58/4e8fdd0009a7f89dbce3c18fff2e0d0fa201d5cdfd16f113b7\n","  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=8062f8764cc445dfb75ef2e83567b08cfd7c32154605e9adb49ef4a13fb1a5f2\n","  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n","Successfully built Questgen flashtext future\n","Installing collected packages: jmespath, docutils, joblib, botocore, tokenizers, sentencepiece, sacremoses, s3transfer, PyYAML, pytz, future, unidecode, transformers, strsim, sense2vec, pytorch-lightning, networkx, flashtext, boto3, Questgen\n","  Attempting uninstall: docutils\n","    Found existing installation: docutils 0.17.1\n","    Uninstalling docutils-0.17.1:\n","      Successfully uninstalled docutils-0.17.1\n","  Attempting uninstall: joblib\n","    Found existing installation: joblib 1.0.1\n","    Uninstalling joblib-1.0.1:\n","      Successfully uninstalled joblib-1.0.1\n","  Attempting uninstall: PyYAML\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","  Attempting uninstall: pytz\n","    Found existing installation: pytz 2018.9\n","    Uninstalling pytz-2018.9:\n","      Successfully uninstalled pytz-2018.9\n","  Attempting uninstall: future\n","    Found existing installation: future 0.16.0\n","    Uninstalling future-0.16.0:\n","      Successfully uninstalled future-0.16.0\n","  Attempting uninstall: networkx\n","    Found existing installation: networkx 2.5.1\n","    Uninstalling networkx-2.5.1:\n","      Successfully uninstalled networkx-2.5.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Successfully installed PyYAML-5.4.1 Questgen-1.0.0 boto3-1.14.40 botocore-1.17.63 docutils-0.15.2 flashtext-2.7 future-0.18.2 jmespath-0.10.0 joblib-0.14.1 networkx-2.4 pytorch-lightning-0.8.1 pytz-2020.1 s3transfer-0.3.7 sacremoses-0.0.45 sense2vec-1.0.3 sentencepiece-0.1.96 strsim-0.0.3 tokenizers-0.8.1rc1 transformers-3.0.2 unidecode-1.1.1\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pytz"]}}},"metadata":{"tags":[]}},{"output_type":"stream","text":["Collecting sense2vec==1.0.2\n","  Downloading sense2vec-1.0.2.tar.gz (54 kB)\n","\u001b[?25l\r\u001b[K     |██████                          | 10 kB 24.9 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 20 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 30 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 40 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 51 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 54 kB 1.3 MB/s \n","\u001b[?25hRequirement already satisfied: spacy<3.0.0,>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from sense2vec==1.0.2) (2.2.4)\n","Requirement already satisfied: srsly>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from sense2vec==1.0.2) (1.0.5)\n","Requirement already satisfied: catalogue>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from sense2vec==1.0.2) (1.0.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from sense2vec==1.0.2) (0.8.2)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from sense2vec==1.0.2) (1.19.5)\n","Requirement already satisfied: importlib_metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from sense2vec==1.0.2) (4.6.1)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=0.20->sense2vec==1.0.2) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=0.20->sense2vec==1.0.2) (3.5.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.3->sense2vec==1.0.2) (4.41.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.3->sense2vec==1.0.2) (2.23.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.3->sense2vec==1.0.2) (2.0.5)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.3->sense2vec==1.0.2) (0.4.1)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.3->sense2vec==1.0.2) (3.0.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.3->sense2vec==1.0.2) (57.2.0)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.3->sense2vec==1.0.2) (7.4.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.3->sense2vec==1.0.2) (1.0.5)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.3->sense2vec==1.0.2) (1.1.3)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.2.3->sense2vec==1.0.2) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.2.3->sense2vec==1.0.2) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.2.3->sense2vec==1.0.2) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.2.3->sense2vec==1.0.2) (3.0.4)\n","Building wheels for collected packages: sense2vec\n","  Building wheel for sense2vec (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sense2vec: filename=sense2vec-1.0.2-py2.py3-none-any.whl size=35013 sha256=f85d9d48f5a0f6e6e956cdbcafaa8434e5897ffb8c999deba8f81df74ee752c5\n","  Stored in directory: /root/.cache/pip/wheels/cf/d3/93/fe8e871b410c5456a7b06be0f154ad6bab298462471551f39d\n","Successfully built sense2vec\n","Installing collected packages: sense2vec\n","  Attempting uninstall: sense2vec\n","    Found existing installation: sense2vec 1.0.3\n","    Uninstalling sense2vec-1.0.3:\n","      Successfully uninstalled sense2vec-1.0.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","questgen 1.0.0 requires sense2vec==1.0.3, but you have sense2vec 1.0.2 which is incompatible.\u001b[0m\n","Successfully installed sense2vec-1.0.2\n","Collecting git+https://github.com/boudinfl/pke.git\n","  Cloning https://github.com/boudinfl/pke.git to /tmp/pip-req-build-ziz26hxa\n","  Running command git clone -q https://github.com/boudinfl/pke.git /tmp/pip-req-build-ziz26hxa\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from pke==1.8.1) (3.2.5)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from pke==1.8.1) (2.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pke==1.8.1) (1.19.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pke==1.8.1) (1.4.1)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from pke==1.8.1) (2.2.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pke==1.8.1) (1.15.0)\n","Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pke==1.8.1) (0.0)\n","Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (from pke==1.8.1) (1.1.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pke==1.8.1) (0.18.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pke==1.8.1) (0.14.1)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx->pke==1.8.1) (4.4.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->pke==1.8.1) (0.22.2.post1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy->pke==1.8.1) (57.2.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy->pke==1.8.1) (2.23.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->pke==1.8.1) (0.8.2)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->pke==1.8.1) (1.0.5)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->pke==1.8.1) (3.0.5)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->pke==1.8.1) (2.0.5)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->pke==1.8.1) (0.4.1)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->pke==1.8.1) (7.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy->pke==1.8.1) (4.41.1)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy->pke==1.8.1) (1.1.3)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->pke==1.8.1) (1.0.5)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy->pke==1.8.1) (1.0.0)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->pke==1.8.1) (4.6.1)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy->pke==1.8.1) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy->pke==1.8.1) (3.5.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->pke==1.8.1) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->pke==1.8.1) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->pke==1.8.1) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->pke==1.8.1) (1.24.3)\n","Building wheels for collected packages: pke\n","  Building wheel for pke (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pke: filename=pke-1.8.1-py3-none-any.whl size=8763773 sha256=b8d0b0f8572cc68efa6dc3aa19d610af26b3cbb0d7034ad8e65cd196c76d6be0\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-dbjrzyhr/wheels/fa/b3/09/612ee93bf3ee4164bcd5783e742942cdfc892a86039d3e0a33\n","Successfully built pke\n","Installing collected packages: pke\n","Successfully installed pke-1.8.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V1iESdvz-AIK","executionInfo":{"status":"ok","timestamp":1628580337018,"user_tz":-330,"elapsed":9052,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}},"outputId":"d44159d0-170b-4237-cf0e-90bb1533a206"},"source":["!python -m nltk.downloader universal_tagset\n","!python -m spacy download en"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n","  warn(RuntimeWarning(msg))\n","[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n","[nltk_data]   Unzipping taggers/universal_tagset.zip.\n","Collecting en_core_web_sm==2.2.5\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n","\u001b[K     |████████████████████████████████| 12.0 MB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.2.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.6.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_sm')\n","\u001b[38;5;2m✔ Linking successful\u001b[0m\n","/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n","/usr/local/lib/python3.7/dist-packages/spacy/data/en\n","You can now load the model via spacy.load('en')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cS-ZLHMMhyF8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628580365040,"user_tz":-330,"elapsed":28030,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}},"outputId":"5aab4c7f-b0a2-4cc2-9818-62ea9947bc9b"},"source":["!wget https://github.com/explosion/sense2vec/releases/download/v1.0.0/s2v_reddit_2015_md.tar.gz"],"execution_count":3,"outputs":[{"output_type":"stream","text":["--2021-08-10 07:25:36--  https://github.com/explosion/sense2vec/releases/download/v1.0.0/s2v_reddit_2015_md.tar.gz\n","Resolving github.com (github.com)... 52.192.72.89\n","Connecting to github.com (github.com)|52.192.72.89|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://github-releases.githubusercontent.com/50261113/52126080-0993-11ea-8190-8f0e295df22a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210810%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210810T072536Z&X-Amz-Expires=300&X-Amz-Signature=cbf28370ecfe5b13e7f504d7fcd4a65536dbe89a17f6f3ffdef34e6c5fc4198e&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=50261113&response-content-disposition=attachment%3B%20filename%3Ds2v_reddit_2015_md.tar.gz&response-content-type=application%2Foctet-stream [following]\n","--2021-08-10 07:25:36--  https://github-releases.githubusercontent.com/50261113/52126080-0993-11ea-8190-8f0e295df22a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210810%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210810T072536Z&X-Amz-Expires=300&X-Amz-Signature=cbf28370ecfe5b13e7f504d7fcd4a65536dbe89a17f6f3ffdef34e6c5fc4198e&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=50261113&response-content-disposition=attachment%3B%20filename%3Ds2v_reddit_2015_md.tar.gz&response-content-type=application%2Foctet-stream\n","Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.109.154, 185.199.111.154, 185.199.108.154, ...\n","Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.109.154|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 600444501 (573M) [application/octet-stream]\n","Saving to: ‘s2v_reddit_2015_md.tar.gz’\n","\n","s2v_reddit_2015_md. 100%[===================>] 572.63M  23.1MB/s    in 27s     \n","\n","2021-08-10 07:26:04 (21.0 MB/s) - ‘s2v_reddit_2015_md.tar.gz’ saved [600444501/600444501]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"grcz6tkd9wmN","executionInfo":{"status":"ok","timestamp":1628580374297,"user_tz":-330,"elapsed":9261,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}},"outputId":"425f817e-02a6-4daa-f6f9-3b0b7fe88aeb"},"source":["!tar -xvf  s2v_reddit_2015_md.tar.gz"],"execution_count":4,"outputs":[{"output_type":"stream","text":["./._s2v_old\n","./s2v_old/\n","./s2v_old/._freqs.json\n","./s2v_old/freqs.json\n","./s2v_old/._vectors\n","./s2v_old/vectors\n","./s2v_old/._cfg\n","./s2v_old/cfg\n","./s2v_old/._strings.json\n","./s2v_old/strings.json\n","./s2v_old/._key2row\n","./s2v_old/key2row\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hWNtJbTC9t3s","executionInfo":{"status":"ok","timestamp":1628580374298,"user_tz":-330,"elapsed":25,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}},"outputId":"1d9791b7-84cf-47d7-fde6-d69905236205"},"source":["!ls s2v_old"],"execution_count":5,"outputs":[{"output_type":"stream","text":["cfg  freqs.json  key2row  strings.json\tvectors\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"26jVXxbk-_7_","executionInfo":{"status":"ok","timestamp":1628580375746,"user_tz":-330,"elapsed":1457,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}},"outputId":"348d85db-fbba-4b51-a9cf-55bcd2fed154"},"source":["import nltk\n","nltk.download('stopwords')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["680d970ab65a4072a053d785dbd9ce88","84f6b2bedd4a4346a838eb60f826727c","4b14ca24b1604c229ca165e00047c5ca","44f3d6f219e84fe49ab8f3d75f9895b8","d83f6b66702947749dd34085e937076a","bed5bb0c302a48a9b72f00989a4da6e1","3306082224144cbdaaafcad3b6538f93","2d2290aadce6421c8ce27b8c05755a2f","b54427ddd2034f2faec12da6ee9b61ce","0284b439f5da4611abc51e9bfadef8de","fd7445aae3d94622953632e19f134cd9","9d1f52db7e8a4975b3b347c0b22d1281","ffce8ac2ab7b4c778c2f3cfcffd6c93e","723daa67cc38473cbf0d60e5156b32b7","2e0477a37acc42a3ac5fe4e3cb5267b3","04a311f6b94e48efbdc1fdfb64160289","56b4f51d03a241d8a180c94d8d21a29b","d1338da2191a46168641719a6f63bcb5","055042a6551d46c590c9f1d967c11dcc","7a0b73a29fe04225b2f1fb6d691d6b6b","e41adac70a794ab695038836cd2cd21b","e4f09997fe924d829748b4432f2ff44b","66b35c87f2934018ab8b8b5d81977b7b","d881c2c34ea64b5e94a1f2501ad16370"]},"id":"-Si-xY5l-zuK","executionInfo":{"status":"ok","timestamp":1628580495952,"user_tz":-330,"elapsed":120211,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}},"outputId":"f9cfc115-3a6f-4ee9-c643-2d508a893624"},"source":["from pprint import pprint\n","from Questgen import main\n","qe= main.BoolQGen()\n","payload = {\n","            \"input_text\": \"Sachin Ramesh Tendulkar is a former international cricketer from India and a former captain of the Indian national team. He is widely regarded as one of the greatest batsmen in the history of cricket. He is the highest run scorer of all time in International cricket.\"\n","        }\n","output = qe.predict_boolq(payload)\n","pprint (output)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/brown.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading collection 'popular'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cmudict.zip.\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/genesis.zip.\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/inaugural.zip.\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/names.zip.\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Package stopwords is already up-to-date!\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/treebank.zip.\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/omw.zip.\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet.zip.\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/words.zip.\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection popular\n","[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Package brown is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading collection 'popular'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Package cmudict is already up-to-date!\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Package gazetteers is already up-to-date!\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Package genesis is already up-to-date!\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Package gutenberg is already up-to-date!\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Package inaugural is already up-to-date!\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package movie_reviews is already up-to-date!\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Package names is already up-to-date!\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Package shakespeare is already up-to-date!\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Package stopwords is already up-to-date!\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Package treebank is already up-to-date!\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package twitter_samples is already up-to-date!\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    |   Package omw is already up-to-date!\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    |   Package wordnet is already up-to-date!\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Package wordnet_ic is already up-to-date!\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Package words is already up-to-date!\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Package punkt is already up-to-date!\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package snowball_data is already up-to-date!\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n","[nltk_data]    |       to-date!\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection popular\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"680d970ab65a4072a053d785dbd9ce88","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=791656.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b54427ddd2034f2faec12da6ee9b61ce","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1208.0, style=ProgressStyle(description…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"56b4f51d03a241d8a180c94d8d21a29b","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=891691413.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n","To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n","  return torch.floor_divide(self, other)\n"],"name":"stderr"},{"output_type":"stream","text":["{'Boolean Questions': ['Is sachin ramesh tendulkar the highest run scorer in '\n","                       'cricket?',\n","                       'Is sachin ramesh tendulkar the highest run scorer in '\n","                       'cricket?',\n","                       'Is sachin tendulkar the highest run scorer in '\n","                       'cricket?'],\n"," 'Count': 4,\n"," 'Text': 'Sachin Ramesh Tendulkar is a former international cricketer from '\n","         'India and a former captain of the Indian national team. He is widely '\n","         'regarded as one of the greatest batsmen in the history of cricket. '\n","         'He is the highest run scorer of all time in International cricket.'}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["79b64b35fb3f42afb3dbd351f09a720d","a0c74ee474854d2abd10e4db76d234dc","3989abdfc2e64029a63cf0d80bbc1dfe","b816a62596884003a6485cdb8aadfe56","1a12cd45673f44c99c41f3713e3d7447","3a7db845376641c986caf6b254569a4d","89b17a4cf2514ab080c32bd14a34f98a","2386914382434d899c36365c4c2ca48b","faa9bfbd055a4112a68c485e2c4a7597","8d4ca21ef3f8471183148c1a25892fb7","c72b2ac6ea3f4b109b39d5f2aa1fc6b6","0f10a4889aea4f60b34d8d66be69f48e","5e1620fca9fa486e80eb751712be1890","97b280f092d74da89ba4c97b4cb3d084","44ebad76c8ed4377a7f996362db805b1","b44abc45ac2c4451846ab420cfa53393"]},"id":"qid8dIYS_x9c","executionInfo":{"status":"ok","timestamp":1628580605570,"user_tz":-330,"elapsed":109626,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}},"outputId":"fe1d5ff0-ea62-45a8-c6c5-4a92612e5f21"},"source":["qg = main.QGen()\n","output = qg.predict_mcq(payload)\n","pprint (output)"],"execution_count":8,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"79b64b35fb3f42afb3dbd351f09a720d","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1208.0, style=ProgressStyle(description…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"faa9bfbd055a4112a68c485e2c4a7597","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=891695056.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Running model for generation\n"," Sense2vec_distractors successful for word :  cricketer\n"," Sense2vec_distractors successful for word :  india\n"," Sense2vec_distractors successful for word :  batsmen\n","{'questions': [{'answer': 'cricketer',\n","                'context': 'Sachin Ramesh Tendulkar is a former international '\n","                           'cricketer from India and a former captain of the '\n","                           'Indian national team.',\n","                'extra_options': ['Mark Waugh',\n","                                  'Sharma',\n","                                  'Ricky Ponting',\n","                                  'Afridi',\n","                                  'Kohli',\n","                                  'Dhoni'],\n","                'id': 1,\n","                'options': ['Brett Lee', 'Footballer', 'International Cricket'],\n","                'options_algorithm': 'sense2vec',\n","                'question_statement': \"What is Sachin Ramesh Tendulkar's \"\n","                                      'career?',\n","                'question_type': 'MCQ'},\n","               {'answer': 'india',\n","                'context': 'Sachin Ramesh Tendulkar is a former international '\n","                           'cricketer from India and a former captain of the '\n","                           'Indian national team.',\n","                'extra_options': ['Pakistan',\n","                                  'South Korea',\n","                                  'Nepal',\n","                                  'Philippines',\n","                                  'Zimbabwe'],\n","                'id': 2,\n","                'options': ['Bangladesh', 'Indonesia', 'China'],\n","                'options_algorithm': 'sense2vec',\n","                'question_statement': 'Where is Sachin Ramesh Tendulkar from?',\n","                'question_type': 'MCQ'},\n","               {'answer': 'batsmen',\n","                'context': 'He is widely regarded as one of the greatest '\n","                           'batsmen in the history of cricket.',\n","                'extra_options': ['Ashwin', 'Dhoni', 'Afridi', 'Death Overs'],\n","                'id': 3,\n","                'options': ['Bowlers', 'Wickets', 'Mccullum'],\n","                'options_algorithm': 'sense2vec',\n","                'question_statement': 'What is the best cricketer?',\n","                'question_type': 'MCQ'}],\n"," 'statement': 'Sachin Ramesh Tendulkar is a former international cricketer '\n","              'from India and a former captain of the Indian national team. He '\n","              'is widely regarded as one of the greatest batsmen in the '\n","              'history of cricket. He is the highest run scorer of all time in '\n","              'International cricket.',\n"," 'time_taken': 5.6888039112091064}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3vxJgYZs_oK9","executionInfo":{"status":"ok","timestamp":1628580894844,"user_tz":-330,"elapsed":2715,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}},"outputId":"93af499f-a2de-4879-8b82-08052d4d79fd"},"source":["payload['input_text'] = \"\"\"The biject_to() registry is useful for Hamiltonian Monte Carlo, where samples from a probability distribution with constrained .support are propagated in an unconstrained space, and algorithms are typically rotation invariant\"\"\"\n","output = qg.predict_shortq(payload)\n","pprint (output)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Running model for generation\n","{'questions': [{'Question': 'What are rotation invariant?', 'Answer': 'algorithms', 'id': 1, 'context': 'The biject_to() registry is useful for Hamiltonian Monte Carlo, where samples from a probability distribution with constrained .support are propagated in an unconstrained space, and algorithms are typically rotation invariant'}]}\n","{'questions': [{'Answer': 'algorithms',\n","                'Question': 'What are rotation invariant?',\n","                'context': 'The biject_to() registry is useful for Hamiltonian '\n","                           'Monte Carlo, where samples from a probability '\n","                           'distribution with constrained .support are '\n","                           'propagated in an unconstrained space, and '\n","                           'algorithms are typically rotation invariant',\n","                'id': 1}],\n"," 'statement': 'The biject_to() registry is useful for Hamiltonian Monte Carlo, '\n","              'where samples from a probability distribution with constrained '\n","              '.support are propagated in an unconstrained space, and '\n","              'algorithms are typically rotation invariant'}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"onUNT-1MDjaS","executionInfo":{"status":"ok","timestamp":1628580608459,"user_tz":-330,"elapsed":8,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}}},"source":["import pandas as pd\n","import random\n","import re \n","import csv\n","\n","import urllib.request as req\n","import requests\n","from bs4 import BeautifulSoup"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GUvYT0Isenft"},"source":["### get function"]},{"cell_type":"code","metadata":{"id":"g6_fnWW4WHbS","executionInfo":{"status":"ok","timestamp":1628580608460,"user_tz":-330,"elapsed":8,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}}},"source":["def getQuestionsFromPage(url ):\n","\n","  print(url)\n","  html = req.urlopen(url).read()\n","\n","  questndict = {}\n","  soup = BeautifulSoup(html)\n","  print(\"in soup\"), soup.findAll(\"div\")\n","  for pel in soup.findAll('p'):\n","\n","    if len(pel.text) > 0:\n","      print(pel.text)\n","      pyld = {}\n","\n","      pyld[\"input_text\"] = pel.text\n","\n","      output = qg.predict_shortq(pyld)\n","      pprint (output)\n","\n","      if len(output) > 0:\n","        qalist = output[\"questions\"]\n","\n","        for qa in qalist:\n","          questndict[qa[\"Question\"]] = qa[\"Answer\"]\n","\n","\n","    return questndict\n","      \n","\n"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZwQ2uNcJL49T","executionInfo":{"status":"ok","timestamp":1628580608461,"user_tz":-330,"elapsed":9,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}}},"source":["def collatequestions(qstns, qstnsdict):\n","\n","  for qa in qstns:\n","    qstnsdict[qa[\"Question\"]] = qa[\"Answer\"]\n","\n","  return qstnsdict\n","\n"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-3_lDKcvg7Ct","executionInfo":{"status":"ok","timestamp":1628580656039,"user_tz":-330,"elapsed":47586,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}},"outputId":"9a90c9e6-d3aa-43ca-e6a1-94f2ed13716f"},"source":["hdr = ['h1', 'h2' , 'h3' , 'h4']\n","\n","questndict1 = {}\n","dtypeqalist = []\n","\n","#for url in urllist[5:10]:\n","with open('pyt.txt', 'w') as pt:\n","  \n","  url =\"https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like\"\n","  # \"https://pytorch.org/docs/stable/sparse.html\"\n","  #\"https://pytorch.org/docs/stable/tensors.html\" \n","  #\"https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\"\n","  #\"https://pytorch.org/get-started/locally/\"\n","\n","  #html = requests.get(url)\n","  print(url)\n","  html = req.urlopen(url).read()\n","\n","  soup = BeautifulSoup(html)\n","\n","\n","  functns  = soup.findAll('dl')\n","\n","  tables = soup.find_all('table')\n","\n","  for table in tables:\n","    \n","    print(type(table))\n","\n","    trs = table.find_all('tr')\n","\n","    hashdr = False\n","    if trs:\n","      onehdrs = \"\"\n","      for tr in trs:\n","        hdrs = tr.findAll('th')\n","\n","        if not hashdr:\n","          onehdrs = hdrs\n","\n","\n","        if hdrs:\n","          hashdr = True\n","\n","        tds = tr.findAll('td')\n","        #print('tds ',tds)\n","\n","        if tds and hashdr:\n","          print()\n","          for i in range(1):\n","            #print(len(tds) , len(hdrs))\n","            print(i , tds[i].get_text(strip=True) ,onehdrs[i].get_text(strip=True) )\n","            lyout = tds[i+2].get_text(strip=True)\n","            if re.search(r'@',lyout):\n","              lyout = re.sub(r'@',' matrix multiplication to ',lyout)\n","            \n","            if re.search(r'\\*',lyout):\n","              lyout = re.sub(r'\\*',' element-wise multiplication ',lyout)\n","            \n","            if re.search(r'f',lyout):\n","              lyout = re.sub(r'f',' of a scalar to ',lyout)\n","            #'Is M[sparse_coo] matrix multiplication toV[strided]->V[strided] a validLayout signature fortorch.mv()?|yes',\n","            qstn = \"Is \" + lyout + \" a valid \" + onehdrs[i+2].get_text(strip=True) + ' for ' + tds[i].get_text(strip=True) + \"?|yes\"\n","            \n","            dtypeqalist.append(qstn)\n","            #print(\"sels qstn \", qstn)\n","        elif tds and not hashdr:\n","          \n","          #for i in range(len(tds)):\n","          if 1 == 1:\n","\n","            qstn = ''\n","\n","\n","            if re.match(r'Constructs a|Matrix multiplies a|Performs a|Applies a',tds[1].get_text(strip=True)):\n","              qstn = 'Does ' + tds[0].get_text(strip=True) + ' ' + tds[1].get_text(strip=True) + '?|yes'\n","            elif re.match(r'Returns a',tds[1].get_text(strip=True)):\n","              qstn = re.sub(r'Returns a','How to create ',tds[1].get_text(strip=True)) + '?|' + tds[0].get_text(strip=True)\n","            elif re.match(r'IsTrue',tds[1].get_text(strip=True)):\n","              tokn = tds[1].get_text(strip=True).split(',')\n","              qstn = re.sub(r'IsTrue','How to check ',tokn[0]) + '?|use ' + tds[0].get_text(strip=True)\n","            elif re.match(r'Returns the|Returns True|Returnsself',tds[1].get_text(strip=True)):\n","              substrg = \"\"\n","              if re.match(r'Returns the',tds[1].get_text(strip=True)):\n","                substrg = \"Which function returns the \"\n","              elif re.match(r'Returns True',tds[1].get_text(strip=True)):\n","                substrg = \"Which function returns True \"\n","              if re.match(r'Returnsself',tds[1].get_text(strip=True)):\n","                substrg = \"Which function returns self \"\n","\n","              qstn = re.sub(r'Returns the|Returns True|Returnsself',substrg,tds[1].get_text(strip=True)) + '?|' + tds[0].get_text(strip=True)\n","            elif re.match(r'Given a Tensor quantized by linear(affine) quantization',tds[1].get_text(strip=True)):\n","              tokn = tds[1].get_text(strip=True).split(',')\n","              qstn = tokn[0] + ' , Which function ' + tokn[1] + '?|' + tds[0].get_text(strip=True)\n","            elif re.match(r'Given a Tensor quantized by linear (affine) per-channel quantization',tds[1].get_text(strip=True)):\n","              tokn = tds[1].get_text(strip=True).split(',')\n","              qstn = tokn[0] + ' , Which function ' + tokn[1] + '?|' + tds[0].get_text(strip=True)\n","              \n","\n","            if(len(qstn) > 0):\n","              dtypeqalist.append(qstn)\n","            print('method selctn ',qstn)\n","\n","\n","\n","  if functns and len(functns) > 0: \n","\n","\n","    prev = functns[0].findPreviousSibling() \n","    funcname = \"\"\n","    if prev :\n","      funcname = prev.get_text().strip()\n","    for func in functns:\n","      funcdftn = funcname + '\\n'\n","      #print( '1',funcdftn)\n","      dt = func.find('dt')\n","      #print('2',dt)\n","      if dt :\n","        funcdftn = funcdftn + '\\n' + dt.text\n","        #print('3' ,funcdftn)\n","\n","\n","      p = func.find_all(lambda tag: tag.name == \"p\" ) #and not (tag.find(\"script\")))\n","      if p:\n","        funcdftn = funcdftn + ' ' + funcname + '\\n'\n","        for content in p:\n","          if (len(content.get_text(strip=True))):\n","            dfn = content.get_text(strip=True)\n","            oneline = funcname + ' ' + dfn + '\\n'\n","            funcdftn =  ' ' + funcdftn + dfn + '\\n'\n","            pyld1 = {}\n","\n","            pyld1[\"input_text\"] = oneline.strip()\n","\n","            output1 = qg.predict_shortq(pyld1)\n","            pprint (output)\n","\n","        funcdftn = funcdftn.strip()\n","        print('4',funcdftn)\n","      pyld = {}\n","\n","      pyld[\"input_text\"] = funcdftn\n","\n","      output = qg.predict_shortq(pyld)\n","      pprint (output)\n","\n","      if len(output) > 0:\n","        qalist = output[\"questions\"]\n","        questndict1 = collatequestions(qalist,questndict1)\n","\n","  else:\n","    i = 0\n","    for pel in soup.findAll('p'):\n","      \"\"\"prv = pel.findPreviousSibling() \n","      if prv != None:\n","        if (prv.name in hdr):\n","          print(prv , pel)\n","          pt.write(prv.text + '\\n' + pel.text + '\\n')\n","        elif len(pel.text) > 0:\n","          print(pel)\n","          pt.write(pel.text + '\\n')\"\"\"\n","\n","      if len(pel.text) > 0 :\n","        pyld = {}\n","\n","        pyld[\"input_text\"] = pel.text\n","\n","        output = qg.predict_shortq(pyld)\n","        pprint (output)\n","\n","        if len(output) > 0:\n","          qalist = output[\"questions\"]\n","          questndict1 = collatequestions(qalist,questndict1)\n","\n","\n","\n","#print (questndict1)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like\n","Running model for generation\n","{'questions': [{'Question': 'What is the same size as inputfilled withfill_value?', 'Answer': 'tensor', 'id': 1, 'context': 'torch.full_like¶ Returns a tensor with the same size asinputfilled withfill_value.torch.full_like(input,fill_value)is equivalent totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device).'}]}\n","{'questions': [{'Answer': 'cricketer',\n","                'Question': \"What is Sachin Ramesh Tendulkar's career?\",\n","                'context': 'Sachin Ramesh Tendulkar is a former international '\n","                           'cricketer from India and a former captain of the '\n","                           'Indian national team.',\n","                'id': 1},\n","               {'Answer': 'india',\n","                'Question': 'Where is Sachin Ramesh Tendulkar from?',\n","                'context': 'Sachin Ramesh Tendulkar is a former international '\n","                           'cricketer from India and a former captain of the '\n","                           'Indian national team.',\n","                'id': 2},\n","               {'Answer': 'batsmen',\n","                'Question': 'What is the best cricketer?',\n","                'context': 'He is widely regarded as one of the greatest '\n","                           'batsmen in the history of cricket.',\n","                'id': 3}],\n"," 'statement': 'Sachin Ramesh Tendulkar is a former international cricketer '\n","              'from India and a former captain of the Indian national team. He '\n","              'is widely regarded as one of the greatest batsmen in the '\n","              'history of cricket. He is the highest run scorer of all time in '\n","              'International cricket.'}\n","ZERO\n","{'questions': [{'Answer': 'cricketer',\n","                'Question': \"What is Sachin Ramesh Tendulkar's career?\",\n","                'context': 'Sachin Ramesh Tendulkar is a former international '\n","                           'cricketer from India and a former captain of the '\n","                           'Indian national team.',\n","                'id': 1},\n","               {'Answer': 'india',\n","                'Question': 'Where is Sachin Ramesh Tendulkar from?',\n","                'context': 'Sachin Ramesh Tendulkar is a former international '\n","                           'cricketer from India and a former captain of the '\n","                           'Indian national team.',\n","                'id': 2},\n","               {'Answer': 'batsmen',\n","                'Question': 'What is the best cricketer?',\n","                'context': 'He is widely regarded as one of the greatest '\n","                           'batsmen in the history of cricket.',\n","                'id': 3}],\n"," 'statement': 'Sachin Ramesh Tendulkar is a former international cricketer '\n","              'from India and a former captain of the Indian national team. He '\n","              'is widely regarded as one of the greatest batsmen in the '\n","              'history of cricket. He is the highest run scorer of all time in '\n","              'International cricket.'}\n","Running model for generation\n","{'questions': [{'Question': 'What is fill_value?', 'Answer': 'number', 'id': 1, 'context': 'torch.full_like¶ fill_value– the number to fill the output tensor with.'}]}\n","{'questions': [{'Answer': 'cricketer',\n","                'Question': \"What is Sachin Ramesh Tendulkar's career?\",\n","                'context': 'Sachin Ramesh Tendulkar is a former international '\n","                           'cricketer from India and a former captain of the '\n","                           'Indian national team.',\n","                'id': 1},\n","               {'Answer': 'india',\n","                'Question': 'Where is Sachin Ramesh Tendulkar from?',\n","                'context': 'Sachin Ramesh Tendulkar is a former international '\n","                           'cricketer from India and a former captain of the '\n","                           'Indian national team.',\n","                'id': 2},\n","               {'Answer': 'batsmen',\n","                'Question': 'What is the best cricketer?',\n","                'context': 'He is widely regarded as one of the greatest '\n","                           'batsmen in the history of cricket.',\n","                'id': 3}],\n"," 'statement': 'Sachin Ramesh Tendulkar is a former international cricketer '\n","              'from India and a former captain of the Indian national team. He '\n","              'is widely regarded as one of the greatest batsmen in the '\n","              'history of cricket. He is the highest run scorer of all time in '\n","              'International cricket.'}\n","Running model for generation\n","{'questions': [{'Question': 'What is the data type of the returned tensor?', 'Answer': 'tensor', 'id': 1, 'context': 'torch.full_like¶ dtype(torch.dtype, optional) – the desired data type of returned Tensor.'}, {'Question': 'What is the desired data type of the returned Tensor?', 'Answer': 'data type', 'id': 2, 'context': 'torch.full_like¶ dtype(torch.dtype, optional) – the desired data type of returned Tensor.'}, {'Question': 'What is the default value of ifNone?', 'Answer': 'default', 'id': 3, 'context': 'Default: ifNone, defaults to the dtype ofinput.'}]}\n","{'questions': [{'Answer': 'cricketer',\n","                'Question': \"What is Sachin Ramesh Tendulkar's career?\",\n","                'context': 'Sachin Ramesh Tendulkar is a former international '\n","                           'cricketer from India and a former captain of the '\n","                           'Indian national team.',\n","                'id': 1},\n","               {'Answer': 'india',\n","                'Question': 'Where is Sachin Ramesh Tendulkar from?',\n","                'context': 'Sachin Ramesh Tendulkar is a former international '\n","                           'cricketer from India and a former captain of the '\n","                           'Indian national team.',\n","                'id': 2},\n","               {'Answer': 'batsmen',\n","                'Question': 'What is the best cricketer?',\n","                'context': 'He is widely regarded as one of the greatest '\n","                           'batsmen in the history of cricket.',\n","                'id': 3}],\n"," 'statement': 'Sachin Ramesh Tendulkar is a former international cricketer '\n","              'from India and a former captain of the Indian national team. He '\n","              'is widely regarded as one of the greatest batsmen in the '\n","              'history of cricket. He is the highest run scorer of all time in '\n","              'International cricket.'}\n","Running model for generation\n","{'questions': [{'Question': 'What is the desired layout of the returned tensor?', 'Answer': 'tensor', 'id': 1, 'context': 'torch.full_like¶ layout(torch.layout, optional) – the desired layout of returned tensor.'}, {'Question': 'What is the default layout of input?', 'Answer': 'default', 'id': 2, 'context': 'Default: ifNone, defaults to the layout ofinput.'}]}\n","{'questions': [{'Answer': 'cricketer',\n","                'Question': \"What is Sachin Ramesh Tendulkar's career?\",\n","                'context': 'Sachin Ramesh Tendulkar is a former international '\n","                           'cricketer from India and a former captain of the '\n","                           'Indian national team.',\n","                'id': 1},\n","               {'Answer': 'india',\n","                'Question': 'Where is Sachin Ramesh Tendulkar from?',\n","                'context': 'Sachin Ramesh Tendulkar is a former international '\n","                           'cricketer from India and a former captain of the '\n","                           'Indian national team.',\n","                'id': 2},\n","               {'Answer': 'batsmen',\n","                'Question': 'What is the best cricketer?',\n","                'context': 'He is widely regarded as one of the greatest '\n","                           'batsmen in the history of cricket.',\n","                'id': 3}],\n"," 'statement': 'Sachin Ramesh Tendulkar is a former international cricketer '\n","              'from India and a former captain of the Indian national team. He '\n","              'is widely regarded as one of the greatest batsmen in the '\n","              'history of cricket. He is the highest run scorer of all time in '\n","              'International cricket.'}\n","Running model for generation\n","{'questions': [{'Question': 'What is the desired device of the returned tensor?', 'Answer': 'tensor', 'id': 1, 'context': 'torch.full_like¶ device(torch.device, optional) – the desired device of returned tensor.'}, {'Question': 'What is the default value of ifNone?', 'Answer': 'default', 'id': 2, 'context': 'Default: ifNone, defaults to the device ofinput.'}]}\n","{'questions': [{'Answer': 'cricketer',\n","                'Question': \"What is Sachin Ramesh Tendulkar's career?\",\n","                'context': 'Sachin Ramesh Tendulkar is a former international '\n","                           'cricketer from India and a former captain of the '\n","                           'Indian national team.',\n","                'id': 1},\n","               {'Answer': 'india',\n","                'Question': 'Where is Sachin Ramesh Tendulkar from?',\n","                'context': 'Sachin Ramesh Tendulkar is a former international '\n","                           'cricketer from India and a former captain of the '\n","                           'Indian national team.',\n","                'id': 2},\n","               {'Answer': 'batsmen',\n","                'Question': 'What is the best cricketer?',\n","                'context': 'He is widely regarded as one of the greatest '\n","                           'batsmen in the history of cricket.',\n","                'id': 3}],\n"," 'statement': 'Sachin Ramesh Tendulkar is a former international cricketer '\n","              'from India and a former captain of the Indian national team. He '\n","              'is widely regarded as one of the greatest batsmen in the '\n","              'history of cricket. He is the highest run scorer of all time in '\n","              'International cricket.'}\n","Running model for generation\n","{'questions': [{'Question': 'What is the return value of a tensor?', 'Answer': 'tensor', 'id': 1, 'context': 'torch.full_like¶ requires_grad(bool,optional) – If autograd should record operations on the\\nreturned tensor.'}]}\n","{'questions': [{'Answer': 'cricketer',\n","                'Question': \"What is Sachin Ramesh Tendulkar's career?\",\n","                'context': 'Sachin Ramesh Tendulkar is a former international '\n","                           'cricketer from India and a former captain of the '\n","                           'Indian national team.',\n","                'id': 1},\n","               {'Answer': 'india',\n","                'Question': 'Where is Sachin Ramesh Tendulkar from?',\n","                'context': 'Sachin Ramesh Tendulkar is a former international '\n","                           'cricketer from India and a former captain of the '\n","                           'Indian national team.',\n","                'id': 2},\n","               {'Answer': 'batsmen',\n","                'Question': 'What is the best cricketer?',\n","                'context': 'He is widely regarded as one of the greatest '\n","                           'batsmen in the history of cricket.',\n","                'id': 3}],\n"," 'statement': 'Sachin Ramesh Tendulkar is a former international cricketer '\n","              'from India and a former captain of the Indian national team. He '\n","              'is widely regarded as one of the greatest batsmen in the '\n","              'history of cricket. He is the highest run scorer of all time in '\n","              'International cricket.'}\n","Running model for generation\n","{'questions': [{'Question': 'What is the memory format of the returned tensor?', 'Answer': 'tensor', 'id': 1, 'context': 'torch.full_like¶ memory_format(torch.memory_format, optional) – the desired memory format of\\nreturned Tensor.'}, {'Question': 'What is the default format for Torch.preserve_format?', 'Answer': 'default', 'id': 2, 'context': 'Default:torch.preserve_format.'}]}\n","{'questions': [{'Answer': 'cricketer',\n","                'Question': \"What is Sachin Ramesh Tendulkar's career?\",\n","                'context': 'Sachin Ramesh Tendulkar is a former international '\n","                           'cricketer from India and a former captain of the '\n","                           'Indian national team.',\n","                'id': 1},\n","               {'Answer': 'india',\n","                'Question': 'Where is Sachin Ramesh Tendulkar from?',\n","                'context': 'Sachin Ramesh Tendulkar is a former international '\n","                           'cricketer from India and a former captain of the '\n","                           'Indian national team.',\n","                'id': 2},\n","               {'Answer': 'batsmen',\n","                'Question': 'What is the best cricketer?',\n","                'context': 'He is widely regarded as one of the greatest '\n","                           'batsmen in the history of cricket.',\n","                'id': 3}],\n"," 'statement': 'Sachin Ramesh Tendulkar is a former international cricketer '\n","              'from India and a former captain of the Indian national team. He '\n","              'is widely regarded as one of the greatest batsmen in the '\n","              'history of cricket. He is the highest run scorer of all time in '\n","              'International cricket.'}\n","4 torch.full_like¶\n","\n","\n","torch.full_like(input, fill_value, *, dtype=None, layout=torch.strided, device=None, requires_grad=False, memory_format=torch.preserve_format) → Tensor¶ torch.full_like¶\n","Returns a tensor with the same size asinputfilled withfill_value.torch.full_like(input,fill_value)is equivalent totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device).\n","input(Tensor) – the size ofinputwill determine size of the output tensor.\n","fill_value– the number to fill the output tensor with.\n","dtype(torch.dtype, optional) – the desired data type of returned Tensor.\n","Default: ifNone, defaults to the dtype ofinput.\n","layout(torch.layout, optional) – the desired layout of returned tensor.\n","Default: ifNone, defaults to the layout ofinput.\n","device(torch.device, optional) – the desired device of returned tensor.\n","Default: ifNone, defaults to the device ofinput.\n","requires_grad(bool,optional) – If autograd should record operations on the\n","returned tensor. Default:False.\n","memory_format(torch.memory_format, optional) – the desired memory format of\n","returned Tensor. Default:torch.preserve_format.\n","Running model for generation\n","{'questions': [{'Question': 'What is the same size as inputfilled withfill_value?', 'Answer': 'tensor', 'id': 1, 'context': 'torch.full_like¶\\n\\n\\ntorch.full_like(input, fill_value, *, dtype=None, layout=torch.strided, device=None, requires_grad=False, memory_format=torch.preserve_format) → Tensor¶ torch.full_like¶\\nReturns a tensor with the same size asinputfilled withfill_value.torch.full_like(input,fill_value)is equivalent totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). torch.full_like¶\\n\\n\\ntorch.full_like(input, fill_value, *, dtype=None, layout=torch.strided, device=None, requires_grad=False, memory_format=torch.preserve_format) → Tensor¶ torch.full_like¶\\nReturns a tensor with the same size asinputfilled withfill_value.torch.full_like(input,fill_value)is equivalent totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). memory_format(torch.memory_format, optional) – the desired memory format of\\nreturned Tensor.'}, {'Question': 'What is the default layout ofinput?', 'Answer': 'default', 'id': 2, 'context': 'Default: ifNone, defaults to the layout ofinput. Default: ifNone, defaults to the device ofinput. Default: ifNone, defaults to the dtype ofinput.'}, {'Question': 'What is the default memory format of a returned tensor?', 'Answer': 'optional', 'id': 3, 'context': 'memory_format(torch.memory_format, optional) – the desired memory format of\\nreturned Tensor. requires_grad(bool,optional) – If autograd should record operations on the\\nreturned tensor. dtype(torch.dtype, optional) – the desired data type of returned Tensor.'}]}\n","{'questions': [{'Answer': 'tensor',\n","                'Question': 'What is the same size as inputfilled '\n","                            'withfill_value?',\n","                'context': 'torch.full_like¶\\n'\n","                           '\\n'\n","                           '\\n'\n","                           'torch.full_like(input, fill_value, *, dtype=None, '\n","                           'layout=torch.strided, device=None, '\n","                           'requires_grad=False, '\n","                           'memory_format=torch.preserve_format) → Tensor¶ '\n","                           'torch.full_like¶\\n'\n","                           'Returns a tensor with the same size asinputfilled '\n","                           'withfill_value.torch.full_like(input,fill_value)is '\n","                           'equivalent '\n","                           'totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). '\n","                           'torch.full_like¶\\n'\n","                           '\\n'\n","                           '\\n'\n","                           'torch.full_like(input, fill_value, *, dtype=None, '\n","                           'layout=torch.strided, device=None, '\n","                           'requires_grad=False, '\n","                           'memory_format=torch.preserve_format) → Tensor¶ '\n","                           'torch.full_like¶\\n'\n","                           'Returns a tensor with the same size asinputfilled '\n","                           'withfill_value.torch.full_like(input,fill_value)is '\n","                           'equivalent '\n","                           'totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). '\n","                           'memory_format(torch.memory_format, optional) – the '\n","                           'desired memory format of\\n'\n","                           'returned Tensor.',\n","                'id': 1},\n","               {'Answer': 'default',\n","                'Question': 'What is the default layout ofinput?',\n","                'context': 'Default: ifNone, defaults to the layout ofinput. '\n","                           'Default: ifNone, defaults to the device ofinput. '\n","                           'Default: ifNone, defaults to the dtype ofinput.',\n","                'id': 2},\n","               {'Answer': 'optional',\n","                'Question': 'What is the default memory format of a returned '\n","                            'tensor?',\n","                'context': 'memory_format(torch.memory_format, optional) – the '\n","                           'desired memory format of\\n'\n","                           'returned Tensor. requires_grad(bool,optional) – If '\n","                           'autograd should record operations on the\\n'\n","                           'returned tensor. dtype(torch.dtype, optional) – '\n","                           'the desired data type of returned Tensor.',\n","                'id': 3}],\n"," 'statement': 'torch.full_like¶\\n'\n","              '\\n'\n","              '\\n'\n","              'torch.full_like(input, fill_value, *, dtype=None, '\n","              'layout=torch.strided, device=None, requires_grad=False, '\n","              'memory_format=torch.preserve_format) → Tensor¶ '\n","              'torch.full_like¶\\n'\n","              'Returns a tensor with the same size asinputfilled '\n","              'withfill_value.torch.full_like(input,fill_value)is equivalent '\n","              'totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). '\n","              'input(Tensor) – the size ofinputwill determine size of the '\n","              'output tensor. fill_value– the number to fill the output tensor '\n","              'with. dtype(torch.dtype, optional) – the desired data type of '\n","              'returned Tensor. Default: ifNone, defaults to the dtype '\n","              'ofinput. layout(torch.layout, optional) – the desired layout of '\n","              'returned tensor. Default: ifNone, defaults to the layout '\n","              'ofinput. device(torch.device, optional) – the desired device of '\n","              'returned tensor. Default: ifNone, defaults to the device '\n","              'ofinput. requires_grad(bool,optional) – If autograd should '\n","              'record operations on the\\n'\n","              'returned tensor. memory_format(torch.memory_format, optional) – '\n","              'the desired memory format of\\n'\n","              'returned Tensor. Default:torch.preserve_format.'}\n","ZERO\n","{'questions': [{'Answer': 'tensor',\n","                'Question': 'What is the same size as inputfilled '\n","                            'withfill_value?',\n","                'context': 'torch.full_like¶\\n'\n","                           '\\n'\n","                           '\\n'\n","                           'torch.full_like(input, fill_value, *, dtype=None, '\n","                           'layout=torch.strided, device=None, '\n","                           'requires_grad=False, '\n","                           'memory_format=torch.preserve_format) → Tensor¶ '\n","                           'torch.full_like¶\\n'\n","                           'Returns a tensor with the same size asinputfilled '\n","                           'withfill_value.torch.full_like(input,fill_value)is '\n","                           'equivalent '\n","                           'totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). '\n","                           'torch.full_like¶\\n'\n","                           '\\n'\n","                           '\\n'\n","                           'torch.full_like(input, fill_value, *, dtype=None, '\n","                           'layout=torch.strided, device=None, '\n","                           'requires_grad=False, '\n","                           'memory_format=torch.preserve_format) → Tensor¶ '\n","                           'torch.full_like¶\\n'\n","                           'Returns a tensor with the same size asinputfilled '\n","                           'withfill_value.torch.full_like(input,fill_value)is '\n","                           'equivalent '\n","                           'totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). '\n","                           'memory_format(torch.memory_format, optional) – the '\n","                           'desired memory format of\\n'\n","                           'returned Tensor.',\n","                'id': 1},\n","               {'Answer': 'default',\n","                'Question': 'What is the default layout ofinput?',\n","                'context': 'Default: ifNone, defaults to the layout ofinput. '\n","                           'Default: ifNone, defaults to the device ofinput. '\n","                           'Default: ifNone, defaults to the dtype ofinput.',\n","                'id': 2},\n","               {'Answer': 'optional',\n","                'Question': 'What is the default memory format of a returned '\n","                            'tensor?',\n","                'context': 'memory_format(torch.memory_format, optional) – the '\n","                           'desired memory format of\\n'\n","                           'returned Tensor. requires_grad(bool,optional) – If '\n","                           'autograd should record operations on the\\n'\n","                           'returned tensor. dtype(torch.dtype, optional) – '\n","                           'the desired data type of returned Tensor.',\n","                'id': 3}],\n"," 'statement': 'torch.full_like¶\\n'\n","              '\\n'\n","              '\\n'\n","              'torch.full_like(input, fill_value, *, dtype=None, '\n","              'layout=torch.strided, device=None, requires_grad=False, '\n","              'memory_format=torch.preserve_format) → Tensor¶ '\n","              'torch.full_like¶\\n'\n","              'Returns a tensor with the same size asinputfilled '\n","              'withfill_value.torch.full_like(input,fill_value)is equivalent '\n","              'totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). '\n","              'input(Tensor) – the size ofinputwill determine size of the '\n","              'output tensor. fill_value– the number to fill the output tensor '\n","              'with. dtype(torch.dtype, optional) – the desired data type of '\n","              'returned Tensor. Default: ifNone, defaults to the dtype '\n","              'ofinput. layout(torch.layout, optional) – the desired layout of '\n","              'returned tensor. Default: ifNone, defaults to the layout '\n","              'ofinput. device(torch.device, optional) – the desired device of '\n","              'returned tensor. Default: ifNone, defaults to the device '\n","              'ofinput. requires_grad(bool,optional) – If autograd should '\n","              'record operations on the\\n'\n","              'returned tensor. memory_format(torch.memory_format, optional) – '\n","              'the desired memory format of\\n'\n","              'returned Tensor. Default:torch.preserve_format.'}\n","Running model for generation\n","{'questions': [{'Question': 'What is fill_value?', 'Answer': 'number', 'id': 1, 'context': 'torch.full_like¶ fill_value– the number to fill the output tensor with.'}]}\n","{'questions': [{'Answer': 'tensor',\n","                'Question': 'What is the same size as inputfilled '\n","                            'withfill_value?',\n","                'context': 'torch.full_like¶\\n'\n","                           '\\n'\n","                           '\\n'\n","                           'torch.full_like(input, fill_value, *, dtype=None, '\n","                           'layout=torch.strided, device=None, '\n","                           'requires_grad=False, '\n","                           'memory_format=torch.preserve_format) → Tensor¶ '\n","                           'torch.full_like¶\\n'\n","                           'Returns a tensor with the same size asinputfilled '\n","                           'withfill_value.torch.full_like(input,fill_value)is '\n","                           'equivalent '\n","                           'totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). '\n","                           'torch.full_like¶\\n'\n","                           '\\n'\n","                           '\\n'\n","                           'torch.full_like(input, fill_value, *, dtype=None, '\n","                           'layout=torch.strided, device=None, '\n","                           'requires_grad=False, '\n","                           'memory_format=torch.preserve_format) → Tensor¶ '\n","                           'torch.full_like¶\\n'\n","                           'Returns a tensor with the same size asinputfilled '\n","                           'withfill_value.torch.full_like(input,fill_value)is '\n","                           'equivalent '\n","                           'totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). '\n","                           'memory_format(torch.memory_format, optional) – the '\n","                           'desired memory format of\\n'\n","                           'returned Tensor.',\n","                'id': 1},\n","               {'Answer': 'default',\n","                'Question': 'What is the default layout ofinput?',\n","                'context': 'Default: ifNone, defaults to the layout ofinput. '\n","                           'Default: ifNone, defaults to the device ofinput. '\n","                           'Default: ifNone, defaults to the dtype ofinput.',\n","                'id': 2},\n","               {'Answer': 'optional',\n","                'Question': 'What is the default memory format of a returned '\n","                            'tensor?',\n","                'context': 'memory_format(torch.memory_format, optional) – the '\n","                           'desired memory format of\\n'\n","                           'returned Tensor. requires_grad(bool,optional) – If '\n","                           'autograd should record operations on the\\n'\n","                           'returned tensor. dtype(torch.dtype, optional) – '\n","                           'the desired data type of returned Tensor.',\n","                'id': 3}],\n"," 'statement': 'torch.full_like¶\\n'\n","              '\\n'\n","              '\\n'\n","              'torch.full_like(input, fill_value, *, dtype=None, '\n","              'layout=torch.strided, device=None, requires_grad=False, '\n","              'memory_format=torch.preserve_format) → Tensor¶ '\n","              'torch.full_like¶\\n'\n","              'Returns a tensor with the same size asinputfilled '\n","              'withfill_value.torch.full_like(input,fill_value)is equivalent '\n","              'totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). '\n","              'input(Tensor) – the size ofinputwill determine size of the '\n","              'output tensor. fill_value– the number to fill the output tensor '\n","              'with. dtype(torch.dtype, optional) – the desired data type of '\n","              'returned Tensor. Default: ifNone, defaults to the dtype '\n","              'ofinput. layout(torch.layout, optional) – the desired layout of '\n","              'returned tensor. Default: ifNone, defaults to the layout '\n","              'ofinput. device(torch.device, optional) – the desired device of '\n","              'returned tensor. Default: ifNone, defaults to the device '\n","              'ofinput. requires_grad(bool,optional) – If autograd should '\n","              'record operations on the\\n'\n","              'returned tensor. memory_format(torch.memory_format, optional) – '\n","              'the desired memory format of\\n'\n","              'returned Tensor. Default:torch.preserve_format.'}\n","Running model for generation\n","{'questions': [{'Question': 'What is the data type of the returned tensor?', 'Answer': 'tensor', 'id': 1, 'context': 'torch.full_like¶ dtype(torch.dtype, optional) – the desired data type of returned Tensor.'}, {'Question': 'What is the desired data type of the returned Tensor?', 'Answer': 'data type', 'id': 2, 'context': 'torch.full_like¶ dtype(torch.dtype, optional) – the desired data type of returned Tensor.'}, {'Question': 'What is the default value of ifNone?', 'Answer': 'default', 'id': 3, 'context': 'Default: ifNone, defaults to the dtype ofinput.'}]}\n","{'questions': [{'Answer': 'tensor',\n","                'Question': 'What is the same size as inputfilled '\n","                            'withfill_value?',\n","                'context': 'torch.full_like¶\\n'\n","                           '\\n'\n","                           '\\n'\n","                           'torch.full_like(input, fill_value, *, dtype=None, '\n","                           'layout=torch.strided, device=None, '\n","                           'requires_grad=False, '\n","                           'memory_format=torch.preserve_format) → Tensor¶ '\n","                           'torch.full_like¶\\n'\n","                           'Returns a tensor with the same size asinputfilled '\n","                           'withfill_value.torch.full_like(input,fill_value)is '\n","                           'equivalent '\n","                           'totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). '\n","                           'torch.full_like¶\\n'\n","                           '\\n'\n","                           '\\n'\n","                           'torch.full_like(input, fill_value, *, dtype=None, '\n","                           'layout=torch.strided, device=None, '\n","                           'requires_grad=False, '\n","                           'memory_format=torch.preserve_format) → Tensor¶ '\n","                           'torch.full_like¶\\n'\n","                           'Returns a tensor with the same size asinputfilled '\n","                           'withfill_value.torch.full_like(input,fill_value)is '\n","                           'equivalent '\n","                           'totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). '\n","                           'memory_format(torch.memory_format, optional) – the '\n","                           'desired memory format of\\n'\n","                           'returned Tensor.',\n","                'id': 1},\n","               {'Answer': 'default',\n","                'Question': 'What is the default layout ofinput?',\n","                'context': 'Default: ifNone, defaults to the layout ofinput. '\n","                           'Default: ifNone, defaults to the device ofinput. '\n","                           'Default: ifNone, defaults to the dtype ofinput.',\n","                'id': 2},\n","               {'Answer': 'optional',\n","                'Question': 'What is the default memory format of a returned '\n","                            'tensor?',\n","                'context': 'memory_format(torch.memory_format, optional) – the '\n","                           'desired memory format of\\n'\n","                           'returned Tensor. requires_grad(bool,optional) – If '\n","                           'autograd should record operations on the\\n'\n","                           'returned tensor. dtype(torch.dtype, optional) – '\n","                           'the desired data type of returned Tensor.',\n","                'id': 3}],\n"," 'statement': 'torch.full_like¶\\n'\n","              '\\n'\n","              '\\n'\n","              'torch.full_like(input, fill_value, *, dtype=None, '\n","              'layout=torch.strided, device=None, requires_grad=False, '\n","              'memory_format=torch.preserve_format) → Tensor¶ '\n","              'torch.full_like¶\\n'\n","              'Returns a tensor with the same size asinputfilled '\n","              'withfill_value.torch.full_like(input,fill_value)is equivalent '\n","              'totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). '\n","              'input(Tensor) – the size ofinputwill determine size of the '\n","              'output tensor. fill_value– the number to fill the output tensor '\n","              'with. dtype(torch.dtype, optional) – the desired data type of '\n","              'returned Tensor. Default: ifNone, defaults to the dtype '\n","              'ofinput. layout(torch.layout, optional) – the desired layout of '\n","              'returned tensor. Default: ifNone, defaults to the layout '\n","              'ofinput. device(torch.device, optional) – the desired device of '\n","              'returned tensor. Default: ifNone, defaults to the device '\n","              'ofinput. requires_grad(bool,optional) – If autograd should '\n","              'record operations on the\\n'\n","              'returned tensor. memory_format(torch.memory_format, optional) – '\n","              'the desired memory format of\\n'\n","              'returned Tensor. Default:torch.preserve_format.'}\n","Running model for generation\n","{'questions': [{'Question': 'What is the desired layout of the returned tensor?', 'Answer': 'tensor', 'id': 1, 'context': 'torch.full_like¶ layout(torch.layout, optional) – the desired layout of returned tensor.'}, {'Question': 'What is the default layout of input?', 'Answer': 'default', 'id': 2, 'context': 'Default: ifNone, defaults to the layout ofinput.'}]}\n","{'questions': [{'Answer': 'tensor',\n","                'Question': 'What is the same size as inputfilled '\n","                            'withfill_value?',\n","                'context': 'torch.full_like¶\\n'\n","                           '\\n'\n","                           '\\n'\n","                           'torch.full_like(input, fill_value, *, dtype=None, '\n","                           'layout=torch.strided, device=None, '\n","                           'requires_grad=False, '\n","                           'memory_format=torch.preserve_format) → Tensor¶ '\n","                           'torch.full_like¶\\n'\n","                           'Returns a tensor with the same size asinputfilled '\n","                           'withfill_value.torch.full_like(input,fill_value)is '\n","                           'equivalent '\n","                           'totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). '\n","                           'torch.full_like¶\\n'\n","                           '\\n'\n","                           '\\n'\n","                           'torch.full_like(input, fill_value, *, dtype=None, '\n","                           'layout=torch.strided, device=None, '\n","                           'requires_grad=False, '\n","                           'memory_format=torch.preserve_format) → Tensor¶ '\n","                           'torch.full_like¶\\n'\n","                           'Returns a tensor with the same size asinputfilled '\n","                           'withfill_value.torch.full_like(input,fill_value)is '\n","                           'equivalent '\n","                           'totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). '\n","                           'memory_format(torch.memory_format, optional) – the '\n","                           'desired memory format of\\n'\n","                           'returned Tensor.',\n","                'id': 1},\n","               {'Answer': 'default',\n","                'Question': 'What is the default layout ofinput?',\n","                'context': 'Default: ifNone, defaults to the layout ofinput. '\n","                           'Default: ifNone, defaults to the device ofinput. '\n","                           'Default: ifNone, defaults to the dtype ofinput.',\n","                'id': 2},\n","               {'Answer': 'optional',\n","                'Question': 'What is the default memory format of a returned '\n","                            'tensor?',\n","                'context': 'memory_format(torch.memory_format, optional) – the '\n","                           'desired memory format of\\n'\n","                           'returned Tensor. requires_grad(bool,optional) – If '\n","                           'autograd should record operations on the\\n'\n","                           'returned tensor. dtype(torch.dtype, optional) – '\n","                           'the desired data type of returned Tensor.',\n","                'id': 3}],\n"," 'statement': 'torch.full_like¶\\n'\n","              '\\n'\n","              '\\n'\n","              'torch.full_like(input, fill_value, *, dtype=None, '\n","              'layout=torch.strided, device=None, requires_grad=False, '\n","              'memory_format=torch.preserve_format) → Tensor¶ '\n","              'torch.full_like¶\\n'\n","              'Returns a tensor with the same size asinputfilled '\n","              'withfill_value.torch.full_like(input,fill_value)is equivalent '\n","              'totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). '\n","              'input(Tensor) – the size ofinputwill determine size of the '\n","              'output tensor. fill_value– the number to fill the output tensor '\n","              'with. dtype(torch.dtype, optional) – the desired data type of '\n","              'returned Tensor. Default: ifNone, defaults to the dtype '\n","              'ofinput. layout(torch.layout, optional) – the desired layout of '\n","              'returned tensor. Default: ifNone, defaults to the layout '\n","              'ofinput. device(torch.device, optional) – the desired device of '\n","              'returned tensor. Default: ifNone, defaults to the device '\n","              'ofinput. requires_grad(bool,optional) – If autograd should '\n","              'record operations on the\\n'\n","              'returned tensor. memory_format(torch.memory_format, optional) – '\n","              'the desired memory format of\\n'\n","              'returned Tensor. Default:torch.preserve_format.'}\n","Running model for generation\n","{'questions': [{'Question': 'What is the desired device of the returned tensor?', 'Answer': 'tensor', 'id': 1, 'context': 'torch.full_like¶ device(torch.device, optional) – the desired device of returned tensor.'}, {'Question': 'What is the default value of ifNone?', 'Answer': 'default', 'id': 2, 'context': 'Default: ifNone, defaults to the device ofinput.'}]}\n","{'questions': [{'Answer': 'tensor',\n","                'Question': 'What is the same size as inputfilled '\n","                            'withfill_value?',\n","                'context': 'torch.full_like¶\\n'\n","                           '\\n'\n","                           '\\n'\n","                           'torch.full_like(input, fill_value, *, dtype=None, '\n","                           'layout=torch.strided, device=None, '\n","                           'requires_grad=False, '\n","                           'memory_format=torch.preserve_format) → Tensor¶ '\n","                           'torch.full_like¶\\n'\n","                           'Returns a tensor with the same size asinputfilled '\n","                           'withfill_value.torch.full_like(input,fill_value)is '\n","                           'equivalent '\n","                           'totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). '\n","                           'torch.full_like¶\\n'\n","                           '\\n'\n","                           '\\n'\n","                           'torch.full_like(input, fill_value, *, dtype=None, '\n","                           'layout=torch.strided, device=None, '\n","                           'requires_grad=False, '\n","                           'memory_format=torch.preserve_format) → Tensor¶ '\n","                           'torch.full_like¶\\n'\n","                           'Returns a tensor with the same size asinputfilled '\n","                           'withfill_value.torch.full_like(input,fill_value)is '\n","                           'equivalent '\n","                           'totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). '\n","                           'memory_format(torch.memory_format, optional) – the '\n","                           'desired memory format of\\n'\n","                           'returned Tensor.',\n","                'id': 1},\n","               {'Answer': 'default',\n","                'Question': 'What is the default layout ofinput?',\n","                'context': 'Default: ifNone, defaults to the layout ofinput. '\n","                           'Default: ifNone, defaults to the device ofinput. '\n","                           'Default: ifNone, defaults to the dtype ofinput.',\n","                'id': 2},\n","               {'Answer': 'optional',\n","                'Question': 'What is the default memory format of a returned '\n","                            'tensor?',\n","                'context': 'memory_format(torch.memory_format, optional) – the '\n","                           'desired memory format of\\n'\n","                           'returned Tensor. requires_grad(bool,optional) – If '\n","                           'autograd should record operations on the\\n'\n","                           'returned tensor. dtype(torch.dtype, optional) – '\n","                           'the desired data type of returned Tensor.',\n","                'id': 3}],\n"," 'statement': 'torch.full_like¶\\n'\n","              '\\n'\n","              '\\n'\n","              'torch.full_like(input, fill_value, *, dtype=None, '\n","              'layout=torch.strided, device=None, requires_grad=False, '\n","              'memory_format=torch.preserve_format) → Tensor¶ '\n","              'torch.full_like¶\\n'\n","              'Returns a tensor with the same size asinputfilled '\n","              'withfill_value.torch.full_like(input,fill_value)is equivalent '\n","              'totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). '\n","              'input(Tensor) – the size ofinputwill determine size of the '\n","              'output tensor. fill_value– the number to fill the output tensor '\n","              'with. dtype(torch.dtype, optional) – the desired data type of '\n","              'returned Tensor. Default: ifNone, defaults to the dtype '\n","              'ofinput. layout(torch.layout, optional) – the desired layout of '\n","              'returned tensor. Default: ifNone, defaults to the layout '\n","              'ofinput. device(torch.device, optional) – the desired device of '\n","              'returned tensor. Default: ifNone, defaults to the device '\n","              'ofinput. requires_grad(bool,optional) – If autograd should '\n","              'record operations on the\\n'\n","              'returned tensor. memory_format(torch.memory_format, optional) – '\n","              'the desired memory format of\\n'\n","              'returned Tensor. Default:torch.preserve_format.'}\n","Running model for generation\n","{'questions': [{'Question': 'What is the return value of a tensor?', 'Answer': 'tensor', 'id': 1, 'context': 'torch.full_like¶ requires_grad(bool,optional) – If autograd should record operations on the\\nreturned tensor.'}]}\n","{'questions': [{'Answer': 'tensor',\n","                'Question': 'What is the same size as inputfilled '\n","                            'withfill_value?',\n","                'context': 'torch.full_like¶\\n'\n","                           '\\n'\n","                           '\\n'\n","                           'torch.full_like(input, fill_value, *, dtype=None, '\n","                           'layout=torch.strided, device=None, '\n","                           'requires_grad=False, '\n","                           'memory_format=torch.preserve_format) → Tensor¶ '\n","                           'torch.full_like¶\\n'\n","                           'Returns a tensor with the same size asinputfilled '\n","                           'withfill_value.torch.full_like(input,fill_value)is '\n","                           'equivalent '\n","                           'totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). '\n","                           'torch.full_like¶\\n'\n","                           '\\n'\n","                           '\\n'\n","                           'torch.full_like(input, fill_value, *, dtype=None, '\n","                           'layout=torch.strided, device=None, '\n","                           'requires_grad=False, '\n","                           'memory_format=torch.preserve_format) → Tensor¶ '\n","                           'torch.full_like¶\\n'\n","                           'Returns a tensor with the same size asinputfilled '\n","                           'withfill_value.torch.full_like(input,fill_value)is '\n","                           'equivalent '\n","                           'totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). '\n","                           'memory_format(torch.memory_format, optional) – the '\n","                           'desired memory format of\\n'\n","                           'returned Tensor.',\n","                'id': 1},\n","               {'Answer': 'default',\n","                'Question': 'What is the default layout ofinput?',\n","                'context': 'Default: ifNone, defaults to the layout ofinput. '\n","                           'Default: ifNone, defaults to the device ofinput. '\n","                           'Default: ifNone, defaults to the dtype ofinput.',\n","                'id': 2},\n","               {'Answer': 'optional',\n","                'Question': 'What is the default memory format of a returned '\n","                            'tensor?',\n","                'context': 'memory_format(torch.memory_format, optional) – the '\n","                           'desired memory format of\\n'\n","                           'returned Tensor. requires_grad(bool,optional) – If '\n","                           'autograd should record operations on the\\n'\n","                           'returned tensor. dtype(torch.dtype, optional) – '\n","                           'the desired data type of returned Tensor.',\n","                'id': 3}],\n"," 'statement': 'torch.full_like¶\\n'\n","              '\\n'\n","              '\\n'\n","              'torch.full_like(input, fill_value, *, dtype=None, '\n","              'layout=torch.strided, device=None, requires_grad=False, '\n","              'memory_format=torch.preserve_format) → Tensor¶ '\n","              'torch.full_like¶\\n'\n","              'Returns a tensor with the same size asinputfilled '\n","              'withfill_value.torch.full_like(input,fill_value)is equivalent '\n","              'totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). '\n","              'input(Tensor) – the size ofinputwill determine size of the '\n","              'output tensor. fill_value– the number to fill the output tensor '\n","              'with. dtype(torch.dtype, optional) – the desired data type of '\n","              'returned Tensor. Default: ifNone, defaults to the dtype '\n","              'ofinput. layout(torch.layout, optional) – the desired layout of '\n","              'returned tensor. Default: ifNone, defaults to the layout '\n","              'ofinput. device(torch.device, optional) – the desired device of '\n","              'returned tensor. Default: ifNone, defaults to the device '\n","              'ofinput. requires_grad(bool,optional) – If autograd should '\n","              'record operations on the\\n'\n","              'returned tensor. memory_format(torch.memory_format, optional) – '\n","              'the desired memory format of\\n'\n","              'returned Tensor. Default:torch.preserve_format.'}\n","Running model for generation\n","{'questions': [{'Question': 'What is the memory format of the returned tensor?', 'Answer': 'tensor', 'id': 1, 'context': 'torch.full_like¶ memory_format(torch.memory_format, optional) – the desired memory format of\\nreturned Tensor.'}, {'Question': 'What is the default format for Torch.preserve_format?', 'Answer': 'default', 'id': 2, 'context': 'Default:torch.preserve_format.'}]}\n","{'questions': [{'Answer': 'tensor',\n","                'Question': 'What is the same size as inputfilled '\n","                            'withfill_value?',\n","                'context': 'torch.full_like¶\\n'\n","                           '\\n'\n","                           '\\n'\n","                           'torch.full_like(input, fill_value, *, dtype=None, '\n","                           'layout=torch.strided, device=None, '\n","                           'requires_grad=False, '\n","                           'memory_format=torch.preserve_format) → Tensor¶ '\n","                           'torch.full_like¶\\n'\n","                           'Returns a tensor with the same size asinputfilled '\n","                           'withfill_value.torch.full_like(input,fill_value)is '\n","                           'equivalent '\n","                           'totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). '\n","                           'torch.full_like¶\\n'\n","                           '\\n'\n","                           '\\n'\n","                           'torch.full_like(input, fill_value, *, dtype=None, '\n","                           'layout=torch.strided, device=None, '\n","                           'requires_grad=False, '\n","                           'memory_format=torch.preserve_format) → Tensor¶ '\n","                           'torch.full_like¶\\n'\n","                           'Returns a tensor with the same size asinputfilled '\n","                           'withfill_value.torch.full_like(input,fill_value)is '\n","                           'equivalent '\n","                           'totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). '\n","                           'memory_format(torch.memory_format, optional) – the '\n","                           'desired memory format of\\n'\n","                           'returned Tensor.',\n","                'id': 1},\n","               {'Answer': 'default',\n","                'Question': 'What is the default layout ofinput?',\n","                'context': 'Default: ifNone, defaults to the layout ofinput. '\n","                           'Default: ifNone, defaults to the device ofinput. '\n","                           'Default: ifNone, defaults to the dtype ofinput.',\n","                'id': 2},\n","               {'Answer': 'optional',\n","                'Question': 'What is the default memory format of a returned '\n","                            'tensor?',\n","                'context': 'memory_format(torch.memory_format, optional) – the '\n","                           'desired memory format of\\n'\n","                           'returned Tensor. requires_grad(bool,optional) – If '\n","                           'autograd should record operations on the\\n'\n","                           'returned tensor. dtype(torch.dtype, optional) – '\n","                           'the desired data type of returned Tensor.',\n","                'id': 3}],\n"," 'statement': 'torch.full_like¶\\n'\n","              '\\n'\n","              '\\n'\n","              'torch.full_like(input, fill_value, *, dtype=None, '\n","              'layout=torch.strided, device=None, requires_grad=False, '\n","              'memory_format=torch.preserve_format) → Tensor¶ '\n","              'torch.full_like¶\\n'\n","              'Returns a tensor with the same size asinputfilled '\n","              'withfill_value.torch.full_like(input,fill_value)is equivalent '\n","              'totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). '\n","              'input(Tensor) – the size ofinputwill determine size of the '\n","              'output tensor. fill_value– the number to fill the output tensor '\n","              'with. dtype(torch.dtype, optional) – the desired data type of '\n","              'returned Tensor. Default: ifNone, defaults to the dtype '\n","              'ofinput. layout(torch.layout, optional) – the desired layout of '\n","              'returned tensor. Default: ifNone, defaults to the layout '\n","              'ofinput. device(torch.device, optional) – the desired device of '\n","              'returned tensor. Default: ifNone, defaults to the device '\n","              'ofinput. requires_grad(bool,optional) – If autograd should '\n","              'record operations on the\\n'\n","              'returned tensor. memory_format(torch.memory_format, optional) – '\n","              'the desired memory format of\\n'\n","              'returned Tensor. Default:torch.preserve_format.'}\n","4 torch.full_like¶\n","\n","Parameters torch.full_like¶\n","input(Tensor) – the size ofinputwill determine size of the output tensor.\n","fill_value– the number to fill the output tensor with.\n","dtype(torch.dtype, optional) – the desired data type of returned Tensor.\n","Default: ifNone, defaults to the dtype ofinput.\n","layout(torch.layout, optional) – the desired layout of returned tensor.\n","Default: ifNone, defaults to the layout ofinput.\n","device(torch.device, optional) – the desired device of returned tensor.\n","Default: ifNone, defaults to the device ofinput.\n","requires_grad(bool,optional) – If autograd should record operations on the\n","returned tensor. Default:False.\n","memory_format(torch.memory_format, optional) – the desired memory format of\n","returned Tensor. Default:torch.preserve_format.\n","Running model for generation\n","{'questions': [{'Question': 'What is the desired data type of returned Tensor?', 'Answer': 'data type', 'id': 1, 'context': 'dtype(torch.dtype, optional) – the desired data type of returned Tensor.'}, {'Question': 'What is the default layout ofinput?', 'Answer': 'default', 'id': 2, 'context': 'Default: ifNone, defaults to the layout ofinput. Default: ifNone, defaults to the device ofinput. Default: ifNone, defaults to the dtype ofinput.'}]}\n","{'questions': [{'Answer': 'data type',\n","                'Question': 'What is the desired data type of returned Tensor?',\n","                'context': 'dtype(torch.dtype, optional) – the desired data '\n","                           'type of returned Tensor.',\n","                'id': 1},\n","               {'Answer': 'default',\n","                'Question': 'What is the default layout ofinput?',\n","                'context': 'Default: ifNone, defaults to the layout ofinput. '\n","                           'Default: ifNone, defaults to the device ofinput. '\n","                           'Default: ifNone, defaults to the dtype ofinput.',\n","                'id': 2}],\n"," 'statement': 'torch.full_like¶\\n'\n","              '\\n'\n","              'Parameters torch.full_like¶\\n'\n","              'input(Tensor) – the size ofinputwill determine size of the '\n","              'output tensor. fill_value– the number to fill the output tensor '\n","              'with. dtype(torch.dtype, optional) – the desired data type of '\n","              'returned Tensor. Default: ifNone, defaults to the dtype '\n","              'ofinput. layout(torch.layout, optional) – the desired layout of '\n","              'returned tensor. Default: ifNone, defaults to the layout '\n","              'ofinput. device(torch.device, optional) – the desired device of '\n","              'returned tensor. Default: ifNone, defaults to the device '\n","              'ofinput. requires_grad(bool,optional) – If autograd should '\n","              'record operations on the\\n'\n","              'returned tensor. memory_format(torch.memory_format, optional) – '\n","              'the desired memory format of\\n'\n","              'returned Tensor. Default:torch.preserve_format.'}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eOaGphSUv-9r","executionInfo":{"status":"ok","timestamp":1628580656041,"user_tz":-330,"elapsed":22,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}},"outputId":"554499cd-3ed3-4c85-c61b-df76f5a108ae"},"source":["dtypeqalist"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[]"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OPA7Q1IWQ5rS","executionInfo":{"status":"ok","timestamp":1628580672182,"user_tz":-330,"elapsed":16149,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}},"outputId":"934d7dd7-1852-4309-8d30-a6a3652e8d85"},"source":["hdr = ['h1', 'h2' , 'h3' , 'h4']\n","\n","with open('pyt.txt', 'w') as pt:\n","  questndict1 = {}\n","  url =  \"https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\"\n","  #\"https://pytorch.org/get-started/locally/\"\n","\n","  #html = requests.get(url)\n","  html = req.urlopen(url).read()\n","\n","  soup = BeautifulSoup(html)\n","\n","\n","  functns  = soup.findAll('dl') #, class_=\"function\")\n","\n","  if functns and len(functns) > 0: \n","    prev = functns[0].findPreviousSibling() \n","    funcname = \"\"\n","    if prev :\n","      funcname = prev.get_text().strip()\n","    for func in functns:\n","      funcdftn = funcname + '\\n'\n","      #print( '1',funcdftn)\n","      dt = func.find('dt')\n","      #print('2',dt)\n","      if dt :\n","        funcdftn = funcdftn + '\\n' + dt.text\n","        #print('3' ,funcdftn)\n","\n","\n","      p = func.find_all(lambda tag: tag.name == \"p\" ) #and not (tag.find(\"script\")))\n","      \n","      for content in p:\n","        dfn = content.get_text(strip=True)\n","        funcdftn =  ' ' + funcdftn + dfn + '\\n'\n","\n","      funcdftn = funcdftn.strip()\n","      print('4',funcdftn)\n","      pyld = {}\n","\n","      pyld[\"input_text\"] = funcdftn\n","\n","      output = qg.predict_shortq(pyld)\n","      pprint (output)\n","\n","      if len(output) > 0:\n","        qalist = output[\"questions\"]\n","        questndict1 = collatequestions(qalist,questndict1)\n","\n","  else:\n","    i = 0\n","    for pel in soup.findAll('p'):\n","      \"\"\"prv = pel.findPreviousSibling() \n","      if prv != None:\n","        if (prv.name in hdr):\n","          print(prv , pel)\n","          pt.write(prv.text + '\\n' + pel.text + '\\n')\n","        elif len(pel.text) > 0:\n","          print(pel)\n","          pt.write(pel.text + '\\n')\"\"\"\n","\n","      if len(pel.text) > 0 :\n","        pyld = {}\n","\n","        pyld[\"input_text\"] = pel.text\n","\n","        output = qg.predict_shortq(pyld)\n","        pprint (output)\n","\n","        if len(output) > 0:\n","          qalist = output[\"questions\"]\n","          questndict1 = collatequestions(qalist,questndict1)\n","\n","\n","\n","#print (len(questndict1))"],"execution_count":15,"outputs":[{"output_type":"stream","text":["4 torch.svd_lowrank¶\n","\n","\n","torch.svd_lowrank(A, q=6, niter=2, M=None)[source]¶Return the singular value decomposition(U,S,V)of a matrix,\n","batches of matrices, or a sparse matrixAAAsuch thatA≈Udiag(S)VTA \\approx U diag(S) V^TA≈Udiag(S)VT. In caseMMMis given, then\n","SVD is computed for the matrixA−MA - MA−M.\n","Note\n","The implementation is based on the Algorithm 5.1 from\n","Halko et al, 2009.\n","Note\n","To obtain repeatable results, reset the seed for the\n","pseudorandom number generator\n","Note\n","The input is assumed to be a low-rank matrix.\n","Note\n","In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\n","higher performance characteristics. The low-rank SVD\n","will be useful for huge sparse matrices thattorch.linalg.svd()cannot handle.\n","A (Tensor): the input tensor of size(∗,m,n)(*, m, n)(∗,m,n)\n","q (int, optional): a slightly overestimated rank of A.\n","conduct; niter must be a nonnegative\n","integer, and defaults to 2\n","(∗,1,n)(*, 1, n)(∗,1,n).\n","Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\n","structure with randomness: probabilistic algorithms for\n","constructing approximate matrix decompositions,\n","arXiv:0909.4061 [math.NA; math.PR], 2009 (available atarXiv).\n","Running model for generation\n","{'questions': [{'Question': 'What is the singular value decomposition of a matrix, batches of matrices, or a sparse matrixAAA?', 'Answer': 'matrices', 'id': 1, 'context': 'torch.svd_lowrank¶\\n\\n\\ntorch.svd_lowrank(A, q=6, niter=2, M=None)[source]¶Return the singular value decomposition(U,S,V)of a matrix,\\nbatches of matrices, or a sparse matrixAAAsuch thatA≈Udiag(S)VTA \\\\approx U diag(S) V^TA≈Udiag(S)VT. Note\\nIn general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\\nhigher performance characteristics. The low-rank SVD\\nwill be useful for huge sparse matrices thattorch.linalg.svd()cannot handle.'}, {'Question': 'What is the full-rank SVD implementationtorch.linalg.svd()?', 'Answer': 'svd', 'id': 2, 'context': 'Note\\nIn general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\\nhigher performance characteristics. Note\\nIn general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\\nhigher performance characteristics. The low-rank SVD\\nwill be useful for huge sparse matrices thattorch.linalg.svd()cannot handle.'}]}\n","{'questions': [{'Answer': 'matrices',\n","                'Question': 'What is the singular value decomposition of a '\n","                            'matrix, batches of matrices, or a sparse '\n","                            'matrixAAA?',\n","                'context': 'torch.svd_lowrank¶\\n'\n","                           '\\n'\n","                           '\\n'\n","                           'torch.svd_lowrank(A, q=6, niter=2, '\n","                           'M=None)[source]¶Return the singular value '\n","                           'decomposition(U,S,V)of a matrix,\\n'\n","                           'batches of matrices, or a sparse matrixAAAsuch '\n","                           'thatA≈Udiag(S)VTA \\\\approx U diag(S) '\n","                           'V^TA≈Udiag(S)VT. Note\\n'\n","                           'In general, use the full-rank SVD '\n","                           'implementationtorch.linalg.svd()for dense matrices '\n","                           'due to its 10-fold\\n'\n","                           'higher performance characteristics. The low-rank '\n","                           'SVD\\n'\n","                           'will be useful for huge sparse matrices '\n","                           'thattorch.linalg.svd()cannot handle.',\n","                'id': 1},\n","               {'Answer': 'svd',\n","                'Question': 'What is the full-rank SVD '\n","                            'implementationtorch.linalg.svd()?',\n","                'context': 'Note\\n'\n","                           'In general, use the full-rank SVD '\n","                           'implementationtorch.linalg.svd()for dense matrices '\n","                           'due to its 10-fold\\n'\n","                           'higher performance characteristics. Note\\n'\n","                           'In general, use the full-rank SVD '\n","                           'implementationtorch.linalg.svd()for dense matrices '\n","                           'due to its 10-fold\\n'\n","                           'higher performance characteristics. The low-rank '\n","                           'SVD\\n'\n","                           'will be useful for huge sparse matrices '\n","                           'thattorch.linalg.svd()cannot handle.',\n","                'id': 2}],\n"," 'statement': 'torch.svd_lowrank¶\\n'\n","              '\\n'\n","              '\\n'\n","              'torch.svd_lowrank(A, q=6, niter=2, M=None)[source]¶Return the '\n","              'singular value decomposition(U,S,V)of a matrix,\\n'\n","              'batches of matrices, or a sparse matrixAAAsuch '\n","              'thatA≈Udiag(S)VTA \\\\approx U diag(S) V^TA≈Udiag(S)VT. In '\n","              'caseMMMis given, then\\n'\n","              'SVD is computed for the matrixA−MA - MA−M. Note\\n'\n","              'The implementation is based on the Algorithm 5.1 from\\n'\n","              'Halko et al, 2009. Note\\n'\n","              'To obtain repeatable results, reset the seed for the\\n'\n","              'pseudorandom number generator\\n'\n","              'Note\\n'\n","              'The input is assumed to be a low-rank matrix. Note\\n'\n","              'In general, use the full-rank SVD '\n","              'implementationtorch.linalg.svd()for dense matrices due to its '\n","              '10-fold\\n'\n","              'higher performance characteristics. The low-rank SVD\\n'\n","              'will be useful for huge sparse matrices '\n","              'thattorch.linalg.svd()cannot handle. A (Tensor): the input '\n","              'tensor of size(∗,m,n)(*, m, n)(∗,m,n)\\n'\n","              'q (int, optional): a slightly overestimated rank of A.\\n'\n","              'conduct; niter must be a nonnegative\\n'\n","              'integer, and defaults to 2\\n'\n","              '(∗,1,n)(*, 1, n)(∗,1,n). Nathan Halko, Per-Gunnar Martinsson, '\n","              'and Joel Tropp, Finding\\n'\n","              'structure with randomness: probabilistic algorithms for\\n'\n","              'constructing approximate matrix decompositions,\\n'\n","              'arXiv:0909.4061 [math.NA; math.PR], 2009 (available atarXiv).'}\n","4 torch.svd_lowrank¶\n","\n","Args::A (Tensor): the input tensor of size(∗,m,n)(*, m, n)(∗,m,n)\n","q (int, optional): a slightly overestimated rank of A.\n","conduct; niter must be a nonnegative\n","integer, and defaults to 2\n","(∗,1,n)(*, 1, n)(∗,1,n).\n","Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\n","structure with randomness: probabilistic algorithms for\n","constructing approximate matrix decompositions,\n","arXiv:0909.4061 [math.NA; math.PR], 2009 (available atarXiv).\n","Running model for generation\n","{'questions': [{'Question': 'What is the key to finding structure with?', 'Answer': 'randomness', 'id': 1, 'context': 'Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\\nstructure with randomness: probabilistic algorithms for\\nconstructing approximate matrix decompositions,\\narXiv:0909.4061 [math.NA; math.PR], 2009 (available atarXiv).'}]}\n","{'questions': [{'Answer': 'randomness',\n","                'Question': 'What is the key to finding structure with?',\n","                'context': 'Nathan Halko, Per-Gunnar Martinsson, and Joel '\n","                           'Tropp, Finding\\n'\n","                           'structure with randomness: probabilistic '\n","                           'algorithms for\\n'\n","                           'constructing approximate matrix decompositions,\\n'\n","                           'arXiv:0909.4061 [math.NA; math.PR], 2009 '\n","                           '(available atarXiv).',\n","                'id': 1}],\n"," 'statement': 'torch.svd_lowrank¶\\n'\n","              '\\n'\n","              'Args::A (Tensor): the input tensor of size(∗,m,n)(*, m, '\n","              'n)(∗,m,n)\\n'\n","              'q (int, optional): a slightly overestimated rank of A.\\n'\n","              'conduct; niter must be a nonnegative\\n'\n","              'integer, and defaults to 2\\n'\n","              '(∗,1,n)(*, 1, n)(∗,1,n). Nathan Halko, Per-Gunnar Martinsson, '\n","              'and Joel Tropp, Finding\\n'\n","              'structure with randomness: probabilistic algorithms for\\n'\n","              'constructing approximate matrix decompositions,\\n'\n","              'arXiv:0909.4061 [math.NA; math.PR], 2009 (available atarXiv).'}\n","4 torch.svd_lowrank¶\n","\n","niter (int, optional): the number of subspace iterations toconduct; niter must be a nonnegative\n","integer, and defaults to 2\n","(∗,1,n)(*, 1, n)(∗,1,n).\n","Running model for generation\n","{'questions': [{'Question': 'What is the number of subspace iterations toconduct?', 'Answer': 'niter', 'id': 1, 'context': 'torch.svd_lowrank¶\\n\\nniter (int, optional): the number of subspace iterations toconduct; niter must be a nonnegative\\ninteger, and defaults to 2\\n(∗,1,n)(*, 1, n)(∗,1,n). torch.svd_lowrank¶\\n\\nniter (int, optional): the number of subspace iterations toconduct; niter must be a nonnegative\\ninteger, and defaults to 2\\n(∗,1,n)(*, 1, n)(∗,1,n).'}]}\n","{'questions': [{'Answer': 'niter',\n","                'Question': 'What is the number of subspace iterations '\n","                            'toconduct?',\n","                'context': 'torch.svd_lowrank¶\\n'\n","                           '\\n'\n","                           'niter (int, optional): the number of subspace '\n","                           'iterations toconduct; niter must be a nonnegative\\n'\n","                           'integer, and defaults to 2\\n'\n","                           '(∗,1,n)(*, 1, n)(∗,1,n). torch.svd_lowrank¶\\n'\n","                           '\\n'\n","                           'niter (int, optional): the number of subspace '\n","                           'iterations toconduct; niter must be a nonnegative\\n'\n","                           'integer, and defaults to 2\\n'\n","                           '(∗,1,n)(*, 1, n)(∗,1,n).',\n","                'id': 1}],\n"," 'statement': 'torch.svd_lowrank¶\\n'\n","              '\\n'\n","              'niter (int, optional): the number of subspace iterations '\n","              'toconduct; niter must be a nonnegative\\n'\n","              'integer, and defaults to 2\\n'\n","              '(∗,1,n)(*, 1, n)(∗,1,n).'}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5kZo4BT5lgSt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628580672184,"user_tz":-330,"elapsed":23,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}},"outputId":"3112d6e2-e934-45b3-8753-e8b59bcf209f"},"source":["dtypeqalist"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"UttqkvEGPKWN","colab":{"base_uri":"https://localhost:8080/","height":174},"executionInfo":{"status":"error","timestamp":1628580672194,"user_tz":-330,"elapsed":28,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}},"outputId":"6e8cf0ea-44f8-47b5-bb8f-1ae2a9943c9f"},"source":["questndict"],"execution_count":17,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-e301fed96915>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mquestndict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'questndict' is not defined"]}]},{"cell_type":"code","metadata":{"id":"O0y9KftqTi46","executionInfo":{"status":"aborted","timestamp":1628580672186,"user_tz":-330,"elapsed":18,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}}},"source":["output['questions'][0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5kHnAUUrprZ5","executionInfo":{"status":"aborted","timestamp":1628580672187,"user_tz":-330,"elapsed":19,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}}},"source":["questndict"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tZKE92JoeseD"},"source":["### call function"]},{"cell_type":"markdown","metadata":{"id":"WocEh4HoYwbP"},"source":["### loop files"]},{"cell_type":"code","metadata":{"id":"I5ZZGHjRYvA9","executionInfo":{"status":"aborted","timestamp":1628580672189,"user_tz":-330,"elapsed":21,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}}},"source":["hdr = ['h1', 'h2' , 'h3' , 'h4']\n","\n","questndict = {}\n","\n","for url in urllist[5:10]:\n","#with open('pyt.txt', 'w') as pt:\n","  \n","  #url =  \"https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\"\n","  #\"https://pytorch.org/get-started/locally/\"\n","\n","  #html = requests.get(url)\n","  print(url)\n","  html = req.urlopen(url).read()\n","\n","  soup = BeautifulSoup(html)\n","\n","\n","  functns  = soup.findAll('dl')\n","\n","  if functns and len(functns) > 0: \n","\n","    \"\"\"for func in functns:\n","      p = func.find_all(lambda tag: tag.name == \"p\" and not (tag.find(\"script\")))\n","      funcdftn = \"\"\n","      for content in p:\n","        dfn = content.get_text(strip=True)\n","        funcdftn =  ' ' + funcdftn + dfn + '\\n'\n","\n","      funcdftn = funcdftn.strip()\n","\n","      pyld = {}\n","\n","      pyld[\"input_text\"] = funcdftn\n","\n","      output = qg.predict_shortq(pyld)\n","      pprint (output)\"\"\"\n","\n","    prev = functns[0].findPreviousSibling() \n","    funcname = \"\"\n","    if prev :\n","      funcname = prev.get_text().strip()\n","    for func in functns:\n","      funcdftn = funcname + '\\n'\n","      #print( '1',funcdftn)\n","      dt = func.find('dt')\n","      #print('2',dt)\n","      if dt :\n","        funcdftn = funcdftn + '\\n' + dt.text\n","        #print('3' ,funcdftn)\n","\n","\n","      p = func.find_all(lambda tag: tag.name == \"p\" ) #and not (tag.find(\"script\")))\n","      if p:\n","        funcdftn = funcdftn + ' ' + funcname + '\\n'\n","        for content in p:\n","          if (len(content.get_text(strip=True))):\n","            dfn = content.get_text(strip=True)\n","            oneline = funcname + ' ' + dfn + '\\n'\n","            funcdftn =  ' ' + funcdftn + dfn + '\\n'\n","            pyld1 = {}\n","\n","            pyld1[\"input_text\"] = oneline.strip()\n","\n","            output1 = qg.predict_shortq(pyld1)\n","            pprint (output)\n","\n","        funcdftn = funcdftn.strip()\n","        #print('4',funcdftn)\n","      pyld = {}\n","\n","      pyld[\"input_text\"] = funcdftn\n","\n","      output = qg.predict_shortq(pyld)\n","      pprint (output)\n","\n","      if len(output) > 0:\n","        qalist = output[\"questions\"]\n","        questndict = collatequestions(qalist,questndict)\n","\n","  else:\n","    i = 0\n","    for pel in soup.findAll('p'):\n","      \"\"\"prv = pel.findPreviousSibling() \n","      if prv != None:\n","        if (prv.name in hdr):\n","          print(prv , pel)\n","          pt.write(prv.text + '\\n' + pel.text + '\\n')\n","        elif len(pel.text) > 0:\n","          print(pel)\n","          pt.write(pel.text + '\\n')\"\"\"\n","\n","      if len(pel.text) > 0 :\n","        pyld = {}\n","\n","        pyld[\"input_text\"] = pel.text\n","\n","        output = qg.predict_shortq(pyld)\n","        pprint (output)\n","\n","        if len(output) > 0:\n","          qalist = output[\"questions\"]\n","          questndict = collatequestions(qalist,questndict)\n","\n","\n","\n","print (len(questndict))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kTv15mQZbEwG","executionInfo":{"status":"aborted","timestamp":1628580672190,"user_tz":-330,"elapsed":21,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}}},"source":["questndict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E92PKobVYW6c","executionInfo":{"status":"aborted","timestamp":1628580672191,"user_tz":-330,"elapsed":21,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}}},"source":["hdr = ['h1', 'h2' , 'h3' , 'h4']\n","\n","with open('pyt.txt', 'w') as pt:\n","\n","  url =  \"https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\"\n","  #url = \"https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\"\n","  #\"https://pytorch.org/get-started/locally/\"\n","\n","  questansw = getQuestionsFromPage(url)\n","  \n","  data = [{'Question':q,'Answer':a} for q , a  in questansw.items()]\n","\n","  df = pd.DataFrame(data)\n","\n","  df.to_csv('existing.csv', mode='a', index=False, header=False)\n","\n","\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z0j2waYCgV3-","executionInfo":{"status":"aborted","timestamp":1628580672193,"user_tz":-330,"elapsed":23,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}}},"source":["hdr = ['h1', 'h2' , 'h3' , 'h4']\n","\n","questndict1 = {}\n","dtypeqalist = []\n","\n","#for url in urllist[5:10]:\n","with open('pyt.txt', 'w') as pt:\n","  \n","  url =\"https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like\"\n","  # \"https://pytorch.org/docs/stable/sparse.html\"\n","  #\"https://pytorch.org/docs/stable/tensors.html\" \n","  #\"https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\"\n","  #\"https://pytorch.org/get-started/locally/\"\n","\n","  #html = requests.get(url)\n","  print(url)\n","  html = req.urlopen(url).read()\n","\n","  soup = BeautifulSoup(html)\n","\n","\n","  functns  = soup.findAll('dl')\n","\n","  tables = soup.find_all('table')\n","\n","  for table in tables:\n","    \n","    print(type(table))\n","\n","    trs = table.find_all('tr')\n","\n","    hashdr = False\n","    if trs:\n","      onehdrs = \"\"\n","      for tr in trs:\n","        hdrs = tr.findAll('th')\n","\n","        if not hashdr:\n","          onehdrs = hdrs\n","\n","\n","        if hdrs:\n","          hashdr = True\n","\n","        tds = tr.findAll('td')\n","        #print('tds ',tds)\n","\n","        if tds and hashdr:\n","          print()\n","          for i in range(1):\n","            #print(len(tds) , len(hdrs))\n","            print(i , tds[i].get_text(strip=True) ,onehdrs[i].get_text(strip=True) )\n","            lyout = tds[i+2].get_text(strip=True)\n","            if re.search(r'@',lyout):\n","              lyout = re.sub(r'@',' matrix multiplication to ',lyout)\n","            \n","            if re.search(r'\\*',lyout):\n","              lyout = re.sub(r'\\*',' element-wise multiplication ',lyout)\n","            \n","            if re.search(r'f',lyout):\n","              lyout = re.sub(r'f',' of a scalar to ',lyout)\n","            #'Is M[sparse_coo] matrix multiplication toV[strided]->V[strided] a validLayout signature fortorch.mv()?|yes',\n","            qstn = \"Is \" + lyout + \" a valid \" + onehdrs[i+2].get_text(strip=True) + ' for ' + tds[i].get_text(strip=True) + \"?|yes\"\n","            \n","            dtypeqalist.append(qstn)\n","            #print(\"sels qstn \", qstn)\n","        elif tds and not hashdr:\n","          \n","          #for i in range(len(tds)):\n","          if 1 == 1:\n","\n","            qstn = ''\n","\n","\n","            if re.match(r'Constructs a|Matrix multiplies a|Performs a|Applies a',tds[1].get_text(strip=True)):\n","              qstn = 'Does ' + tds[0].get_text(strip=True) + ' ' + tds[1].get_text(strip=True) + '?|yes'\n","            elif re.match(r'Returns a',tds[1].get_text(strip=True)):\n","              qstn = re.sub(r'Returns a','How to create ',tds[1].get_text(strip=True)) + '?|' + tds[0].get_text(strip=True)\n","            elif re.match(r'IsTrue',tds[1].get_text(strip=True)):\n","              tokn = tds[1].get_text(strip=True).split(',')\n","              qstn = re.sub(r'IsTrue','How to check ',tokn[0]) + '?|use ' + tds[0].get_text(strip=True)\n","            elif re.match(r'Returns the|Returns True|Returnsself',tds[1].get_text(strip=True)):\n","              substrg = \"\"\n","              if re.match(r'Returns the',tds[1].get_text(strip=True)):\n","                substrg = \"Which function returns the \"\n","              elif re.match(r'Returns True',tds[1].get_text(strip=True)):\n","                substrg = \"Which function returns True \"\n","              if re.match(r'Returnsself',tds[1].get_text(strip=True)):\n","                substrg = \"Which function returns self \"\n","\n","              qstn = re.sub(r'Returns the|Returns True|Returnsself',substrg,tds[1].get_text(strip=True)) + '?|' + tds[0].get_text(strip=True)\n","            elif re.match(r'Given a Tensor quantized by linear(affine) quantization',tds[1].get_text(strip=True)):\n","              tokn = tds[1].get_text(strip=True).split(',')\n","              qstn = tokn[0] + ' , Which function ' + tokn[1] + '?|' + tds[0].get_text(strip=True)\n","            elif re.match(r'Given a Tensor quantized by linear (affine) per-channel quantization',tds[1].get_text(strip=True)):\n","              tokn = tds[1].get_text(strip=True).split(',')\n","              qstn = tokn[0] + ' , Which function ' + tokn[1] + '?|' + tds[0].get_text(strip=True)\n","              \n","\n","            if(len(qstn) > 0):\n","              dtypeqalist.append(qstn)\n","            print('method selctn ',qstn)\n","\n","\n","\n","  if functns and len(functns) > 0: \n","\n","\n","    prev = functns[0].findPreviousSibling() \n","    funcname = \"\"\n","    if prev :\n","      funcname = prev.get_text().strip()\n","    for func in functns:\n","      funcdftn = funcname + '\\n'\n","      #print( '1',funcdftn)\n","      dt = func.find('dt')\n","      #print('2',dt)\n","      if dt :\n","        funcdftn = funcdftn + '\\n' + dt.text\n","        print('3' ,dt.get_text())\n","\n","        dds = func.find_all('dd')\n","\n","        if dds:\n","          print(6, dds)\n","          for dd in dds:\n","            lis = dd.find_all('li')\n","            \n","            if lis:\n","              for li in lis:\n","                 ps = li.find_all('p')\n","                 if ps:\n","                   for p in ps:\n","                     print('5',p.get_text())\n","\n","\n","\n","\n","      p = func.find_all(lambda tag: tag.name == \"p\" ) #and not (tag.find(\"script\")))\n","      if p:\n","        funcdftn = funcdftn + ' ' + funcname + '\\n'\n","        for content in p:\n","          if (len(content.get_text(strip=True))):\n","            dfn = content.get_text(strip=True)\n","            oneline = funcname + ' ' + dfn + '\\n'\n","            funcdftn =  ' ' + funcdftn + dfn + '\\n'\n","            pyld1 = {}\n","\n","            pyld1[\"input_text\"] = oneline.strip()\n","\n","            \"\"\"output1 = qg.predict_shortq(pyld1)\n","            pprint (output)\"\"\"\n","\n","        funcdftn = funcdftn.strip()\n","        print('4',funcdftn)\n"],"execution_count":null,"outputs":[]}]}