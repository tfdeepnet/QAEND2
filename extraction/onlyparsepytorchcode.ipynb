{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"onlyparsepytorchcode.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1qW4YWeUHtS799PM-iA7oeVWpgIhy8A5p","authorship_tag":"ABX9TyNPY3kUCxugqE1EQyJ7t3L/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"jsmYCaET5epL"},"source":["https://github.com/indrajithi/genquest"]},{"cell_type":"code","metadata":{"id":"W0r1lgT5jS25","executionInfo":{"status":"ok","timestamp":1628683220677,"user_tz":-330,"elapsed":613,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}}},"source":["import pandas as pd\n","import random\n","import re \n","import csv\n","import urllib.request as req\n","import requests\n","from bs4 import BeautifulSoup\n","import json"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qflLqom-10ta","executionInfo":{"status":"ok","timestamp":1628683221756,"user_tz":-330,"elapsed":752,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}},"outputId":"c32b6e8f-d580-45ef-aae4-171d5d8fbdcb"},"source":["users = [\"Naveen M\",  \"Parinita Bora\", \"Anish V\", \"Megha Shyam T\", \"Nikhil Shrimali\", \"Deepak Hazarika\",  \"Vishak Bharadwaj\", \"Ritambhra Korpal\", \"Arghya\"]\n","\n","with open('/content/teamurlallocation.csv' , 'r') as dh:\n","  usrdh = False\n","  teamurllist = []\n","  i = 0\n","  j = 0\n","  for line in dh.readlines():\n","    #print(line)\n","    \n","    i += 1\n","    print(i, line.strip())\n","    teamurllist.append(line.strip())\n","      #print(i , line)\n","    print(j)  \n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["1 https://pytorch.org/docs/stable/generated/torch.atanh.html#torch.atanh\n","0\n","2 https://discuss.pytorch.org/\n","0\n","3 https://pytorch.org/docs/stable/generated/torch.logaddexp.html#torch.logaddexp\n","0\n","4 https://pytorch.org/docs/stable/testing.html\n","0\n","5 https://pytorch.org/docs/stable/generated/torch.diag.html#torch.diag\n","0\n","6 https://pytorch.org/docs/stable/torch.html#math-operations\n","0\n","7 https://pytorch.org/docs/stable/torch.overrides.html\n","0\n","8 https://pytorch.org/docs/stable/generated/torch.mvlgamma.html#torch.mvlgamma\n","0\n","9 https://pytorch.org/docs/stable/generated/torch.fake_quantize_per_channel_affine.html#torch.fake_quantize_per_channel_affine\n","0\n","10 https://pytorch.org/docs/stable/generated/torch.atleast_1d.html#torch.atleast_1d\n","0\n","11 https://pytorch.org/docs/stable/generated/torch.heaviside.html#torch.heaviside\n","0\n","12 https://pytorch.org/docs/stable/generated/torch.is_storage.html#torch.is_storage\n","0\n","13 https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg\n","0\n","14 https://pytorch.org/docs/stable/generated/torch.atleast_2d.html#torch.atleast_2d\n","0\n","15 https://pytorch.org/docs/stable/generated/torch.ldexp.html#torch.ldexp\n","0\n","16 https://pytorch.org/docs/stable/generated/torch.promote_types.html#torch.promote_types\n","0\n","17 https://pytorch.org/docs/stable/generated/torch.set_num_interop_threads.html#torch.set_num_interop_threads\n","0\n","18 https://pytorch.org/docs/stable/generated/torch.nextafter.html#torch.nextafter\n","0\n","19 https://pytorch.org/docs/stable/generated/torch.stack.html#torch.stack\n","0\n","20 https://pytorch.org/blog/\n","0\n","21 https://pytorch.org/docs/stable/generated/torch.set_default_tensor_type.html#torch.set_default_tensor_type\n","0\n","22 https://pytorch.org/docs/stable/generated/torch.diagflat.html#torch.diagflat\n","0\n","23 https://pytorch.org/docs/stable/generated/torch.lu_unpack.html#torch.lu_unpack\n","0\n","24 https://pytorch.org/docs/stable/generated/torch.true_divide.html#torch.true_divide\n","0\n","25 https://pytorch.org/docs/stable/generated/torch.addmv.html#torch.addmv\n","0\n","26 https://pytorch.org/docs/stable/generated/torch.nanmedian.html#torch.nanmedian\n","0\n","27 https://pytorch.org/docs/stable/generated/torch.empty_strided.html#torch.empty_strided\n","0\n","28 https://pytorch.org/docs/stable/generated/torch.isnan.html#torch.isnan\n","0\n","29 https://pytorch.org/docs/stable/generated/torch.atleast_3d.html#torch.atleast_3d\n","0\n","30 https://pytorch.org/docs/stable/generated/torch.bitwise_xor.html#torch.bitwise_xor\n","0\n","31 https://pytorch.org/docs/stable/generated/torch.sinh.html#torch.sinh\n","0\n","32 https://pytorch.org/docs/stable/generated/torch.roll.html#torch.roll\n","0\n","33 https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile\n","0\n","34 https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk\n","0\n","35 https://pytorch.org/docs/stable/nn.functional.html\n","0\n","36 https://pytorch.org/docs/stable/generated/torch.sqrt.html#torch.sqrt\n","0\n","37 https://pytorch.org/docs/stable/generated/torch.minimum.html#torch.minimum\n","0\n","38 https://pytorch.org/docs/stable/generated/torch.floor_divide.html#torch.floor_divide\n","0\n","39 https://pytorch.org/docs/stable/generated/torch.sgn.html#torch.sgn\n","0\n","40 https://pytorch.org/docs/stable/generated/torch.ones_like.html#torch.ones_like\n","0\n","41 https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad\n","0\n","42 https://pytorch.org/docs/stable/generated/torch.get_num_threads.html#torch.get_num_threads\n","0\n","43 https://pytorch.org/docs/stable/generated/torch.view_as_complex.html#torch.view_as_complex\n","0\n","44 https://pytorch.org/docs/stable/generated/torch.swapaxes.html#torch.swapaxes\n","0\n","45 https://pytorch.org/docs/stable/generated/torch.logical_xor.html#torch.logical_xor\n","0\n","46 https://pytorch.org/docs/stable/generated/torch.clip.html#torch.clip\n","0\n","47 https://pytorch.org/docs/stable/generated/torch.arcsinh.html#torch.arcsinh\n","0\n","48 https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype\n","0\n","49 https://pytorch.org/docs/stable/generated/torch.absolute.html#torch.absolute\n","0\n","50 https://pytorch.org/docs/stable/generated/torch.median.html#torch.median\n","0\n","51 https://pytorch.org/docs/stable/generated/torch.arcsin.html#torch.arcsin\n","0\n","52 https://pytorch.org/docs/stable/onnx.html\n","0\n","53 https://pytorch.org/docs/stable/generated/torch.tril_indices.html#torch.tril_indices\n","0\n","54 https://pytorch.org/docs/stable/generated/torch.sub.html#torch.sub\n","0\n","55 https://pytorch.org/docs/stable/amp.html\n","0\n","56 https://pytorch.org/docs/stable/generated/torch.logdet.html#torch.logdet\n","0\n","57 https://pytorch.org/resources\n","0\n","58 https://pytorch.org/docs/stable/generated/torch.lt.html#torch.lt\n","0\n","59 https://pytorch.org/docs/stable/generated/torch.quantize_per_tensor.html#torch.quantize_per_tensor\n","0\n","60 https://pytorch.org/docs/stable/generated/torch.arctan.html#torch.arctan\n","0\n","61 https://pytorch.org/docs/stable/generated/torch.square.html#torch.square\n","0\n","62 https://pytorch.org/docs/stable/generated/torch.take.html#torch.take\n","0\n","63 https://pytorch.org/docs/stable/generated/torch.arccosh.html#torch.arccosh\n","0\n","64 https://pytorch.org/docs/stable/torch.html#torch.torch.default_generator\n","0\n","65 https://pytorch.org/docs/stable/generated/torch.normal.html#torch.normal\n","0\n","66 https://pytorch.org/docs/stable/generated/torch.std_mean.html#torch.std_mean\n","0\n","67 https://pytorch.org/docs/stable/generated/torch.swapdims.html#torch.swapdims\n","0\n","68 https://pytorch.org/docs/stable/generated/torch.dstack.html#torch.dstack\n","0\n","69 https://pytorch.org/docs/stable/special.html#torch.special.erfinv\n","0\n","70 https://pytorch.org/docs/stable/generated/torch.isneginf.html#torch.isneginf\n","0\n","71 https://pytorch.org/docs/stable/generated/torch.tensordot.html#torch.tensordot\n","0\n","72 https://pytorch.org/docs/stable/generated/torch.clamp.html#torch.clamp\n","0\n","73 https://pytorch.org/docs/stable/generated/torch.renorm.html#torch.renorm\n","0\n","74 https://pytorch.org/docs/stable/generated/torch.logcumsumexp.html#torch.logcumsumexp\n","0\n","75 https://pytorch.org/docs/stable/generated/torch.scatter.html#torch.scatter\n","0\n","76 https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat\n","0\n","77 https://pytorch.org/docs/stable/generated/torch.cdist.html#torch.cdist\n","0\n","78 https://pytorch.org/docs/stable/generated/torch.index_select.html#torch.index_select\n","0\n","79 https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc\n","0\n","80 https://pytorch.org/docs/stable/generated/torch.cholesky_solve.html#torch.cholesky_solve\n","0\n","81 https://pytorch.org/docs/stable/generated/torch.argmin.html#torch.argmin\n","0\n","82 https://pytorch.org/docs/stable/distributions.html\n","0\n","83 https://pytorch.org/docs/stable/generated/torch.quasirandom.SobolEngine.html#torch.quasirandom.SobolEngine\n","0\n","84 https://pytorch.org/docs/stable/generated/torch.unique_consecutive.html#torch.unique_consecutive\n","0\n","85 https://pytorch.org/docs/stable/generated/torch.signbit.html#torch.signbit\n","0\n","86 https://pytorch.org/docs/stable/generated/torch.isinf.html#torch.isinf\n","0\n","87 https://pytorch.org/docs/stable/generated/torch.log1p.html#torch.log1p\n","0\n","88 https://pytorch.org/docs/stable/generated/torch.greater.html#torch.greater\n","0\n","89 https://pytorch.org/docs/stable/generated/torch.randperm.html#torch.randperm\n","0\n","90 https://pytorch.org/docs/stable/generated/torch.erfc.html#torch.erfc\n","0\n","91 https://pytorch.org/docs/stable/generated/torch.diff.html#torch.diff\n","0\n","92 https://pytorch.org/docs/stable/generated/torch.bernoulli.html#torch.bernoulli\n","0\n","93 https://pytorch.org/docs/stable/generated/torch.amin.html#torch.amin\n","0\n","94 https://pytorch.org/docs/stable/generated/torch.equal.html#torch.equal\n","0\n","95 https://pytorch.org/docs/stable/generated/torch.logical_not.html#torch.logical_not\n","0\n","96 https://pytorch.org/docs/stable/generated/torch.inverse.html#torch.inverse\n","0\n","97 https://pytorch.org/docs/stable/generated/torch.blackman_window.html#torch.blackman_window\n","0\n","98 https://pytorch.org/docs/stable/_sources/torch.rst.txt\n","0\n","99 https://pytorch.org/docs/stable/generated/torch.erf.html#torch.erf\n","0\n","100 https://pytorch.org/docs/stable/torch.html#spectral-ops\n","0\n","101 https://pytorch.org/docs/stable/generated/torch.acos.html#torch.acos\n","0\n","102 https://pytorch.org/docs/stable/generated/torch.subtract.html#torch.subtract\n","0\n","103 https://pytorch.org/docs/stable/generated/torch.chunk.html#torch.chunk\n","0\n","104 https://pytorch.org/docs/stable/torch.html#parallelism\n","0\n","105 https://pytorch.org/docs/stable/generated/torch.abs.html#torch.abs\n","0\n","106 https://pytorch.org/audio/stable\n","0\n","107 https://pytorch.org/docs/stable/generated/torch.from_numpy.html#torch.from_numpy\n","0\n","108 https://pytorch.org/docs/stable/generated/torch.ravel.html#torch.ravel\n","0\n","109 https://pytorch.org/docs/stable/generated/torch.sinc.html#torch.sinc\n","0\n","110 https://pytorch.org/docs/stable/benchmark_utils.html\n","0\n","111 https://pytorch.org/docs/stable/generated/torch.save.html#torch.save\n","0\n","112 https://pytorch.org/docs/stable/generated/torch.set_grad_enabled.html#torch.set_grad_enabled\n","0\n","113 https://pytorch.org/docs/stable/generated/torch.combinations.html#torch.combinations\n","0\n","114 https://pytorch.org/docs/stable/generated/torch.mv.html#torch.mv\n","0\n","115 https://pytorch.org/docs/stable/generated/torch.amax.html#torch.amax\n","0\n","116 https://pytorch.org/docs/stable/generated/torch.ger.html#torch.ger\n","0\n","117 https://pytorch.org/docs/stable/generated/torch.numel.html#torch.numel\n","0\n","118 https://pytorch.org/docs/stable/generated/torch.logspace.html#torch.logspace\n","0\n","119 https://pytorch.org/docs/stable/generated/torch.acosh.html#torch.acosh\n","0\n","120 https://pytorch.org/docs/stable/package.html\n","0\n","121 https://pytorch.org/docs/stable/generated/torch.cumsum.html#torch.cumsum\n","0\n","122 https://pytorch.org/docs/stable/generated/torch.linalg.matrix_power.html#torch.linalg.matrix_power\n","0\n","123 https://pytorch.org/docs/stable/torch.html#quasi-random-sampling\n","0\n","124 https://pytorch.org/docs/stable/profiler.html\n","0\n","125 https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique\n","0\n","126 https://pytorch.org/docs/stable/generated/torch.asin.html#torch.asin\n","0\n","127 https://pytorch.org/docs/stable/optim.html\n","0\n","128 https://pytorch.org/docs/stable/generated/torch.Tensor.random_.html#torch.Tensor.random_\n","0\n","129 https://pytorch.org/docs/stable/generated/torch.nanquantile.html#torch.nanquantile\n","0\n","130 https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig\n","0\n","131 https://pytorch.org/docs/stable/generated/torch.sigmoid.html#torch.sigmoid\n","0\n","132 https://pytorch.org/docs/stable/generated/torch.gcd.html#torch.gcd\n","0\n","133 https://pytorch.org/docs/stable/generated/torch.matrix_exp.html#torch.matrix_exp\n","0\n","134 https://pytorch.org/docs/stable/generated/torch.is_floating_point.html#torch.is_floating_point\n","0\n","135 https://pytorch.org/docs/stable/nn.html\n","0\n","136 https://pytorch.org/docs/stable/hub.html\n","0\n","137 https://pytorch.org/docs/stable/generated/torch.log10.html#torch.log10\n","0\n","138 https://pytorch.org/docs/stable/generated/torch.fmin.html#torch.fmin\n","0\n","139 https://pytorch.org/docs/stable/generated/torch.outer.html#torch.outer\n","0\n","140 https://pytorch.org/docs/stable/generated/torch.tan.html#torch.tan\n","0\n","141 https://pytorch.org/docs/stable/generated/torch.addr.html#torch.addr\n","0\n","142 https://twitter.com/pytorch\n","0\n","143 https://pytorch.org/docs/stable/generated/torch.cumprod.html#torch.cumprod\n","0\n","144 https://pytorch.org/docs/stable/generated/torch.inner.html#torch.inner\n","0\n","145 https://pytorch.org/docs/stable/generated/torch.isclose.html#torch.isclose\n","0\n","146 https://pytorch.org/docs/stable/generated/torch.rsqrt.html#torch.rsqrt\n","0\n","147 https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted\n","0\n","148 https://pytorch.org/docs/stable/generated/torch.block_diag.html#torch.block_diag\n","0\n","149 https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv\n","0\n","150 https://pytorch.org/docs/stable/backends.html\n","0\n","151 https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md\n","0\n","152 https://pytorch.org/docs/stable/generated/torch.bitwise_or.html#torch.bitwise_or\n","0\n","153 https://pytorch.org/docs/stable/generated/torch.igammac.html#torch.igammac\n","0\n","154 https://pytorch.org/docs/stable/fft.html\n","0\n","155 https://pytorch.org/docs/stable/generated/torch.randint.html#torch.randint\n","0\n","156 https://pytorch.org/docs/stable/generated/torch.masked_select.html#torch.masked_select\n","0\n","157 https://pytorch.org/docs/stable/generated/torch.randn_like.html#torch.randn_like\n","0\n","158 https://pytorch.org/docs/stable/torch.html#tensors\n","0\n","159 https://pytorch.org/docs/stable/generated/torch.mm.html#torch.mm\n","0\n","160 https://pytorch.org/docs/stable/generated/torch.hamming_window.html#torch.hamming_window\n","0\n","161 https://pytorch.org/docs/stable/generated/torch.movedim.html#torch.movedim\n","0\n","162 https://pytorch.org/docs/stable/generated/torch.addmm.html#torch.addmm\n","0\n","163 https://pytorch.org/docs/stable/torch.html#serialization\n","0\n","164 https://pytorch.org/docs/stable/generated/torch.prod.html#torch.prod\n","0\n","165 https://pytorch.org/docs/stable/generated/torch.exp2.html#torch.exp2\n","0\n","166 https://pytorch.org/docs/stable/generated/torch.where.html#torch.where\n","0\n","167 https://pytorch.org/docs/stable/generated/torch.empty_like.html#torch.empty_like\n","0\n","168 https://pytorch.org/docs/stable/generated/torch.bmm.html#torch.bmm\n","0\n","169 https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn\n","0\n","170 https://pytorch.org/docs/stable/generated/torch.poisson.html#torch.poisson\n","0\n","171 https://pytorch.org/docs/stable/generated/torch.vstack.html#torch.vstack\n","0\n","172 https://pytorch.org/docs/stable/generated/torch.set_warn_always.html#torch.set_warn_always\n","0\n","173 https://pytorch.org/docs/stable/generated/torch.randint_like.html#torch.randint_like\n","0\n","174 https://pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape\n","0\n","175 https://pytorch.org/docs/stable/generated/torch.bucketize.html#torch.bucketize\n","0\n","176 https://pytorch.org/docs/stable/generated/torch.scatter_add.html#torch.scatter_add\n","0\n","177 https://pytorch.org/docs/stable/cpp_index.html\n","0\n","178 https://pytorch.org/docs/stable/generated/torch.copysign.html#torch.copysign\n","0\n","179 https://pytorch.org/docs/stable/generated/torch._assert.html#torch._assert\n","0\n","180 https://pytorch.org/docs/stable/generated/torch.kthvalue.html#torch.kthvalue\n","0\n","181 https://pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky\n","0\n","182 https://pytorch.org/docs/stable/generated/torch.histc.html#torch.histc\n","0\n","183 https://pytorch.org/docs/stable/torch.html#creation-ops\n","0\n","184 https://pytorch.org/docs/stable/torch.html#torch\n","0\n","185 https://pytorch.org/text/stable\n","0\n","186 https://pytorch.org/docs/stable/generated/torch.eq.html#torch.eq\n","0\n","187 https://pytorch.org/docs/stable/torch.html#utilities\n","0\n","188 https://pytorch.org/docs/stable/generated/torch.unsqueeze.html#torch.unsqueeze\n","0\n","189 https://pytorch.org/docs/stable/torch.html#random-sampling\n","0\n","190 https://pytorch.org/docs/stable/generated/torch.matrix_rank.html#torch.matrix_rank\n","0\n","191 https://pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax\n","0\n","192 https://pytorch.org/docs/stable/generated/torch.imag.html#torch.imag\n","0\n","193 https://pytorch.org/docs/stable/generated/torch.exp.html#torch.exp\n","0\n","194 https://pytorch.org/docs/stable/generated/torch.initial_seed.html#torch.initial_seed\n","0\n","195 https://pytorch.org/docs/stable/generated/torch.Tensor.cauchy_.html#torch.Tensor.cauchy_\n","0\n","196 https://pytorch.org/docs/stable/generated/torch.angle.html#torch.angle\n","0\n","197 https://pytorch.org/docs/stable/special.html#torch.special.exp2\n","0\n","198 https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_\n","0\n","199 https://pytorch.org/docs/stable/generated/torch.range.html#torch.range\n","0\n","200 https://pytorch.org/docs/stable/generated/torch.broadcast_to.html#torch.broadcast_to\n","0\n","201 https://pytorch.org/docs/stable/generated/torch.is_complex.html#torch.is_complex\n","0\n","202 https://pytorch.org/docs/stable/generated/torch.squeeze.html#torch.squeeze\n","0\n","203 https://pytorch.org/ecosystem\n","0\n","204 https://pytorch.org/docs/stable/random.html\n","0\n","205 https://pytorch.org/docs/stable/generated/torch.ormqr.html#torch.ormqr\n","0\n","206 https://pytorch.org/docs/stable/generated/torch.pinverse.html#torch.pinverse\n","0\n","207 https://pytorch.org/docs/stable/notes/randomness.html\n","0\n","208 https://pytorch.org/docs/stable/generated/torch.det.html#torch.det\n","0\n","209 https://pytorch.org/docs/stable/generated/torch.dsplit.html#torch.dsplit\n","0\n","210 https://pytorch.org/docs/stable/generated/torch.can_cast.html#torch.can_cast\n","0\n","211 https://pytorch.org/features\n","0\n","212 https://pytorch.org/docs/stable/generated/torch.linalg.slogdet.html#torch.linalg.slogdet\n","0\n","213 https://pytorch.org/docs/stable/generated/torch.qr.html#torch.qr\n","0\n","214 https://pytorch.org/docs/stable/generated/torch.nan_to_num.html#torch.nan_to_num\n","0\n","215 https://pytorch.org/docs/stable/generated/torch.slogdet.html#torch.slogdet\n","0\n","216 https://pytorch.org/docs/stable/generated/torch.div.html#torch.div\n","0\n","217 https://pytorch.org/docs/stable/generated/torch.fix.html#torch.fix\n","0\n","218 https://pytorch.org/docs/stable/jit.html\n","0\n","219 https://pytorch.org/vision/stable/index.html\n","0\n","220 https://pytorch.org/docs/stable/generated/torch.less_equal.html#torch.less_equal\n","0\n","221 https://pytorch.org/docs/stable/generated/torch.column_stack.html#torch.column_stack\n","0\n","222 https://pytorch.org/docs/stable/generated/torch.is_tensor.html\n","0\n","223 https://pytorch.org/docs/stable/generated/torch.set_flush_denormal.html#torch.set_flush_denormal\n","0\n","224 https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum\n","0\n","225 https://pytorch.org/docs/stable/generated/torch.lerp.html#torch.lerp\n","0\n","226 https://pytorch.org/docs/stable/generated/torch.maximum.html#torch.maximum\n","0\n","227 https://pytorch.org/docs/stable/generated/torch.bartlett_window.html#torch.bartlett_window\n","0\n","228 https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm\n","0\n","229 https://pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros\n","0\n","230 https://pytorch.org/docs/stable/generated/torch.arctanh.html#torch.arctanh\n","0\n","231 https://pytorch.org/docs/stable/generated/torch.any.html#torch.any\n","0\n","232 https://pytorch.org/audio/stable/index.html\n","0\n","233 https://pytorch.org/docs/stable/generated/torch.lu_solve.html#torch.lu_solve\n","0\n","234 https://github.com/rtfd/sphinx_rtd_theme\n","0\n","235 https://pytorch.org/docs/stable/generated/torch.argsort.html#torch.argsort\n","0\n","236 https://pytorch.org/docs/stable/generated/torch.cummax.html#torch.cummax\n","0\n","237 https://pytorch.org/docs/stable/generated/torch.gather.html#torch.gather\n","0\n","238 https://www.facebook.com/pytorch\n","0\n","239 https://pytorch.org/docs/stable/generated/torch.isreal.html#torch.isreal\n","0\n","240 https://pytorch.org/docs/stable/nn.init.html\n","0\n","241 https://pytorch.org/docs/stable/generated/torch.addbmm.html#torch.addbmm\n","0\n","242 https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv\n","0\n","243 https://pytorch.org/docs/stable/generated/torch.result_type.html#torch.result_type\n","0\n","244 https://pytorch.org/docs/stable/generated/torch.set_default_dtype.html#torch.set_default_dtype\n","0\n","245 https://pytorch.org/docs/stable/generated/torch.cummin.html#torch.cummin\n","0\n","246 https://pytorch.org/docs/stable/generated/torch.mode.html#torch.mode\n","0\n","247 https://pytorch.org/docs/stable/generated/torch.not_equal.html#torch.not_equal\n","0\n","248 https://pytorch.org/docs/stable/generated/torch.is_tensor.html#torch.is_tensor\n","0\n","249 https://pytorch.org/docs/stable/distributed.optim.html\n","0\n","250 https://pytorch.org/serve/\n","0\n","251 https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft\n","0\n","252 https://pytorch.org/docs/stable/generated/torch.arange.html#torch.arange\n","0\n","253 https://pytorch.org/docs/stable/generated/torch.hypot.html#torch.hypot\n","0\n","254 https://pytorch.org/docs/stable/torch.html#in-place-random-sampling\n","0\n","255 https://pytorch.org/docs/stable/generated/torch.as_strided.html#torch.as_strided\n","0\n","256 https://pytorch.org/docs/stable/generated/torch.transpose.html#torch.transpose\n","0\n","257 https://pytorch.org/docs/stable/generated/torch.cross.html#torch.cross\n","0\n","258 https://pytorch.org/docs/stable/generated/torch.rot90.html#torch.rot90\n","0\n","259 https://pytorch.org/docs/stable/torch.html#pointwise-ops\n","0\n","260 https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_\n","0\n","261 https://pytorch.org/docs/stable/generated/torch.triu.html#torch.triu\n","0\n","262 https://pytorch.org/docs/stable/generated/torch.polygamma.html#torch.polygamma\n","0\n","263 https://pytorch.org/docs/stable/generated/torch.var.html#torch.var\n","0\n","264 https://pytorch.org/docs/stable/special.html#torch.special.erf\n","0\n","265 https://pytorch.org/docs/stable/generated/torch.allclose.html#torch.allclose\n","0\n","266 https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n","0\n","267 https://pytorch.org/docs/stable/generated/torch.arccos.html#torch.arccos\n","0\n","268 https://pytorch.org/docs/stable/generated/torch.count_nonzero.html#torch.count_nonzero\n","0\n","269 https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n","0\n","270 https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid\n","0\n","271 https://pytorch.org/docs/stable/generated/torch.get_num_interop_threads.html#torch.get_num_interop_threads\n","0\n","272 https://pytorch.org/docs/stable/generated/torch.is_nonzero.html#torch.is_nonzero\n","0\n","273 https://pytorch.org/docs/stable/tensors.html\n","0\n","274 https://pytorch.org/docs/stable/generated/torch.logit.html#torch.logit\n","0\n","275 https://pytorch.org/docs/stable/sparse.html\n","0\n","276 https://pytorch.org/docs/stable/generated/torch.sin.html#torch.sin\n","0\n","277 https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like\n","0\n","278 https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft\n","0\n","279 https://pytorch.org/docs/stable/generated/torch.gradient.html#torch.gradient\n","0\n","280 https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount\n","0\n","281 https://pytorch.org/docs/stable/generated/torch.set_rng_state.html#torch.set_rng_state\n","0\n","282 https://pytorch.org/docs/stable/generated/torch.conj.html#torch.conj\n","0\n","283 https://pytorch.org/docs/stable/generated/torch.positive.html#torch.positive\n","0\n","284 https://pytorch.org/docs/stable/generated/torch.atan.html#torch.atan\n","0\n","285 https://pytorch.org/docs/stable/generated/torch.row_stack.html#torch.row_stack\n","0\n","286 https://pytorch.org/docs/stable/generated/torch.Tensor.normal_.html#torch.Tensor.normal_\n","0\n","287 https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor\n","0\n","288 https://pytorch.org/#community-module\n","0\n","289 https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices\n","0\n","290 https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder\n","0\n","291 https://pytorch.org/docs/stable/generated/torch.seed.html#torch.seed\n","0\n","292 https://pytorch.org/docs/stable/generated/torch.divide.html#torch.divide\n","0\n","293 https://pytorch.org/docs/stable/generated/torch.set_num_threads.html#torch.set_num_threads\n","0\n","294 https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye\n","0\n","295 https://pytorch.org/docs/stable/generated/torch.erfinv.html#torch.erfinv\n","0\n","296 https://pytorch.org/docs/stable/generated/torch.trunc.html#torch.trunc\n","0\n","297 https://pytorch.org/docs/stable/generated/torch.log2.html#torch.log2\n","0\n","298 https://pytorch.org/docs/stable/generated/torch.expm1.html#torch.expm1\n","0\n","299 https://pytorch.org/vision/stable\n","0\n","300 https://pytorch.org/elastic/\n","0\n","301 https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window\n","0\n","302 https://pytorch.org/docs/stable/linalg.html\n","0\n","303 https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor\n","0\n","304 https://pytorch.org/docs/stable/generated/torch.logical_or.html#torch.logical_or\n","0\n","305 https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve\n","0\n","306 https://pytorch.org/docs/stable/cuda.html\n","0\n","307 https://pytorch.org/docs/stable/special.html#torch.special.logit\n","0\n","308 https://pytorch.org/docs/stable/__config__.html\n","0\n","309 https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander\n","0\n","310 https://pytorch.org/docs/stable/distributed.elastic.html\n","0\n","311 https://pytorch.org/docs/stable/generated/torch.frexp.html#torch.frexp\n","0\n","312 https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv\n","0\n","313 https://pytorch.org/docs/stable/generated/torch.moveaxis.html#torch.moveaxis\n","0\n","314 https://pytorch.org/docs/stable/generated/torch.lgamma.html#torch.lgamma\n","0\n","315 https://pytorch.org/docs/stable/futures.html\n","0\n","316 https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod\n","0\n","317 https://pytorch.org/docs/stable/generated/torch.dist.html#torch.dist\n","0\n","318 https://pytorch.org/docs/stable/generated/torch.multiply.html#torch.multiply\n","0\n","319 https://pytorch.org/docs/stable/generated/torch.empty.html#torch.empty\n","0\n","320 https://pytorch.org/docs/stable/generated/torch.nansum.html#torch.nansum\n","0\n","321 https://pytorch.org/docs/stable/generated/torch.diagonal.html#torch.diagonal\n","0\n","322 https://pytorch.org/docs/stable/generated/torch.le.html#torch.le\n","0\n","323 https://pytorch.org/docs/stable/torch.html#locally-disabling-gradient-computation\n","0\n","324 https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand\n","0\n","325 https://pytorch.org/docs/stable/generated/torch.take_along_dim.html#torch.take_along_dim\n","0\n","326 https://pytorch.org/docs/stable/generated/torch.isposinf.html#torch.isposinf\n","0\n","327 https://pytorch.org/docs/stable/generated/torch.logical_and.html#torch.logical_and\n","0\n","328 https://pytorch.org/docs/stable/generated/torch.neg.html#torch.neg\n","0\n","329 https://pytorch.org/docs/stable/generated/torch.trace.html#torch.trace\n","0\n","330 https://pytorch.org/docs/stable/distributed.html\n","0\n","331 https://pytorch.org/docs/stable/index.html\n","0\n","332 https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray\n","0\n","333 https://pytorch.org/docs/stable/generated/torch.ceil.html#torch.ceil\n","0\n","334 https://pytorch.org/docs/stable/torch.html#comparison-ops\n","0\n","335 https://pytorch.org/docs/stable/generated/torch.linalg.det.html#torch.linalg.det\n","0\n","336 https://pytorch.org/docs/stable/generated/torch.nonzero.html#torch.nonzero\n","0\n","337 https://pytorch.org/\n","0\n","338 https://pytorch.org/docs/stable/generated/torch.reciprocal.html#torch.reciprocal\n","0\n","339 https://pytorch.org/docs/stable/generated/torch.is_warn_always_enabled.html#torch.is_warn_always_enabled\n","0\n","340 https://pytorch.org/docs/stable/torch.html#\n","0\n","341 https://pytorch.org/docs/stable/generated/torch.dequantize.html#torch.dequantize\n","0\n","342 https://pytorch.org/docs/stable/generated/torch.sort.html#torch.sort\n","0\n","343 https://pytorch.org/docs/stable/generated/torch.float_power.html#torch.float_power\n","0\n","344 https://pytorch.org/docs/stable/generated/torch.diag_embed.html#torch.diag_embed\n","0\n","345 https://pytorch.org/docs/stable/generated/torch.frac.html#torch.frac\n","0\n","346 https://pytorch.org/docs/stable/generated/torch.linalg.householder_product.html#torch.linalg.householder_product\n","0\n","347 https://pytorch.org/docs/stable/generated/torch.tril.html#torch.tril\n","0\n","348 https://pytorch.org/docs/stable/generated/torch.i0.html#torch.i0\n","0\n","349 https://pytorch.org/docs/stable/generated/torch.orgqr.html#torch.orgqr\n","0\n","350 https://pytorch.org/docs/stable/generated/torch.hstack.html#torch.hstack\n","0\n","351 https://pytorch.org/docs/stable/mobile_optimizer.html\n","0\n","352 https://pytorch.org/docs/stable/generated/torch.set_printoptions.html#torch.set_printoptions\n","0\n","353 https://pytorch.org/docs/stable/generated/torch.pow.html#torch.pow\n","0\n","354 https://pytorch.org/docs/stable/special.html\n","0\n","355 https://pytorch.org/docs/stable/generated/torch.floor.html#torch.floor\n","0\n","356 https://pytorch.org/text/stable/index.html\n","0\n","357 https://pytorch.org/mobile\n","0\n","358 https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode\n","0\n","359 https://pytorch.org/docs/stable/generated/torch.as_tensor.html#torch.as_tensor\n","0\n","360 https://pytorch.org/docs/stable/generated/torch.trapz.html#torch.trapz\n","0\n","361 https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul\n","0\n","362 https://pytorch.org/docs/stable/generated/torch.greater_equal.html#torch.greater_equal\n","0\n","363 https://pytorch.org/docs/stable/generated/torch.logsumexp.html#torch.logsumexp\n","0\n","364 https://pytorch.org/docs/stable/generated/torch.max.html#torch.max\n","0\n","365 https://pytorch.org/docs/stable/model_zoo.html\n","0\n","366 https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean\n","0\n","367 https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile\n","0\n","368 https://pytorch.org/docs/stable/generated/torch.vdot.html#torch.vdot\n","0\n","369 https://pytorch.org/docs/stable/generated/torch.asinh.html#torch.asinh\n","0\n","370 https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window\n","0\n","371 https://pytorch.org/docs/stable/generated/torch.lstsq.html#torch.lstsq\n","0\n","372 https://pytorch.org/docs/stable/generated/torch.are_deterministic_algorithms_enabled.html#torch.are_deterministic_algorithms_enabled\n","0\n","373 https://pytorch.org/docs/stable/generated/torch.view_as_real.html#torch.view_as_real\n","0\n","374 https://pytorch.org/docs/stable/generated/torch.isfinite.html#torch.isfinite\n","0\n","375 https://pytorch.org/docs/stable/generated/torch.Tensor.geometric_.html#torch.Tensor.geometric_\n","0\n","376 https://pytorch.org/docs/stable/generated/torch.flipud.html#torch.flipud\n","0\n","377 https://pytorch.org/docs/stable/generated/torch.full.html#torch.full\n","0\n","378 https://pytorch.org/docs/stable/generated/torch.Tensor.uniform_.html#torch.Tensor.uniform_\n","0\n","379 https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator\n","0\n","380 https://pytorch.org/docs/stable/generated/torch.is_inference_mode_enabled.html#torch.is_inference_mode_enabled\n","0\n","381 https://pytorch.org/docs/stable/generated/torch.deg2rad.html#torch.deg2rad\n","0\n","382 https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean\n","0\n","383 https://pytorch.org/docs/stable/storage.html\n","0\n","384 https://pytorch.org/docs/stable/generated/torch.complex.html#torch.complex\n","0\n","385 https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve\n","0\n","386 https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones\n","0\n","387 https://pytorch.org/docs/stable/generated/torch.t.html#torch.t\n","0\n","388 https://pytorch.org/docs/stable/special.html#torch.special.expm1\n","0\n","389 https://pytorch.org/docs/stable/generated/torch.gt.html#torch.gt\n","0\n","390 https://pytorch.org/docs/stable/generated/torch.round.html#torch.round\n","0\n","391 https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank\n","0\n","392 https://www.youtube.com/pytorch\n","0\n","393 https://pytorch.org/docs/stable/generated/torch.narrow.html#torch.narrow\n","0\n","394 https://pytorch.org/docs/stable/generated/torch.min.html#torch.min\n","0\n","395 https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf\n","0\n","396 https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial\n","0\n","397 https://pytorch.org/docs/stable/notes/modules.html\n","0\n","398 https://pytorch.org/docs/stable/tensors.html#torch.Tensor\n","0\n","399 https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip\n","0\n","400 https://pytorch.org/docs/stable/special.html#torch.special.erfc\n","0\n","401 https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd\n","0\n","402 https://pytorch.org/docs/stable/generated/torch.real.html#torch.real\n","0\n","403 https://pytorch.org/docs/stable/generated/torch.xlogy.html#torch.xlogy\n","0\n","404 https://pytorch.org/docs/stable/dlpack.html\n","0\n","405 https://pytorch.org/docs/stable/generated/torch.quantize_per_channel.html#torch.quantize_per_channel\n","0\n","406 https://pytorch.org/docs/stable/generated/torch.enable_grad.html#torch.enable_grad\n","0\n","407 https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig\n","0\n","408 https://pytorch.org/docs/stable/data.html\n","0\n","409 https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum\n","0\n","410 https://pytorch.org/docs/stable/generated/torch.bitwise_not.html#torch.bitwise_not\n","0\n","411 https://pytorch.org/docs/stable/generated/torch.vsplit.html#torch.vsplit\n","0\n","412 https://pytorch.org/docs/stable/generated/torch.hsplit.html#torch.hsplit\n","0\n","413 https://pytorch.org/docs/stable/special.html#torch.special.expit\n","0\n","414 https://pytorch.org/docs/stable/generated/torch.fliplr.html#torch.fliplr\n","0\n","415 https://pytorch.org/javadoc/\n","0\n","416 https://pytorch.org/docs/stable/generated/torch.cosh.html#torch.cosh\n","0\n","417 https://pytorch.org/docs/stable/generated/torch.rad2deg.html#torch.rad2deg\n","0\n","418 https://pytorch.org/docs/stable/generated/torch.Tensor.log_normal_.html#torch.Tensor.log_normal_\n","0\n","419 https://pytorch.org/docs/stable/generated/torch.broadcast_shapes.html#torch.broadcast_shapes\n","0\n","420 https://pytorch.org/docs/stable/generated/torch.get_default_dtype.html#torch.get_default_dtype\n","0\n","421 https://pytorch.org/docs/stable/generated/torch.cartesian_prod.html#torch.cartesian_prod\n","0\n","422 https://pytorch.org/docs/stable/torch.html#generators\n","0\n","423 https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like\n","0\n","424 https://pytorch.org/docs/stable/generated/torch.flatten.html#torch.flatten\n","0\n","425 https://pytorch.org/docs/stable/generated/torch.bitwise_and.html#torch.bitwise_and\n","0\n","426 https://pytorch.org/docs/stable/generated/torch.clone.html#torch.clone\n","0\n","427 https://pytorch.org/docs/stable/generated/torch.digamma.html#torch.digamma\n","0\n","428 https://pytorch.org/docs/stable/torch.html#indexing-slicing-joining-mutating-ops\n","0\n","429 https://pytorch.org/serve\n","0\n","430 https://pytorch.org/docs/stable/fx.html\n","0\n","431 https://pytorch.org/docs/stable/generated/torch.split.html#torch.split\n","0\n","432 https://pytorch.org/docs/stable/generated/torch.sign.html#torch.sign\n","0\n","433 https://pytorch.org/docs/stable/cpp_extension.html\n","0\n","434 https://pytorch.org/docs/stable/torch.html#other-operations\n","0\n","435 https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave\n","0\n","436 https://pytorch.org/docs/stable/generated/torch.add.html#torch.add\n","0\n","437 https://pytorch.org/docs/stable/generated/torch.get_rng_state.html#torch.get_rng_state\n","0\n","438 https://pytorch.org/docs/stable/generated/torch.kron.html#torch.kron\n","0\n","439 https://pytorch.org/docs/stable/generated/torch.polar.html#torch.polar\n","0\n","440 https://pytorch.org/docs/stable/generated/torch.logaddexp2.html#torch.logaddexp2\n","0\n","441 https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms\n","0\n","442 https://pytorch.org/docs/stable/generated/torch.addcmul.html#torch.addcmul\n","0\n","443 https://pytorch.org/docs/stable/generated/torch.fmax.html#torch.fmax\n","0\n","444 https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split\n","0\n","445 https://pytorch.org/docs/stable/generated/torch.rand_like.html#torch.rand_like\n","0\n","446 https://pytorch.org/docs/stable/generated/torch.std.html#torch.std\n","0\n","447 https://pytorch.org/docs/stable/generated/torch.cos.html#torch.cos\n","0\n","448 https://pytorch.org/docs/stable/quantization.html\n","0\n","449 https://pytorch.org/docs/stable/generated/torch.is_grad_enabled.html#torch.is_grad_enabled\n","0\n","450 https://pytorch.org/docs/stable/generated/torch.fake_quantize_per_tensor_affine.html#torch.fake_quantize_per_tensor_affine\n","0\n","451 https://pytorch.org/docs/stable/generated/torch.Tensor.bernoulli_.html#torch.Tensor.bernoulli_\n","0\n","452 https://pytorch.org/docs/stable/generated/torch.baddbmm.html#torch.baddbmm\n","0\n","453 https://pytorch.org/docs/stable/generated/torch.ge.html#torch.ge\n","0\n","454 https://pytorch.org/docs/stable/generated/torch.all.html#torch.all\n","0\n","455 https://pytorch.org/docs/stable/generated/torch.dot.html#torch.dot\n","0\n","456 https://pytorch.org/docs/stable/tensorboard.html\n","0\n","457 https://pytorch.org/docs/stable/generated/torch.linspace.html#torch.linspace\n","0\n","458 https://pytorch.org/docs/stable/generated/torch.lcm.html#torch.lcm\n","0\n","459 https://pytorch.org/docs/stable/autograd.html\n","0\n","460 https://pytorch.org/docs/stable/generated/torch.chain_matmul.html#torch.chain_matmul\n","0\n","461 https://pytorch.org/docs/stable/generated/torch.tanh.html#torch.tanh\n","0\n","462 https://pytorch.org/docs/stable/generated/torch.atan2.html#torch.atan2\n","0\n","463 https://pytorch.org/docs/stable/torch.html#reduction-ops\n","0\n","464 https://pytorch.org/docs/stable/generated/torch.ne.html#torch.ne\n","0\n","465 https://pytorch.org/tutorials\n","0\n","466 https://pytorch.org/docs/stable/generated/torch.msort.html#torch.msort\n","0\n","467 https://github.com/pytorch/pytorch\n","0\n","468 https://pytorch.org/docs/stable/bottleneck.html\n","0\n","469 https://pytorch.org/docs/stable/checkpoint.html\n","0\n","470 https://pytorch.org/docs/stable/generated/torch.unbind.html#torch.unbind\n","0\n","471 https://pytorch.org/docs/stable/generated/torch.negative.html#torch.negative\n","0\n","472 https://pytorch.org/docs/stable/generated/torch.cholesky_inverse.html#torch.cholesky_inverse\n","0\n","473 https://pytorch.org/docs/stable/generated/torch.compiled_with_cxx11_abi.html#torch.compiled_with_cxx11_abi\n","0\n","474 https://pytorch.org/docs/stable/generated/torch.matrix_power.html#torch.matrix_power\n","0\n","475 https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu\n","0\n","476 https://pytorch.org/docs/stable/generated/torch.igamma.html#torch.igamma\n","0\n","477 https://pytorch.org/docs/stable/generated/torch.load.html#torch.load\n","0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CYJ8_xSqtoWA","executionInfo":{"status":"ok","timestamp":1628683221756,"user_tz":-330,"elapsed":21,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}},"outputId":"d52be501-0bfc-4122-e348-3db0e2f5e7f9"},"source":["len(teamurllist)"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["477"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"n-NsATRxbkAi","executionInfo":{"status":"ok","timestamp":1628683221757,"user_tz":-330,"elapsed":5,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}}},"source":["qalist = []\n","idn = 0"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"yRus9HDh-CI8","executionInfo":{"status":"ok","timestamp":1628683221758,"user_tz":-330,"elapsed":5,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}}},"source":["start_idx = 0\n","end_idx = 478"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jCWA_L0cRZoW"},"source":["##production"]},{"cell_type":"code","metadata":{"id":"kJUANvzPRGlS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628683291288,"user_tz":-330,"elapsed":69535,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}},"outputId":"6b6b1f1b-98f5-4343-b047-d52e2bab0300"},"source":["uniqdict = {}\n","\n","for url in teamurllist[start_idx:end_idx]:\n","  #url = 'https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc' \n","  # 'https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc'\n","  #'https://pytorch.org/docs/stable/autograd.html'\n","  # 'https://pytorch.org/docs/stable/generated/torch.load.html#torch.load'\n","  #print(url)\n","  try:\n","    html = req.urlopen(url).read()\n","  except:\n","    continue\n","  soup = BeautifulSoup(html)\n","\n","\n","  pdict = {}\n","\n","  \n","  divs = soup.find_all('div' ,  class_=\"highlight\") #class_=\"section\") #\n","  \n","\n","  if divs:\n","\n","    for div in divs:\n","      \n","      \n","      \n","      sctn = div.find_parent('div', class_=\"section\")\n","\n","      if sctn:\n","\n","        whichhdr = False\n","        hdrtxt = \"\"\n","        #print('1')\n","        try:\n","          #print('2')\n","          hdr = sctn.find('h1')\n","        \n","          hdrtxt = hdr.get_text().strip()\n","          hdrtxt = re.sub(r\"[^a-zA-Z0-9]+\", ' ', hdrtxt).strip()\n","          #qstnsubject = qstnsubject + hdrtxt + '?'\n","          #print('2a')\n","        except:\n","          try:\n","            #print('3')\n","            hdr = sctn.find('h2')\n","            hdrtxt = hdr.get_text().strip()\n","            hdrtxt = re.sub(r\"[^a-zA-Z0-9]+\", ' ', hdrtxt).strip()\n","            #qstnsubject = qstnsubject + hdrtxt + '?'\n","            #print('3a')\n","          except:\n","            try:\n","              #print('4')\n","              hdr = sctn.find('h3')\n","              hdrtxt = hdr.get_text().strip()\n","              hdrtxt = re.sub(r\"[^a-zA-Z0-9]+\", ' ', hdrtxt).strip()\n","              #qstnsubject = qstnsubject + hdrtxt + '?'\n","              #print('4a')\n","            except:\n","              #print('5')\n","              1 == 1\n","            \n","\n","        #print(div.get('class')[0])\n","      \n","        #if hglts:\n","\n","\n","\n","\n","        #for hglt in hglts:\n","        if 1 == 1:\n","\n","          #qstnsubject = 'How to use ' + hdrtxt + '?'\n","\n","          #prnt = div.find_parent('div', class_=\"highlight-default\")\n","          prnt = div.find_parent('div',  {'class':['highlight-default', 'highlight-python']})\n","\n","          ctxt = \" \"\n","\n","          funcname = \"\"\n","          if prnt:\n","            print('before dts')\n","            dl = prnt.find_parent('dl' ,class_=\"function\")\n","            dlm = prnt.find_parent('dl' ,class_=\"method\")\n","            dlc = prnt.find_parent('dl' ,class_=\"class\")\n","            #dv = prnt.find_parent('div', class_=\"admonition note\")\n","            dv = prnt.find_parent('div', {'class':['admonition note', 'section']})            \n","\n","            if dl:\n","              print('in dl')\n","              dt = dl.find_next('dt')\n","\n","              if dt:\n","                funcname = dt.get('id')\n","                print('dt ',dt.get('id'))\n","            elif dlm:\n","              print('in dl')\n","              dt = dlm.find_next('dt')\n","\n","              if dt:\n","                funcname = dt.get('id')\n","                print('dlmdt ',dt.get('id'))\n","              elif prnt.find_previous_sibling('p') :\n","\n","                pc = prnt.find_previous_sibling('p')\n","\n","                if pc:\n","                  funcname = pc.get_text(strip = True)\n","\n","            elif dlc:\n","              print('in dlc')\n","              pc = prnt.find_previous_sibling('p')\n","\n","              if pc:\n","                funcname = pc.get_text(strip = True)\n","\n","                if re.match(r'Example',funcname ):\n","                  dlct = dlc.find_next('dt')\n","                  funcname = dlct.get('id')\n","    \n","                print('dlcdt ',funcname)\n","            elif dv :\n","              #pc = prnt.find_previous_sibling('p')\n","              dpps = prnt.find_previous_siblings('p')\n","\n","              \"\"\"if pc:\n","                funcname = pc.get_text(strip = True)\n","              print(funcname)\"\"\"\n","\n","              if dpps:\n","                scope = 1\n","                plen = len(dpps)\n","                #print('p len ', plen)\n","                if  plen >= 2:\n","                  scope = 2\n","            \n","                for pp in reversed(dpps[:scope]):\n","                  content = pp.get_text(strip=True)\n","                  if not re.search(r'Example',content):\n","                    funcname = funcname + content\n","                print('from dpps ' , funcname)\n","\n","         \n","            \"\"\"\n","            #print('7')\"\"\"\n","            pps = prnt.find_previous_siblings('p')\n","\n","            if pps:\n","              scope = 1\n","              plen = len(pps)\n","              #print('p len ', plen)\n","              if plen > 2:\n","                scope = 2\n","            \n","            \n","              for pp in reversed (pps[:scope]):\n","                ptext = pp.get_text(strip=True)\n","                if not re.search(r'Example',ptext):\n","                  ctxt = ctxt + ptext\n","                #print('from p ' , ctxt)\n","\n","          localctxt = ctxt\n","          \"\"\"if len(hdrtxt) > 0 : \n","            ctxt = hdrtxt\"\"\"\n","\n","          if len(funcname) > 0:\n","            ctxt = funcname\n","          elif len(hdrtxt) > 0 : \n","            ctxt = hdrtxt\n","\n","          if len(ctxt) < 0:\n","            print('url' , url)\n","          \n","          qstnsubject = 'How to use ' + ctxt +  ', give an example?'\n","\n","          qadict = {}\n","          try:\n","\n","            pr = div.find('pre').text\n","\n","\n","            qadict['Answer'] = pr\n","\n","            if len(qadict['Answer']) > 0: \n","              # and len(qadict['context'])\n","              idn += 1\n","\n","              if qstnsubject in uniqdict:\n","                qstnsubject = 'How ' + localctxt +  ', give an example?'\n","                \n","              uniqdict[qstnsubject] = url\n","\n","              qadict['Question'] = qstnsubject\n","              \n","              #print('6',qstnsubject)\n","\n","              qadict['Id'] = idn\n","              qadict['source'] = url\n","              qadict['context'] = localctxt\n","              qalist.append(qadict)\n","          except:\n","            print('no hglts')\n","            continue\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["before dts\n","in dl\n","dt  torch.atanh\n","before dts\n","in dl\n","dt  torch.logaddexp\n","before dts\n","in dl\n","dt  torch.testing.assert_close\n","before dts\n","in dl\n","dt  torch.testing.assert_close\n","before dts\n","in dl\n","dt  torch.testing.assert_close\n","before dts\n","in dl\n","dt  torch.testing.assert_close\n","before dts\n","in dl\n","dt  torch.testing.assert_close\n","before dts\n","in dl\n","dt  torch.testing.assert_close\n","before dts\n","in dl\n","dt  torch.testing.assert_close\n","before dts\n","in dl\n","dt  torch.testing.assert_close\n","before dts\n","in dl\n","dt  torch.testing.assert_close\n","before dts\n","in dl\n","dt  torch.diag\n","before dts\n","in dl\n","dt  torch.diag\n","before dts\n","from dpps  The context managerstorch.no_grad(),torch.enable_grad(), andtorch.set_grad_enabled()are helpful for locally disabling and enabling\n","gradient computation. SeeLocally disabling gradient computationfor more details on\n","their usage.  These context managers are thread local, so they won’t\n","work if you send work to another thread using thethreadingmodule, etc.\n","before dts\n","in dl\n","dt  torch.overrides.get_ignored_functions\n","before dts\n","in dl\n","dt  torch.overrides.get_testing_overrides\n","before dts\n","in dl\n","dt  torch.overrides.handle_torch_function\n","before dts\n","in dl\n","dt  torch.overrides.is_tensor_like\n","before dts\n","in dl\n","dt  torch.overrides.is_tensor_like\n","before dts\n","in dl\n","dt  torch.overrides.is_tensor_like\n","before dts\n","in dl\n","dt  torch.overrides.is_tensor_method_or_property\n","before dts\n","in dl\n","dt  torch.overrides.wrap_torch_function\n","before dts\n","in dl\n","dt  torch.mvlgamma\n","before dts\n","in dl\n","dt  torch.fake_quantize_per_channel_affine\n","before dts\n","in dl\n","dt  torch.atleast_1d\n","before dts\n","in dl\n","dt  torch.heaviside\n","before dts\n","in dl\n","dt  torch.atleast_2d\n","before dts\n","in dl\n","dt  torch.ldexp\n","before dts\n","in dl\n","dt  torch.promote_types\n","before dts\n","in dl\n","dt  torch.nextafter\n","before dts\n","in dl\n","dt  torch.set_default_tensor_type\n","before dts\n","in dl\n","dt  torch.diagflat\n","before dts\n","in dl\n","dt  torch.lu_unpack\n","before dts\n","in dl\n","dt  torch.addmv\n","before dts\n","in dl\n","dt  torch.nanmedian\n","before dts\n","in dl\n","dt  torch.nanmedian\n","before dts\n","in dl\n","dt  torch.empty_strided\n","before dts\n","in dl\n","dt  torch.isnan\n","before dts\n","in dl\n","dt  torch.atleast_3d\n","before dts\n","in dl\n","dt  torch.bitwise_xor\n","before dts\n","in dl\n","dt  torch.sinh\n","before dts\n","in dl\n","dt  torch.roll\n","before dts\n","in dl\n","dt  torch.tile\n","before dts\n","in dl\n","dt  torch.topk\n","before dts\n","in dl\n","dt  torch.sqrt\n","before dts\n","in dl\n","dt  torch.minimum\n","before dts\n","in dl\n","dt  torch.floor_divide\n","before dts\n","in dl\n","dt  torch.sgn\n","before dts\n","in dl\n","dt  torch.ones_like\n","before dts\n","in dlc\n","dlcdt  torch.no_grad\n","before dts\n","in dl\n","dt  torch.view_as_complex\n","before dts\n","in dl\n","dt  torch.swapaxes\n","before dts\n","in dl\n","dt  torch.logical_xor\n","before dts\n","from dpps  A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\n","non-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\n","values when determining the minimumdtypesof an operand.  Quantized and complex types\n","are not yet supported.\n","before dts\n","from dpps  \n","before dts\n","from dpps  Atorch.devicecan be constructed via a string or via a string and device ordinalVia a string:\n","before dts\n","from dpps  Via a string:Via a string and device ordinal:\n","before dts\n","from dpps  NoteThetorch.deviceargument in functions can generally be substituted with a string.\n","This allows for fast prototyping of code.\n","before dts\n","from dpps  NoteThetorch.deviceargument in functions can generally be substituted with a string.\n","This allows for fast prototyping of code.\n","before dts\n","from dpps  NoteFor legacy reasons, a device can be constructed via a single device ordinal, which is treated\n","as a cuda device.  This matchesTensor.get_device(), which returns an ordinal for cuda\n","tensors and is not supported for cpu tensors.\n","before dts\n","from dpps  NoteMethods which take a device will generally accept a (properly formatted) string\n","or (legacy) integer device ordinal, i.e. the following are all equivalent:\n","before dts\n","from dpps  torch.stridedrepresents dense Tensors and is the memory layout that\n","is most commonly used. Each strided tensor has an associatedtorch.Storage, which holds its data. These tensors provide\n","multi-dimensional,stridedview of a storage. Strides are a list of integers: the k-th stride\n","represents the jump in the memory necessary to go from one element to the\n","next one in the k-th dimension of the Tensor. This concept makes it possible\n","to perform many tensor operations efficiently.\n","before dts\n","in dl\n","dt  torch.median\n","before dts\n","in dl\n","dt  torch.median\n","before dts\n","from dpps  Here is a simple script which exports a pretrained AlexNet as defined in\n","torchvision into ONNX.  It runs a single round of inference and then\n","saves the resulting traced model toalexnet.onnx:\n","before dts\n","from dpps  Here is a simple script which exports a pretrained AlexNet as defined in\n","torchvision into ONNX.  It runs a single round of inference and then\n","saves the resulting traced model toalexnet.onnx:The resultingalexnet.onnxis a binary protobuf file which contains both\n","the network structure and parameters of the model you exported\n","(in this case, AlexNet).  The keyword argumentverbose=Truecauses the\n","exporter to print out a human-readable representation of the network:\n","before dts\n","from dpps  The resultingalexnet.onnxis a binary protobuf file which contains both\n","the network structure and parameters of the model you exported\n","(in this case, AlexNet).  The keyword argumentverbose=Truecauses the\n","exporter to print out a human-readable representation of the network:You can also verify the protobuf using theONNXlibrary.\n","You can installONNXwith conda:\n","before dts\n","from dpps  You can also verify the protobuf using theONNXlibrary.\n","You can installONNXwith conda:Then, you can run:\n","before dts\n","from dpps  To run the exported script withcaffe2, you will need to installcaffe2: If you don’t have one already, Pleasefollow the install instructions.Once these are installed, you can use the backend for Caffe2:\n","before dts\n","from dpps  You can also run the exported model withONNX Runtime,\n","you will need to installONNX Runtime: pleasefollow these instructions.Once these are installed, you can use the backend for ONNX Runtime:\n","before dts\n","from dpps  The ONNX exporter can be bothtrace-basedandscript-basedexporter.We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\n","of a part of a model.  Checkout this example:\n","before dts\n","from dpps  We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\n","of a part of a model.  Checkout this example:Withtrace-basedexporter, we get the result ONNX graph which unrolls the for loop:\n","before dts\n","from dpps  Withtrace-basedexporter, we get the result ONNX graph which unrolls the for loop:To utilizescript-basedexporter for capturing the dynamic loop,\n","we can write the loop in script, and call it from the regular nn.Module:\n","before dts\n","from dpps  To utilizescript-basedexporter for capturing the dynamic loop,\n","we can write the loop in script, and call it from the regular nn.Module:Now the exported ONNX graph becomes:\n","before dts\n","from dpps  Now the exported ONNX graph becomes:The dynamic control flow is captured correctly. We can verify in backends with different loop range.\n","before dts\n","from dpps  The dynamic control flow is captured correctly. We can verify in backends with different loop range.To avoid exporting a variable scalar tensor as a fixed value constant as part of the ONNX model, please\n","avoid use oftorch.Tensor.item(). Torch supports implicit cast of single-element tensors to numbers.\n","E.g.:\n","before dts\n","from dpps  TorchScript only supports a subset of Python types. You can find more details about type annotationhere.Due to optimization purposes, TorchScript only supports variables with single static types for script functions.\n","By default, each variable is assumed to be Tensor. If an argument to a ScriptModule function is not Tensor,\n","its type should be specified using MyPy-style annotations.\n","before dts\n","from dpps  Due to optimization purposes, TorchScript only supports variables with single static types for script functions.\n","By default, each variable is assumed to be Tensor. If an argument to a ScriptModule function is not Tensor,\n","its type should be specified using MyPy-style annotations.If the type annotation is not specified, TorchScript compiler fails with the runtime error below.\n","before dts\n","from dpps  PyTorch models can be written using numpy manipulations, but this is not proper when we convert to the ONNX model.\n","For the trace-based exporter, tracing treats the numpy values as the constant node,\n","therefore it calculates the wrong result if we change the input.\n","So the PyTorch model need implement using torch operators.\n","For example, do not use numpy operators on numpy tensors:\n","before dts\n","from dpps  PyTorch models can be written using numpy manipulations, but this is not proper when we convert to the ONNX model.\n","For the trace-based exporter, tracing treats the numpy values as the constant node,\n","therefore it calculates the wrong result if we change the input.\n","So the PyTorch model need implement using torch operators.\n","For example, do not use numpy operators on numpy tensors:do not convert to numpy types:\n","before dts\n","from dpps  do not convert to numpy types:Always use torch tensors and torch operators: torch.concat, etc.\n","In addition, Dropout layer need defined in init function so that inferencing can handle it properly, i.e.,\n","before dts\n","from dpps  There are two ways to handle models which consist of named parameters or keyword arguments as inputs:For example, in the model:\n","before dts\n","from dpps  Not using a dictionary for the keyword arguments and passing all the inputs in the same order\n","as required by the model\n","before dts\n","from dpps  Using a dictionary to represent the keyword arguments. This dictionary is always passed in\n","addition to the non-keyword arguments and is always the last argument in the args tuple.\n","before dts\n","from dpps  There are two ways of exporting the model:For cases in which there are no keyword arguments, models can be exported with either an\n","empty or no dictionary. For example,\n","before dts\n","from dpps  For cases in which there are no keyword arguments, models can be exported with either an\n","empty or no dictionary. For example,An exception to this rule are cases in which the last input is also of a dictionary type.\n","In these cases it is mandatory to have an empty dictionary as the last argument in the\n","args tuple. For example,\n","before dts\n","from dpps  An exception to this rule are cases in which the last input is also of a dictionary type.\n","In these cases it is mandatory to have an empty dictionary as the last argument in the\n","args tuple. For example,Without the presence of the empty dictionary, the export call assumes that the\n","‘x’ input is intended to represent the optional dictionary consisting of named arguments.\n","In order to prevent this from being an issue a constraint is placed to provide an empty\n","dictionary as the last input in the tuple args in such cases.\n","The new call would look like this.\n","before dts\n","from dpps  This type of indexing occurs on the RHS. Export is supported for ONNX opset version >= 9. E.g.:\n","before dts\n","from dpps  This type of indexing occurs on the RHS. Export is supported for ONNX opset version >= 9. E.g.:Below is the list of supported patterns for RHS indexing.\n","before dts\n","from dpps  Below is the list of supported patterns for RHS indexing.And below is the list of unsupported patterns for RHS indexing.\n","before dts\n","from dpps  In code, this type of indexing occurs on the LHS.\n","Export is supported for ONNX opset version >= 11. E.g.:\n","before dts\n","from dpps  In code, this type of indexing occurs on the LHS.\n","Export is supported for ONNX opset version >= 11. E.g.:Below is the list of supported patterns for LHS indexing.\n","before dts\n","from dpps  Below is the list of supported patterns for LHS indexing.And below is the list of unsupported patterns for LHS indexing.\n","before dts\n","from dpps  If the operator is a non-ATen operator, the symbolic function has to be\n","added in the corresponding PyTorch Function class. Please read the following\n","instructions:Symbolic functions should be implemented in Python. All of these functions interact\n","with Python methods which are implemented via C++-Python bindings,\n","but intuitively the interface they provide looks like this:\n","before dts\n","from dpps  The ONNX graph C++ definition is intorch/csrc/jit/ir/ir.h.Here is an example of handling missing symbolic function foreluoperator.\n","We try to export the model and see the error message as below:\n","before dts\n","from dpps  Here is an example of handling missing symbolic function foreluoperator.\n","We try to export the model and see the error message as below:The export fails because PyTorch does not support exportingeluoperator.\n","We findvirtualTensorelu(constTensor&input,Scalaralpha,boolinplace)constoverride;inVariableType.h. This meanseluis an ATen operator.\n","We check theONNX operator list,\n","and confirm thatEluis standardized in ONNX.\n","We add the following lines tosymbolic_opset9.py:\n","before dts\n","from dpps  Following this tutorialExtending TorchScript with Custom C++ Operators,\n","you can create and register your own custom ops implementation in PyTorch. Here’s how to export such model to ONNX.:\n","before dts\n","from dpps  This mode is used to export all operators as regular ONNX operators. This is the defaultoperator_export_typemode.\n","before dts\n","from dpps  This mode is used to export all operators as ATen ops, and avoid conversion to ONNX.\n","before dts\n","from dpps  To fallback on unsupported ATen operators in ONNX. Supported operators are exported to ONNX regularly.\n","In the following example, aten::triu is not supported in ONNX. Exporter falls back on this operator.\n","before dts\n","from dpps  To export a raw ir.\n","before dts\n","from dpps  This mode can be used to export any operator (ATen or non-ATen) that is not registered and supported in ONNX.\n","Exported falls through and exports the operator as is, as custom op. Exporting custom operators\n","enables users to register and implement the operator as part of their runtime backend.\n","before dts\n","from dpps  The tracer records the example inputs shape in the graph. In case the model should accept\n","inputs of dynamic shape, you can utilize the parameterdynamic_axesin export api.\n","before dts\n","from dpps  No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\n","The exporter will try to figure out the right datatype for scalars.  However for cases that it failed\n","to do so, you will need to manually provide the datatype information.  This often happens with scripted models,\n","where the datatypes are not recorded.  We are trying to improve the datatype\n","propagation in the exporter such that manual changes are not required in the future.\n","before dts\n","from dpps  Yes, this is supported now for ONNX opset version >= 11. ONNX introduced the concept of Sequence in opset 11.\n","Similar to list, Sequence is a data type that contains arbitrary number of Tensors.\n","Associated operators are also introduced in ONNX, such as SequenceInsert, SequenceAt, etc.\n","However, in-place list append within loops is not exportable to ONNX. To implement this, please use inplace\n","add operator.\n","E.g.:\n","before dts\n","from dpps  use_external_data_formatargument in export API enables export of models in ONNX external\n","data format. With this option enabled, the exporter stores some model parameters in external\n","binary files, rather than the ONNX file itself. These external binary files are stored in the\n","same location as the ONNX file. Argument ‘f’ must be a string specifying the location of the model.\n","before dts\n","in dl\n","dt  torch.onnx.export\n","before dts\n","in dl\n","dt  torch.onnx.export\n","before dts\n","in dl\n","dt  torch.onnx.export\n","before dts\n","in dl\n","dt  torch.onnx.export\n","before dts\n","in dl\n","dt  torch.onnx.export\n","before dts\n","in dl\n","dt  torch.onnx.export\n","before dts\n","in dl\n","dt  torch.onnx.export\n","before dts\n","in dl\n","dt  torch.onnx.export\n","before dts\n","in dl\n","dt  torch.onnx.export\n","before dts\n","in dl\n","dt  torch.tril_indices\n","before dts\n","in dl\n","dt  torch.sub\n","before dts\n","in dlc\n","dlcdt  torch.cuda.amp.autocast\n","before dts\n","in dlc\n","dlcdt  autocastcan also be used as a decorator, e.g., on theforwardmethod of your model:\n","before dts\n","in dlc\n","dlcdt  Floating-point Tensors produced in an autocast-enabled region may befloat16.\n","After returning to an autocast-disabled region, using them with floating-point\n","Tensors of different dtypes may cause type mismatch errors.  If so, cast the Tensor(s)\n","produced in the autocast region back tofloat32(or other dtype if desired).\n","If a Tensor from the autocast region is alreadyfloat32, the cast is a no-op,\n","and incurs no additional overhead.  Example:\n","before dts\n","in dlc\n","dlcdt  autocast(enabled=False)subregions can be nested in autocast-enabled regions.\n","Locally disabling autocast can be useful, for example, if you want to force a subregion\n","to run in a particulardtype.  Disabling autocast gives you explicit control over\n","the execution type.  In the subregion, inputs from the surrounding region\n","should be cast todtypebefore use:\n","before dts\n","in dl\n","dlmdt  torch.cuda.amp.GradScaler.unscale_\n","before dts\n","in dl\n","dt  torch.logdet\n","before dts\n","in dl\n","dt  torch.lt\n","before dts\n","in dl\n","dt  torch.quantize_per_tensor\n","before dts\n","in dl\n","dt  torch.square\n","before dts\n","in dl\n","dt  torch.take\n","before dts\n","from dpps  The context managerstorch.no_grad(),torch.enable_grad(), andtorch.set_grad_enabled()are helpful for locally disabling and enabling\n","gradient computation. SeeLocally disabling gradient computationfor more details on\n","their usage.  These context managers are thread local, so they won’t\n","work if you send work to another thread using thethreadingmodule, etc.\n","before dts\n","in dl\n","dt  torch.normal\n","before dts\n","in dl\n","dt  torch.normal\n","before dts\n","in dl\n","dt  torch.normal\n","before dts\n","in dl\n","dt  torch.normal\n","before dts\n","in dl\n","dt  torch.std_mean\n","before dts\n","in dl\n","dt  torch.swapdims\n","before dts\n","in dl\n","dt  torch.dstack\n","before dts\n","in dl\n","dt  torch.special.entr\n","before dts\n","in dl\n","dt  torch.special.erf\n","before dts\n","in dl\n","dt  torch.special.erfc\n","before dts\n","in dl\n","dt  torch.special.erfinv\n","before dts\n","in dl\n","dt  torch.special.expit\n","before dts\n","in dl\n","dt  torch.special.expm1\n","before dts\n","in dl\n","dt  torch.special.exp2\n","before dts\n","in dl\n","dt  torch.special.gammaln\n","before dts\n","in dl\n","dt  torch.special.i0e\n","before dts\n","in dl\n","dt  torch.special.logit\n","before dts\n","in dl\n","dt  torch.special.xlog1py\n","before dts\n","in dl\n","dt  torch.isneginf\n","before dts\n","in dl\n","dt  torch.tensordot\n","before dts\n","in dl\n","dt  torch.clamp\n","before dts\n","in dl\n","dt  torch.renorm\n","before dts\n","in dl\n","dt  torch.logcumsumexp\n","before dts\n","in dl\n","dt  torch.cat\n","before dts\n","in dl\n","dt  torch.cdist\n","before dts\n","in dl\n","dt  torch.index_select\n","before dts\n","from dpps  A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\n","non-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\n","values when determining the minimumdtypesof an operand.  Quantized and complex types\n","are not yet supported.\n","before dts\n","from dpps  \n","before dts\n","from dpps  Atorch.devicecan be constructed via a string or via a string and device ordinalVia a string:\n","before dts\n","from dpps  Via a string:Via a string and device ordinal:\n","before dts\n","from dpps  NoteThetorch.deviceargument in functions can generally be substituted with a string.\n","This allows for fast prototyping of code.\n","before dts\n","from dpps  NoteThetorch.deviceargument in functions can generally be substituted with a string.\n","This allows for fast prototyping of code.\n","before dts\n","from dpps  NoteFor legacy reasons, a device can be constructed via a single device ordinal, which is treated\n","as a cuda device.  This matchesTensor.get_device(), which returns an ordinal for cuda\n","tensors and is not supported for cpu tensors.\n","before dts\n","from dpps  NoteMethods which take a device will generally accept a (properly formatted) string\n","or (legacy) integer device ordinal, i.e. the following are all equivalent:\n","before dts\n","from dpps  torch.stridedrepresents dense Tensors and is the memory layout that\n","is most commonly used. Each strided tensor has an associatedtorch.Storage, which holds its data. These tensors provide\n","multi-dimensional,stridedview of a storage. Strides are a list of integers: the k-th stride\n","represents the jump in the memory necessary to go from one element to the\n","next one in the k-th dimension of the Tensor. This concept makes it possible\n","to perform many tensor operations efficiently.\n","before dts\n","in dl\n","dt  torch.cholesky_solve\n","before dts\n","in dl\n","dt  torch.argmin\n","before dts\n","from dpps  whereθ\\thetaθare the parameters,α\\alphaαis the learning rate,rrris the reward andp(a∣πθ(s))p(a|\\pi^\\theta(s))p(a∣πθ(s))is the probability of\n","taking actionaaain statesssgiven policyπθ\\pi^\\thetaπθ.In practice we would sample an action from the output of a network, apply this\n","action in an environment, and then uselog_probto construct an equivalent\n","loss function. Note that we use a negative because optimizers use gradient\n","descent, whilst the rule above assumes gradient ascent. With a categorical\n","policy, the code for implementing REINFORCE would be as follows:\n","before dts\n","from dpps  The other way to implement these stochastic/policy gradients would be to use the\n","reparameterization trick from thersample()method, where the\n","parameterized random variable can be constructed via a parameterized\n","deterministic function of a parameter-free random variable. The reparameterized\n","sample therefore becomes differentiable. The code for implementing the pathwise\n","derivative would be as follows:\n","before dts\n","in dlc\n","dlcdt  torch.distributions.bernoulli.Bernoulli\n","before dts\n","in dlc\n","dlcdt  torch.distributions.beta.Beta\n","before dts\n","in dlc\n","dlcdt  torch.distributions.binomial.Binomial\n","before dts\n","in dlc\n","dlcdt  torch.distributions.categorical.Categorical\n","before dts\n","in dlc\n","dlcdt  torch.distributions.cauchy.Cauchy\n","before dts\n","in dlc\n","dlcdt  torch.distributions.chi2.Chi2\n","before dts\n","in dlc\n","dlcdt  torch.distributions.continuous_bernoulli.ContinuousBernoulli\n","before dts\n","in dlc\n","dlcdt  torch.distributions.dirichlet.Dirichlet\n","before dts\n","in dlc\n","dlcdt  torch.distributions.exponential.Exponential\n","before dts\n","in dlc\n","dlcdt  torch.distributions.fishersnedecor.FisherSnedecor\n","before dts\n","in dlc\n","dlcdt  torch.distributions.gamma.Gamma\n","before dts\n","in dlc\n","dlcdt  torch.distributions.geometric.Geometric\n","before dts\n","in dlc\n","dlcdt  torch.distributions.gumbel.Gumbel\n","before dts\n","in dlc\n","dlcdt  Creates a half-Cauchy distribution parameterized byscalewhere:\n","before dts\n","in dlc\n","dlcdt  torch.distributions.half_cauchy.HalfCauchy\n","before dts\n","in dlc\n","dlcdt  Creates a half-normal distribution parameterized byscalewhere:\n","before dts\n","in dlc\n","dlcdt  torch.distributions.half_normal.HalfNormal\n","before dts\n","in dlc\n","dlcdt  This is mainly useful for changing the shape of the result oflog_prob(). For example to create a diagonal Normal distribution with\n","the same shape as a Multivariate Normal distribution (so they are\n","interchangeable), you can:\n","before dts\n","in dlc\n","dlcdt  torch.distributions.kumaraswamy.Kumaraswamy\n","before dts\n","in dlc\n","dlcdt  torch.distributions.lkj_cholesky.LKJCholesky\n","before dts\n","in dlc\n","dlcdt  torch.distributions.laplace.Laplace\n","before dts\n","in dlc\n","dlcdt  Creates a log-normal distribution parameterized bylocandscalewhere:\n","before dts\n","in dlc\n","dlcdt  torch.distributions.log_normal.LogNormal\n","before dts\n","in dlc\n","dlcdt  Creates a multivariate normal distribution with covariance matrix having a low-rank form\n","parameterized bycov_factorandcov_diag:\n","before dts\n","in dlc\n","dlcdt  torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal\n","before dts\n","in dlc\n","dlcdt  The computation for determinant and inverse of covariance matrix is avoided whencov_factor.shape[1] << cov_factor.shape[0]thanks toWoodbury matrix identityandmatrix determinant lemma.\n","Thanks to these formulas, we just need to compute the determinant and inverse of\n","the small size “capacitance” matrix:\n","before dts\n","in dlc\n","dlcdt  torch.distributions.mixture_same_family.MixtureSameFamily\n","before dts\n","in dlc\n","dlcdt  torch.distributions.multinomial.Multinomial\n","before dts\n","in dlc\n","dlcdt  torch.distributions.multivariate_normal.MultivariateNormal\n","before dts\n","in dlc\n","dlcdt  torch.distributions.normal.Normal\n","before dts\n","in dlc\n","dlcdt  torch.distributions.one_hot_categorical.OneHotCategorical\n","before dts\n","in dlc\n","dlcdt  torch.distributions.pareto.Pareto\n","before dts\n","in dlc\n","dlcdt  torch.distributions.poisson.Poisson\n","before dts\n","in dlc\n","dlcdt  torch.distributions.relaxed_bernoulli.RelaxedBernoulli\n","before dts\n","in dlc\n","dlcdt  torch.distributions.relaxed_categorical.RelaxedOneHotCategorical\n","before dts\n","in dlc\n","dlcdt  torch.distributions.studentT.StudentT\n","before dts\n","in dlc\n","dlcdt  Extension of the Distribution class, which applies a sequence of Transforms\n","to a base distribution.  Let f be the composition of transforms applied:\n","before dts\n","in dlc\n","dlcdt  An example for the usage ofTransformedDistributionwould be:\n","before dts\n","in dlc\n","dlcdt  torch.distributions.uniform.Uniform\n","before dts\n","in dlc\n","before dts\n","in dlc\n","dlcdt  torch.distributions.weibull.Weibull\n","before dts\n","in dl\n","dt  torch.distributions.kl.register_kl\n","before dts\n","in dl\n","dt  torch.distributions.kl.register_kl\n","before dts\n","in dl\n","dt  torch.distributions.kl.register_kl\n","before dts\n","in dlc\n","dlcdt  Caching is useful for transforms whose inverses are either expensive or\n","numerically unstable. Note that care must be taken with memoized values\n","since the autograd graph may be reversed. For example while the following\n","works with or without caching:\n","before dts\n","in dlc\n","dlcdt  However the following will error when caching due to dependency reversal:\n","before dts\n","from dpps  PyTorch provides two globalConstraintRegistryobjects that linkConstraintobjects toTransformobjects. These objects both\n","input constraints and return transforms, but they have different guarantees on\n","bijectivity.Thetransform_to()registry is useful for performing unconstrained\n","optimization on constrained parameters of probability distributions, which are\n","indicated by each distribution’s.arg_constraintsdict. These transforms often\n","overparameterize a space in order to avoid rotation; they are thus more\n","suitable for coordinate-wise optimization algorithms like Adam:\n","before dts\n","from dpps  Thetransform_to()registry is useful for performing unconstrained\n","optimization on constrained parameters of probability distributions, which are\n","indicated by each distribution’s.arg_constraintsdict. These transforms often\n","overparameterize a space in order to avoid rotation; they are thus more\n","suitable for coordinate-wise optimization algorithms like Adam:Thebiject_to()registry is useful for Hamiltonian Monte Carlo, where\n","samples from a probability distribution with constrained.supportare\n","propagated in an unconstrained space, and algorithms are typically rotation\n","invariant.:\n","before dts\n","from dpps  Thebiject_to()registry is useful for Hamiltonian Monte Carlo, where\n","samples from a probability distribution with constrained.supportare\n","propagated in an unconstrained space, and algorithms are typically rotation\n","invariant.:Thebiject_toandtransform_toobjects can be extended by user-defined\n","constraints and transforms using their.register()method either as a\n","function on singleton constraints:\n","before dts\n","from dpps  Thebiject_toandtransform_toobjects can be extended by user-defined\n","constraints and transforms using their.register()method either as a\n","function on singleton constraints:or as a decorator on parameterized constraints:\n","before dts\n","in dl\n","dlmdt  torch.distributions.constraint_registry.ConstraintRegistry.register\n","before dts\n","in dlc\n","dlcdt  torch.quasirandom.SobolEngine\n","before dts\n","in dl\n","dt  torch.unique_consecutive\n","before dts\n","in dl\n","dt  torch.signbit\n","before dts\n","in dl\n","dt  torch.isinf\n","before dts\n","in dl\n","dt  torch.log1p\n","before dts\n","in dl\n","dt  torch.randperm\n","before dts\n","in dl\n","dt  torch.diff\n","before dts\n","in dl\n","dt  torch.bernoulli\n","before dts\n","in dl\n","dt  torch.amin\n","before dts\n","in dl\n","dt  torch.equal\n","before dts\n","in dl\n","dt  torch.logical_not\n","before dts\n","from dpps  The context managerstorch.no_grad(),torch.enable_grad(), andtorch.set_grad_enabled()are helpful for locally disabling and enabling\n","gradient computation. SeeLocally disabling gradient computationfor more details on\n","their usage.  These context managers are thread local, so they won’t\n","work if you send work to another thread using thethreadingmodule, etc.\n","before dts\n","in dl\n","dt  torch.acos\n","before dts\n","from dpps  The context managerstorch.no_grad(),torch.enable_grad(), andtorch.set_grad_enabled()are helpful for locally disabling and enabling\n","gradient computation. SeeLocally disabling gradient computationfor more details on\n","their usage.  These context managers are thread local, so they won’t\n","work if you send work to another thread using thethreadingmodule, etc.\n","before dts\n","in dl\n","dt  torch.abs\n","before dts\n","in dl\n","dt  torch.from_numpy\n","before dts\n","in dl\n","dt  torch.ravel\n","before dts\n","in dl\n","dt  torch.sinc\n","before dts\n","in dlc\n","dlcdt  String to distinguish measurements with identical label and\n","sub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\n","based on the input size  to create a table of the form:\n","before dts\n","in dl\n","dlmdt  torch.utils.benchmark.Timer.blocked_autorange\n","before dts\n","in dl\n","dlmdt  torch.utils.benchmark.CallgrindStats.as_standardized\n","before dts\n","in dl\n","dt  torch.save\n","before dts\n","in dlc\n","dlcdt  torch.set_grad_enabled\n","before dts\n","in dl\n","dt  torch.combinations\n","before dts\n","in dl\n","dt  torch.mv\n","before dts\n","in dl\n","dt  torch.amax\n","before dts\n","in dl\n","dt  torch.numel\n","before dts\n","in dl\n","dt  torch.logspace\n","before dts\n","in dl\n","dt  torch.acosh\n","before dts\n","from dpps  The container format for atorch.packageis ZIP, so any tools that work with standard ZIP files should\n","work for exploring the contents. Some common ways to interact with ZIP files:\n","before dts\n","from dpps  The container format for atorch.packageis ZIP, so any tools that work with standard ZIP files should\n","work for exploring the contents. Some common ways to interact with ZIP files:\n","before dts\n","from dpps  The container format for atorch.packageis ZIP, so any tools that work with standard ZIP files should\n","work for exploring the contents. Some common ways to interact with ZIP files:\n","before dts\n","from dpps  PackageImporterandPackageExporterprovide afile_structure()method, which will return a printable\n","and queryableFolderobject. TheFolderobject is a simple directory structure that you can use to explore the\n","current contents of atorch.package.TheFolderobject itself is directly printable and will print out a file tree representation. To filter what is returned,\n","use the glob-styleincludeandexcludefiltering arguments.\n","before dts\n","from dpps  TheFolderobject itself is directly printable and will print out a file tree representation. To filter what is returned,\n","use the glob-styleincludeandexcludefiltering arguments.Output:\n","before dts\n","from dpps  Output:You can also queryFolderobjects with thehas_file()method.\n","before dts\n","from dpps  PackageExporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\n","Python objects, text, and binary data to a package.\n","before dts\n","from dpps  PackageExporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\n","Python objects, text, and binary data to a package.PackageImporterexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\n","Python objects, text and binary data from a package.\n","before dts\n","from dpps  torch.packageallows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\n","Python’s normal pickling process.Steps:\n","before dts\n","from dpps  torch.packageallows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\n","Python’s normal pickling process.Steps:\n","before dts\n","from dpps  torch.packageallows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\n","Python’s normal pickling process.Steps:\n","before dts\n","from dpps  APackageImporterwill add the attribute__torch_package__to every module that it initializes. Your code can check for the\n","presence of this attribute to determine whether it is executing in a packaged context or not.\n","before dts\n","from dpps  APackageImporterwill add the attribute__torch_package__to every module that it initializes. Your code can check for the\n","presence of this attribute to determine whether it is executing in a packaged context or not.Now, the code will behave differently depending on whether it’s imported normally through your Python environment or imported from atorch.package.\n","before dts\n","from dpps  PackageExporteroffers asave_source_string()method that allows one to save arbitrary Python source code to a module of your choosing.\n","before dts\n","from dpps  PackageImporterimplements theimportlib.resourcesAPI for accessing resources from inside a package.\n","before dts\n","from dpps  PackageImporterimplements theimportlib.resourcesAPI for accessing resources from inside a package.Theimportlib.resourcesAPI allows access to resources from within packaged code.\n","before dts\n","from dpps  Theimportlib.resourcesAPI allows access to resources from within packaged code.Usingimportlib.resourcesis the recommended way to access package contents from within packaged code, since it complies\n","with the Python standard. However, it is also possible to access the parentPackageImporterinstance itself from within\n","packaged code.\n","before dts\n","from dpps  To tell if an object’s code is from atorch.package, use thetorch.package.is_from_package()function.\n","Note: if an object is from a package but its definition is from a module markedexternor fromstdlib,\n","this check will returnFalse.\n","before dts\n","from dpps  To re-export an object that was previously imported by aPackageImporter, you must make the newPackageExporteraware of the originalPackageImporterso that it can find source code for your object’s dependencies.\n","before dts\n","from dpps  To package a TorchScript model, use the samesave_pickleandload_pickleAPIs as you would with any other object.\n","Saving TorchScript objects that are attributes or submodules is supported as well with no extra work.\n","before dts\n","from dpps  Atorch.packagefile is a ZIP archive which conventionally uses the.ptextension. Inside the ZIP archive, there are two kinds of files:As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like:\n","before dts\n","from dpps  The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\n","Thetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n","(that is, newer version of PyTorch will always be able to load oldertorch.packages).Currently, the.data/directory contains the following items:\n","before dts\n","from dpps  All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\n","please consultthis essay(it’s slightly out of date, so double-check implementation details\n","with thePython reference documentation).\n","before dts\n","from dpps  When you issue asave_pickle(obj,...)call,PackageExporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode.In a pickle, an object is saved along with aGLOBALopcode that describes where to find the implementation of the object’s type, like:\n","before dts\n","from dpps  Note that actions are only defined on entire Python modules. There is no way to package “just” a function or class from module and leave the rest out.\n","This is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\n","module, so that’s whattorch.packageuses.Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\n","with an action using methods onPackageImporter, e.g.\n","before dts\n","from dpps  When specifying actions, you can pass multiple patterns, e.g.\n","before dts\n","from dpps  A module will match against this action if it matches any of the patterns.You can also specify patterns to exlcude, e.g.\n","before dts\n","from dpps  Any class that you import from aPackageImporterwill be a version of the class specific to that importer. For example:\n","before dts\n","from dpps  In this example,MyClassandimport_MyClassarenot the same type. In this specific example,MyClassandimport_MyClasshave exactly the\n","same implementation, so you might thing it’s okay to consider them the same class. But consider the situation whereimport_MyClassis coming from an\n","older package with an entirely different implementation ofMyClass— in that case, it’s unsafe to consider them the same class.Under the hood, each importer has a prefix that allows it to uniquely identify classes:\n","before dts\n","in dl\n","dlmdt  torch.package.PackageExporter.close\n","before dts\n","in dl\n","dlmdt  torch.package.PackageExporter.register_extern_hook\n","before dts\n","in dl\n","dlmdt  torch.package.PackageExporter.register_intern_hook\n","before dts\n","in dl\n","dlmdt  torch.package.PackageExporter.register_mock_hook\n","before dts\n","in dl\n","dlmdt  torch.package.PackageImporter.id\n","before dts\n","in dl\n","dt  torch.cumsum\n","before dts\n","in dl\n","dt  torch.linalg.matrix_power\n","before dts\n","in dl\n","dt  torch.linalg.matrix_power\n","before dts\n","from dpps  The context managerstorch.no_grad(),torch.enable_grad(), andtorch.set_grad_enabled()are helpful for locally disabling and enabling\n","gradient computation. SeeLocally disabling gradient computationfor more details on\n","their usage.  These context managers are thread local, so they won’t\n","work if you send work to another thread using thethreadingmodule, etc.\n","before dts\n","in dlc\n","dlcdt  torch.profiler.profile\n","before dts\n","in dlc\n","dlcdt  Using the profiler’sschedule,on_trace_readyandstepfunctions:\n","before dts\n","in dl\n","dt  torch.unique\n","before dts\n","in dl\n","dt  torch.asin\n","before dts\n","from dpps  To construct anOptimizeryou have to give it an iterable containing the\n","parameters (all should beVariables) to optimize. Then,\n","you can specify optimizer-specific options such as the learning rate, weight decay, etc.\n","before dts\n","from dpps  Optimizers also support specifying per-parameter options. To do this, instead\n","of passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\n","aparamskey, containing a list of parameters belonging to it. Other keys\n","should match the keyword arguments accepted by the optimizers, and will be used\n","as optimization options for this group.For example, this is very useful when one wants to specify per-layer learning rates:\n","before dts\n","from dpps  This is a simplified version supported by most optimizers. The function can be\n","called once the gradients are computed using e.g.backward().\n","before dts\n","from dpps  Some optimization algorithms such as Conjugate Gradient and LBFGS need to\n","reevaluate the function multiple times, so you have to pass in a closure that\n","allows them to recompute your model. The closure should clear the gradients,\n","compute the loss, and return it.\n","before dts\n","from dpps  Learning rate scheduling should be applied after optimizer’s update; e.g., you\n","should write your code this way:\n","before dts\n","from dpps  Most learning rate schedulers can be called back-to-back (also referred to as\n","chaining schedulers). The result is that each scheduler is applied one after the\n","other on the learning rate obtained by the one preceding it.\n","before dts\n","from dpps  In many places in the documentation, we will use the following template to refer to schedulers\n","algorithms.\n","before dts\n","from dpps  AveragedModelclass serves to compute the weights of the SWA model. You can create an\n","averaged model by running:\n","before dts\n","from dpps  AveragedModelclass serves to compute the weights of the SWA model. You can create an\n","averaged model by running:Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\n","averages, you can use theupdate_parameters()function:\n","before dts\n","from dpps  Typically, in SWA the learning rate is set to a high constant value.SWALRis a\n","learning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\n","constant. For example, the following code creates a scheduler that linearly anneals the\n","learning rate from its initial value to 0.05 in 5 epochs within each parameter group:\n","before dts\n","from dpps  update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\n","on a given dataloaderloaderat the end of training:\n","before dts\n","from dpps  By default,torch.optim.swa_utils.AveragedModelcomputes a running equal average of\n","the parameters that you provide, but you can also use custom averaging functions with theavg_fnparameter. In the following exampleema_modelcomputes an exponential moving average.\n","before dts\n","from dpps  In the example below,swa_modelis the SWA model that accumulates the averages of the weights.\n","We train the model for a total of 300 epochs and we switch to the SWA learning rate schedule\n","and start to collect SWA averages of the parameters at epoch 160:\n","before dts\n","in dl\n","dt  torch.nanquantile\n","before dts\n","in dl\n","dt  torch.symeig\n","before dts\n","in dl\n","dt  torch.symeig\n","before dts\n","in dl\n","dt  torch.symeig\n","before dts\n","in dl\n","dt  torch.gcd\n","before dts\n","in dl\n","dt  torch.matrix_exp\n","before dts\n","from dpps  Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights)\n","to a github repository by adding a simplehubconf.pyfile;hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n","(example: a pre-trained model you want to publish).\n","before dts\n","from dpps  Here is a code snippet specifies an entrypoint forresnet18model if we expand\n","the implementation inpytorch/vision/hubconf.py.\n","In most case importing the right function inhubconf.pyis sufficient. Here we\n","just want to use the expanded version as an example to show how it works.\n","You can see the full script inpytorch/vision repo\n","before dts\n","from dpps  Here is a code snippet specifies an entrypoint forresnet18model if we expand\n","the implementation inpytorch/vision/hubconf.py.\n","In most case importing the right function inhubconf.pyis sufficient. Here we\n","just want to use the expanded version as an example to show how it works.\n","You can see the full script inpytorch/vision repo\n","before dts\n","in dl\n","dt  torch.hub.list\n","before dts\n","in dl\n","dt  torch.hub.help\n","before dts\n","in dl\n","dt  torch.hub.load\n","before dts\n","in dl\n","dt  torch.hub.download_url_to_file\n","before dts\n","in dl\n","dt  torch.hub.load_state_dict_from_url\n","before dts\n","in dl\n","dt  torch.log10\n","before dts\n","in dl\n","dt  torch.fmin\n","before dts\n","in dl\n","dt  torch.outer\n","before dts\n","in dl\n","dt  torch.tan\n","before dts\n","in dl\n","dt  torch.addr\n","before dts\n","in dl\n","dt  torch.cumprod\n","before dts\n","in dl\n","dt  torch.inner\n","before dts\n","in dl\n","dt  torch.isclose\n","before dts\n","in dl\n","dt  torch.rsqrt\n","before dts\n","in dl\n","dt  torch.searchsorted\n","before dts\n","in dl\n","dt  torch.block_diag\n","before dts\n","in dl\n","dt  torch.linalg.inv\n","before dts\n","in dl\n","dt  torch.linalg.inv\n","before dts\n","in dl\n","dt  torch.bitwise_or\n","before dts\n","in dl\n","dt  torch.igammac\n","before dts\n","in dl\n","dt  torch.randint\n","before dts\n","in dl\n","dt  torch.masked_select\n","before dts\n","from dpps  The context managerstorch.no_grad(),torch.enable_grad(), andtorch.set_grad_enabled()are helpful for locally disabling and enabling\n","gradient computation. SeeLocally disabling gradient computationfor more details on\n","their usage.  These context managers are thread local, so they won’t\n","work if you send work to another thread using thethreadingmodule, etc.\n","before dts\n","in dl\n","dt  torch.mm\n","before dts\n","in dl\n","dt  torch.movedim\n","before dts\n","in dl\n","dt  torch.addmm\n","before dts\n","from dpps  The context managerstorch.no_grad(),torch.enable_grad(), andtorch.set_grad_enabled()are helpful for locally disabling and enabling\n","gradient computation. SeeLocally disabling gradient computationfor more details on\n","their usage.  These context managers are thread local, so they won’t\n","work if you send work to another thread using thethreadingmodule, etc.\n","before dts\n","in dl\n","dt  torch.prod\n","before dts\n","in dl\n","dt  torch.prod\n","before dts\n","in dl\n","dt  torch.where\n","before dts\n","in dl\n","dt  torch.empty_like\n","before dts\n","in dl\n","dt  torch.bmm\n","before dts\n","in dl\n","dt  torch.randn\n","before dts\n","in dl\n","dt  torch.poisson\n","before dts\n","in dl\n","dt  torch.vstack\n","before dts\n","in dl\n","dt  torch.reshape\n","before dts\n","in dl\n","dt  torch.bucketize\n","before dts\n","in dl\n","dt  torch.copysign\n","before dts\n","in dl\n","dt  torch.kthvalue\n","before dts\n","in dl\n","dt  torch.cholesky\n","before dts\n","in dl\n","dt  torch.cholesky\n","before dts\n","in dl\n","dt  torch.cholesky\n","before dts\n","in dl\n","dt  torch.histc\n","before dts\n","from dpps  The context managerstorch.no_grad(),torch.enable_grad(), andtorch.set_grad_enabled()are helpful for locally disabling and enabling\n","gradient computation. SeeLocally disabling gradient computationfor more details on\n","their usage.  These context managers are thread local, so they won’t\n","work if you send work to another thread using thethreadingmodule, etc.\n","before dts\n","from dpps  The context managerstorch.no_grad(),torch.enable_grad(), andtorch.set_grad_enabled()are helpful for locally disabling and enabling\n","gradient computation. SeeLocally disabling gradient computationfor more details on\n","their usage.  These context managers are thread local, so they won’t\n","work if you send work to another thread using thethreadingmodule, etc.\n","before dts\n","in dl\n","dt  torch.eq\n","before dts\n","from dpps  The context managerstorch.no_grad(),torch.enable_grad(), andtorch.set_grad_enabled()are helpful for locally disabling and enabling\n","gradient computation. SeeLocally disabling gradient computationfor more details on\n","their usage.  These context managers are thread local, so they won’t\n","work if you send work to another thread using thethreadingmodule, etc.\n","before dts\n","in dl\n","dt  torch.unsqueeze\n","before dts\n","from dpps  The context managerstorch.no_grad(),torch.enable_grad(), andtorch.set_grad_enabled()are helpful for locally disabling and enabling\n","gradient computation. SeeLocally disabling gradient computationfor more details on\n","their usage.  These context managers are thread local, so they won’t\n","work if you send work to another thread using thethreadingmodule, etc.\n","before dts\n","in dl\n","dt  torch.matrix_rank\n","before dts\n","in dl\n","dt  torch.argmax\n","before dts\n","in dl\n","dt  torch.argmax\n","before dts\n","in dl\n","dt  torch.imag\n","before dts\n","in dl\n","dt  torch.exp\n","before dts\n","in dl\n","dt  torch.angle\n","before dts\n","in dl\n","dt  torch.special.entr\n","before dts\n","in dl\n","dt  torch.special.erf\n","before dts\n","in dl\n","dt  torch.special.erfc\n","before dts\n","in dl\n","dt  torch.special.erfinv\n","before dts\n","in dl\n","dt  torch.special.expit\n","before dts\n","in dl\n","dt  torch.special.expm1\n","before dts\n","in dl\n","dt  torch.special.exp2\n","before dts\n","in dl\n","dt  torch.special.gammaln\n","before dts\n","in dl\n","dt  torch.special.i0e\n","before dts\n","in dl\n","dt  torch.special.logit\n","before dts\n","in dl\n","dt  torch.special.xlog1py\n","before dts\n","in dl\n","dlmdt  torch.Tensor.scatter_\n","before dts\n","in dl\n","dlmdt  torch.Tensor.scatter_\n","before dts\n","in dl\n","dlmdt  torch.Tensor.scatter_\n","before dts\n","in dl\n","dt  torch.range\n","before dts\n","in dl\n","dt  torch.broadcast_to\n","before dts\n","in dl\n","dt  torch.squeeze\n","before dts\n","from dpps  You can usetorch.manual_seed()to seed the RNG for all devices (both\n","CPU and CUDA):\n","before dts\n","from dpps  For custom operators, you might need to set python seed as well:\n","before dts\n","from dpps  If you or any of the libraries you are using rely on NumPy, you can seed the global\n","NumPy RNG with:\n","before dts\n","from dpps  Please check the documentation fortorch.use_deterministic_algorithms()for a full list of affected operations. If an operation does not act correctly\n","according to the documentation, or if you need a deterministic implementation\n","of an operation that does not have one, please submit an issue:https://github.com/pytorch/pytorch/issues?q=label:%22topic:%20determinism%22For example, running the nondeterministic CUDA implementation oftorch.Tensor.index_add_()will throw an error:\n","before dts\n","from dpps  For example, running the nondeterministic CUDA implementation oftorch.Tensor.index_add_()will throw an error:Whentorch.bmm()is called with sparse-dense CUDA tensors it typically uses a\n","nondeterministic algorithm, but when the deterministic flag is turned on, its alternate\n","deterministic implementation will be used:\n","before dts\n","from dpps  DataLoader will reseed workers followingRandomness in multi-process data loadingalgorithm.\n","Useworker_init_fn()andgeneratorto preserve reproducibility:\n","before dts\n","in dl\n","dt  torch.dsplit\n","before dts\n","in dl\n","dt  torch.dsplit\n","before dts\n","in dl\n","dt  torch.can_cast\n","before dts\n","in dl\n","dt  torch.linalg.slogdet\n","before dts\n","in dl\n","dt  torch.qr\n","before dts\n","in dl\n","dt  torch.qr\n","before dts\n","in dl\n","dt  torch.qr\n","before dts\n","in dl\n","dt  torch.nan_to_num\n","before dts\n","in dl\n","dt  torch.div\n","before dts\n","from dpps  Setting the environment variablePYTORCH_JIT=0will disable all script\n","and tracing annotations. If there is hard-to-debug error in one of your\n","TorchScript models, you can use this flag to force everything to run using native\n","Python. Since TorchScript (scripting and tracing) is disabled with this flag,\n","you can use tools likepdbto debug the model code.  For example:\n","before dts\n","from dpps  Setting the environment variablePYTORCH_JIT=0will disable all script\n","and tracing annotations. If there is hard-to-debug error in one of your\n","TorchScript models, you can use this flag to force everything to run using native\n","Python. Since TorchScript (scripting and tracing) is disabled with this flag,\n","you can use tools likepdbto debug the model code.  For example:Debugging this script withpdbworks except for when we invoke the@torch.jit.scriptfunction. We can globally disable\n","JIT, so that we can call the@torch.jit.scriptfunction as a normal Python function and not compile it. If the above script\n","is calleddisable_jit_example.py, we can invoke it like so:\n","before dts\n","from dpps  TorchScript provides a code pretty-printer for allScriptModuleinstances. This\n","pretty-printer gives an interpretation of the script method’s code as valid\n","Python syntax. For example:AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule’s code.\n","If theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\n","code of a method namedfooon aScriptModuleby accessing.foo.code.\n","The example above produces this output:\n","before dts\n","from dpps  graphfollows the same rules described in theInspecting Codesection\n","with regard toforwardmethod lookup.The example script above produces the graph:\n","before dts\n","from dpps  One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\n","of inputs that will be used to re-trace the computation and verify the\n","results. For example:\n","before dts\n","from dpps  One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\n","of inputs that will be used to re-trace the computation and verify the\n","results. For example:Gives us the following diagnostic information:\n","before dts\n","from dpps  In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead:Which produces:\n","before dts\n","from dpps  The tracer produces warnings for several problematic patterns in traced\n","computation. As an example, take a trace of a function that contains an\n","in-place assignment on a slice (a view) of a Tensor:Produces several warnings and a graph which simply returns the input:\n","before dts\n","from dpps  First convert your model from GPU to CPU and then save it, like so:\n","before dts\n","from dpps  Old API:New API:\n","before dts\n","in dl\n","dt  torch.column_stack\n","before dts\n","in dl\n","dt  torch.is_tensor\n","before dts\n","in dl\n","dt  torch.set_flush_denormal\n","before dts\n","in dl\n","dt  torch.einsum\n","before dts\n","in dl\n","dt  torch.lerp\n","before dts\n","in dl\n","dt  torch.maximum\n","before dts\n","in dl\n","dt  torch.norm\n","before dts\n","in dl\n","dt  torch.zeros\n","before dts\n","in dl\n","dt  torch.any\n","before dts\n","in dl\n","dt  torch.any\n","before dts\n","in dl\n","dt  torch.lu_solve\n","before dts\n","in dl\n","dt  torch.argsort\n","before dts\n","in dl\n","dt  torch.cummax\n","before dts\n","in dl\n","dt  torch.gather\n","before dts\n","in dl\n","dt  torch.gather\n","before dts\n","in dl\n","dt  torch.isreal\n","before dts\n","in dl\n","dt  torch.nn.init.calculate_gain\n","before dts\n","in dl\n","dt  torch.nn.init.uniform_\n","before dts\n","in dl\n","dt  torch.nn.init.normal_\n","before dts\n","in dl\n","dt  torch.nn.init.constant_\n","before dts\n","in dl\n","dt  torch.nn.init.ones_\n","before dts\n","in dl\n","dt  torch.nn.init.zeros_\n","before dts\n","in dl\n","dt  torch.nn.init.eye_\n","before dts\n","in dl\n","dt  torch.nn.init.dirac_\n","before dts\n","in dl\n","dt  torch.nn.init.xavier_uniform_\n","before dts\n","in dl\n","dt  torch.nn.init.xavier_normal_\n","before dts\n","in dl\n","dt  torch.nn.init.kaiming_uniform_\n","before dts\n","in dl\n","dt  torch.nn.init.kaiming_normal_\n","before dts\n","in dl\n","dt  torch.nn.init.orthogonal_\n","before dts\n","in dl\n","dt  torch.nn.init.sparse_\n","before dts\n","in dl\n","dt  torch.addbmm\n","before dts\n","in dl\n","dt  torch.linalg.pinv\n","before dts\n","in dl\n","dt  torch.linalg.pinv\n","before dts\n","in dl\n","dt  torch.result_type\n","before dts\n","in dl\n","dt  torch.set_default_dtype\n","before dts\n","in dl\n","dt  torch.cummin\n","before dts\n","in dl\n","dt  torch.mode\n","before dts\n","in dl\n","dt  torch.is_tensor\n","before dts\n","in dlc\n","dlcdt  torch.distributed.optim.ZeroRedundancyOptimizer\n","before dts\n","in dl\n","dt  torch.arange\n","before dts\n","in dl\n","dt  torch.hypot\n","before dts\n","from dpps  The context managerstorch.no_grad(),torch.enable_grad(), andtorch.set_grad_enabled()are helpful for locally disabling and enabling\n","gradient computation. SeeLocally disabling gradient computationfor more details on\n","their usage.  These context managers are thread local, so they won’t\n","work if you send work to another thread using thethreadingmodule, etc.\n","before dts\n","in dl\n","dt  torch.as_strided\n","before dts\n","in dl\n","dt  torch.transpose\n","before dts\n","in dl\n","dt  torch.cross\n","before dts\n","in dl\n","dt  torch.rot90\n","before dts\n","from dpps  The context managerstorch.no_grad(),torch.enable_grad(), andtorch.set_grad_enabled()are helpful for locally disabling and enabling\n","gradient computation. SeeLocally disabling gradient computationfor more details on\n","their usage.  These context managers are thread local, so they won’t\n","work if you send work to another thread using thethreadingmodule, etc.\n","before dts\n","in dl\n","dlmdt  torch.Tensor.scatter_add_\n","before dts\n","in dl\n","dlmdt  torch.Tensor.scatter_add_\n","before dts\n","in dl\n","dt  torch.triu\n","before dts\n","in dl\n","dt  torch.polygamma\n","before dts\n","in dl\n","dt  torch.var\n","before dts\n","in dl\n","dt  torch.special.entr\n","before dts\n","in dl\n","dt  torch.special.erf\n","before dts\n","in dl\n","dt  torch.special.erfc\n","before dts\n","in dl\n","dt  torch.special.erfinv\n","before dts\n","in dl\n","dt  torch.special.expit\n","before dts\n","in dl\n","dt  torch.special.expm1\n","before dts\n","in dl\n","dt  torch.special.exp2\n","before dts\n","in dl\n","dt  torch.special.gammaln\n","before dts\n","in dl\n","dt  torch.special.i0e\n","before dts\n","in dl\n","dt  torch.special.logit\n","before dts\n","in dl\n","dt  torch.special.xlog1py\n","before dts\n","in dl\n","dt  torch.allclose\n","before dts\n","in dl\n","dt  torch.matmul\n","before dts\n","in dl\n","dt  torch.count_nonzero\n","before dts\n","in dl\n","dt  torch.meshgrid\n","before dts\n","in dl\n","dt  torch.is_nonzero\n","before dts\n","from dpps  A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor:\n","before dts\n","from dpps  A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor:A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\n","constructor or tensor creation op:\n","before dts\n","from dpps  For more information about building Tensors, seeCreation OpsThe contents of a tensor can be accessed and modified using Python’s indexing\n","and slicing notation:\n","before dts\n","from dpps  The contents of a tensor can be accessed and modified using Python’s indexing\n","and slicing notation:Usetorch.Tensor.item()to get a Python number from a tensor containing a\n","single value:\n","before dts\n","from dpps  For more information about indexing, seeIndexing, Slicing, Joining, Mutating OpsA tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation.\n","before dts\n","from dpps  A sparse COO tensor can be constructed by providing the two tensors of\n","indices and values, as well as the size of the sparse tensor (when it\n","cannot be inferred from the indices and values tensors) to a functiontorch.sparse_coo_tensor().Suppose we want to define a sparse tensor with the entry 3 at location\n","(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\n","Unspecified elements are assumed to have the same value, fill value,\n","which is zero by default. We would then write:\n","before dts\n","from dpps  Suppose we want to define a sparse tensor with the entry 3 at location\n","(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\n","Unspecified elements are assumed to have the same value, fill value,\n","which is zero by default. We would then write:Note that the inputiis NOT a list of index tuples.  If you want\n","to write your indices this way, you should transpose before passing them to\n","the sparse constructor:\n","before dts\n","from dpps  Note that the inputiis NOT a list of index tuples.  If you want\n","to write your indices this way, you should transpose before passing them to\n","the sparse constructor:An empty sparse COO tensor can be constructed by specifying its size\n","only:\n","before dts\n","from dpps  PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\n","thevaluestensor to be a multi-dimensional tensor so that we\n","have:Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n","[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n","[7, 8] at location (1, 2). We would write\n","before dts\n","from dpps  PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\n","thevaluestensor to be a multi-dimensional tensor so that we\n","have:Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n","[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n","[7, 8] at location (1, 2). We would write\n","before dts\n","from dpps  PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\n","where there may be duplicate coordinates in the indices; in this case,\n","the interpretation is that the value at that index is the sum of all\n","duplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\n","uncoalesced tensor:\n","before dts\n","from dpps  PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\n","where there may be duplicate coordinates in the indices; in this case,\n","the interpretation is that the value at that index is the sum of all\n","duplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\n","uncoalesced tensor:while the coalescing process will accumulate the multi-valued elements\n","into a single value using summation:\n","before dts\n","from dpps  However, some operations can be implemented more efficiently on\n","uncoalesced tensors, and some on coalesced tensors.For instance, addition of sparse COO tensors is implemented by\n","simply concatenating the indices and values tensors:\n","before dts\n","from dpps  Let’s consider the following example:\n","before dts\n","from dpps  Let’s consider the following example:As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\n","some other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties:\n","before dts\n","from dpps  As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\n","some other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties:The number of sparse and dense dimensions can be acquired using\n","methodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance:\n","before dts\n","from dpps  NoteCurrently, one can acquire the COO format data only when the tensor\n","instance is coalesced:\n","before dts\n","from dpps  Currently, one can acquire the COO format data only when the tensor\n","instance is coalesced:For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()andtorch.Tensor._indices():\n","before dts\n","from dpps  Ifsis a sparse COO tensor then its COO format data can be\n","acquired using methodstorch.Tensor.indices()andtorch.Tensor.values().Constructing a new sparse COO tensor results a tensor that is not\n","coalesced:\n","before dts\n","from dpps  Constructing a new sparse COO tensor results a tensor that is not\n","coalesced:but one can construct a coalesced copy of a sparse COO tensor using\n","thetorch.Tensor.coalesce()method:\n","before dts\n","from dpps  When working with uncoalesced sparse COO tensors, one must take into\n","an account the additive nature of uncoalesced data: the values of the\n","same indices are the terms of a sum that evaluation gives the value of\n","the corresponding tensor element. For example, the scalar\n","multiplication on an uncoalesced sparse tensor could be implemented by\n","multiplying all the uncoalesced values with the scalar becausec*(a+b)==c*a+c*bholds. However, any nonlinear operation,\n","say, a square root, cannot be implemented by applying the operation to\n","uncoalesced data becausesqrt(a+b)==sqrt(a)+sqrt(b)does not\n","hold in general.Slicing (with positive step) of a sparse COO tensor is supported only\n","for dense dimensions. Indexing is supported for both sparse and dense\n","dimensions:\n","before dts\n","from dpps  Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\n","Thesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present.\n","before dts\n","from dpps  The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\n","tensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\n","be interpreted as missing values in the sparse tensor:\n","before dts\n","from dpps  The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\n","tensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\n","be interpreted as missing values in the sparse tensor:The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\n","supported on CSR tensors.\n","before dts\n","in dl\n","dt  torch.sin\n","before dts\n","in dl\n","dt  torch.gradient\n","before dts\n","in dl\n","dt  torch.bincount\n","before dts\n","in dl\n","dt  torch.conj\n","before dts\n","in dl\n","dt  torch.positive\n","before dts\n","in dl\n","dt  torch.atan\n","before dts\n","in dl\n","dt  torch.tensor\n","before dts\n","in dl\n","dt  torch.triu_indices\n","before dts\n","in dl\n","dt  torch.remainder\n","before dts\n","in dl\n","dt  torch.eye\n","before dts\n","in dl\n","dt  torch.trunc\n","before dts\n","in dl\n","dt  torch.log2\n","before dts\n","in dl\n","dt  torch.sparse_coo_tensor\n","before dts\n","in dl\n","dt  torch.logical_or\n","before dts\n","in dl\n","dt  torch.solve\n","before dts\n","in dl\n","dt  torch.solve\n","before dts\n","in dl\n","dt  torch.special.entr\n","before dts\n","in dl\n","dt  torch.special.erf\n","before dts\n","in dl\n","dt  torch.special.erfc\n","before dts\n","in dl\n","dt  torch.special.erfinv\n","before dts\n","in dl\n","dt  torch.special.expit\n","before dts\n","in dl\n","dt  torch.special.expm1\n","before dts\n","in dl\n","dt  torch.special.exp2\n","before dts\n","in dl\n","dt  torch.special.gammaln\n","before dts\n","in dl\n","dt  torch.special.i0e\n","before dts\n","in dl\n","dt  torch.special.logit\n","before dts\n","in dl\n","dt  torch.special.xlog1py\n","before dts\n","in dl\n","dt  torch.vander\n","before dts\n","in dl\n","dt  torch.frexp\n","before dts\n","in dl\n","dt  torch.addcdiv\n","before dts\n","in dl\n","dt  torch.moveaxis\n","before dts\n","in dl\n","dt  torch.lgamma\n","before dts\n","in dl\n","dlmdt  torch.futures.Future.add_done_callback\n","before dts\n","in dl\n","dlmdt  torch.futures.Future.set_exception\n","before dts\n","in dl\n","dlmdt  torch.futures.Future.set_result\n","before dts\n","in dl\n","dlmdt  torch.futures.Future.then\n","before dts\n","in dl\n","dt  torch.futures.collect_all\n","before dts\n","in dl\n","dt  torch.fmod\n","before dts\n","in dl\n","dt  torch.dist\n","before dts\n","in dl\n","dt  torch.empty\n","before dts\n","in dl\n","dt  torch.nansum\n","before dts\n","in dl\n","dt  torch.nansum\n","before dts\n","in dl\n","dt  torch.diagonal\n","before dts\n","in dl\n","dt  torch.le\n","before dts\n","from dpps  The context managerstorch.no_grad(),torch.enable_grad(), andtorch.set_grad_enabled()are helpful for locally disabling and enabling\n","gradient computation. SeeLocally disabling gradient computationfor more details on\n","their usage.  These context managers are thread local, so they won’t\n","work if you send work to another thread using thethreadingmodule, etc.\n","before dts\n","in dl\n","dt  torch.rand\n","before dts\n","in dl\n","dt  torch.take_along_dim\n","before dts\n","in dl\n","dt  torch.isposinf\n","before dts\n","in dl\n","dt  torch.logical_and\n","before dts\n","in dl\n","dt  torch.neg\n","before dts\n","in dl\n","dt  torch.trace\n","before dts\n","from dpps  There are two ways to initialize using TCP, both requiring a network address\n","reachable from all processes and a desiredworld_size. The first way\n","requires specifying an address that belongs to the rank 0 process. This\n","initialization method requires that all processes have manually specified ranks.Note that multicast address is not supported anymore in the latest distributed\n","package.group_nameis deprecated as well.\n","before dts\n","from dpps  Another initialization method makes use of a file system that is shared and\n","visible from all machines in a group, along with a desiredworld_size. The URL should start\n","withfile://and contain a path to a non-existent file (in an existing\n","directory) on a shared file system. File-system initialization will automatically\n","create that file if it doesn’t exist, but will not delete the file. Therefore, it\n","is your responsibility to make sure that the file is cleaned up before the nextinit_process_group()call on the same file path/name.Note that automatic rank assignment is not supported anymore in the latest\n","distributed package andgroup_nameis deprecated as well.\n","before dts\n","in dlc\n","before dts\n","in dlc\n","before dts\n","in dlc\n","before dts\n","in dl\n","dt  torch.distributed.Store.set\n","before dts\n","in dl\n","dt  torch.distributed.Store.get\n","before dts\n","in dl\n","dt  torch.distributed.Store.add\n","before dts\n","in dl\n","dt  torch.distributed.Store.wait\n","before dts\n","in dl\n","dt  torch.distributed.Store.wait\n","before dts\n","in dl\n","dt  torch.distributed.Store.num_keys\n","before dts\n","in dl\n","dt  torch.distributed.Store.delete_key\n","before dts\n","in dl\n","dt  torch.distributed.Store.set_timeout\n","before dts\n","from dpps  The following code can serve as a reference regarding semantics for CUDA operations when using distributed collectives.\n","It shows the explicit need to synchronize when using collective outputs on different CUDA streams:\n","before dts\n","in dl\n","dt  torch.distributed.broadcast_object_list\n","before dts\n","in dl\n","dt  torch.distributed.all_reduce\n","before dts\n","in dl\n","dt  torch.distributed.all_reduce\n","before dts\n","in dl\n","dt  torch.distributed.all_gather\n","before dts\n","in dl\n","dt  torch.distributed.all_gather\n","before dts\n","in dl\n","dt  torch.distributed.all_gather_object\n","before dts\n","in dl\n","dt  torch.distributed.gather_object\n","before dts\n","in dl\n","dt  torch.distributed.scatter_object_list\n","before dts\n","in dl\n","dt  torch.distributed.all_to_all\n","before dts\n","in dl\n","dt  torch.distributed.all_to_all\n","before dts\n","in dl\n","dt  torch.distributed.all_to_all\n","before dts\n","from dpps  Note that you can usetorch.profiler(recommended, only available after 1.8.1)  ortorch.autograd.profilerto profile collective communication and point-to-point communication APIs mentioned here. All out-of-the-box backends (gloo,nccl,mpi) are supported and collective communication usage will be rendered as expected in profiling output/traces. Profiling your code is the same as any regular torch operator:\n","before dts\n","from dpps  For example, if the system we use for distributed training has 2 nodes, each\n","of which has 8 GPUs. On each of the 16 GPUs, there is a tensor that we would\n","like to all-reduce. The following code can serve as a reference:Code running on Node 0\n","before dts\n","from dpps  Code running on Node 0Code running on Node 1\n","before dts\n","from dpps  In both cases of single-node distributed training or multi-node distributed\n","training, this utility will launch the given number of processes per node\n","(--nproc_per_node). If used for GPU training, this number needs to be less\n","or equal to the number of GPUs on the current system (nproc_per_node),\n","and each process will be operating on a single GPU fromGPU 0 to\n","GPU (nproc_per_node - 1).How to use this module:\n","before dts\n","from dpps  How to use this module:Node 1:(IP: 192.168.1.1, and has a free port: 1234)\n","before dts\n","from dpps  Node 1:(IP: 192.168.1.1, and has a free port: 1234)Node 2:\n","before dts\n","from dpps  Node 1:(IP: 192.168.1.1, and has a free port: 1234)Node 2:\n","before dts\n","from dpps  2. In your training program, you must parse the command-line argument:--local_rank=LOCAL_PROCESS_RANK, which will be provided by this module.\n","If your training program uses GPUs, you should ensure that your code only\n","runs on the GPU device of LOCAL_PROCESS_RANK. This can be done by:Parsing the local_rank argument\n","before dts\n","from dpps  Parsing the local_rank argumentSet your device to local rank using either\n","before dts\n","from dpps  Set your device to local rank using eitheror\n","before dts\n","from dpps  or3. In your training program, you are supposed to call the following function\n","at the beginning to start the distributed backend. You need to make sure that\n","the init_method usesenv://, which is the only supportedinit_methodby this module.\n","before dts\n","from dpps  3. In your training program, you are supposed to call the following function\n","at the beginning to start the distributed backend. You need to make sure that\n","the init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\n","or usetorch.nn.parallel.DistributedDataParallel()module. If your\n","training program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\n","here is how to configure it.\n","before dts\n","in dl\n","dt  torch.ceil\n","before dts\n","from dpps  The context managerstorch.no_grad(),torch.enable_grad(), andtorch.set_grad_enabled()are helpful for locally disabling and enabling\n","gradient computation. SeeLocally disabling gradient computationfor more details on\n","their usage.  These context managers are thread local, so they won’t\n","work if you send work to another thread using thethreadingmodule, etc.\n","before dts\n","in dl\n","dt  torch.linalg.det\n","before dts\n","in dl\n","dt  torch.nonzero\n","before dts\n","in dl\n","dt  torch.reciprocal\n","before dts\n","from dpps  The context managerstorch.no_grad(),torch.enable_grad(), andtorch.set_grad_enabled()are helpful for locally disabling and enabling\n","gradient computation. SeeLocally disabling gradient computationfor more details on\n","their usage.  These context managers are thread local, so they won’t\n","work if you send work to another thread using thethreadingmodule, etc.\n","before dts\n","in dl\n","dt  torch.sort\n","before dts\n","in dl\n","dt  torch.float_power\n","before dts\n","in dl\n","dt  torch.diag_embed\n","before dts\n","in dl\n","dt  torch.frac\n","before dts\n","in dl\n","dt  torch.linalg.householder_product\n","before dts\n","in dl\n","dt  torch.tril\n","before dts\n","in dl\n","dt  torch.i0\n","before dts\n","in dl\n","dt  torch.hstack\n","before dts\n","in dl\n","dt  torch.pow\n","before dts\n","in dl\n","dt  torch.pow\n","before dts\n","in dl\n","dt  torch.special.entr\n","before dts\n","in dl\n","dt  torch.special.erf\n","before dts\n","in dl\n","dt  torch.special.erfc\n","before dts\n","in dl\n","dt  torch.special.erfinv\n","before dts\n","in dl\n","dt  torch.special.expit\n","before dts\n","in dl\n","dt  torch.special.expm1\n","before dts\n","in dl\n","dt  torch.special.exp2\n","before dts\n","in dl\n","dt  torch.special.gammaln\n","before dts\n","in dl\n","dt  torch.special.i0e\n","before dts\n","in dl\n","dt  torch.special.logit\n","before dts\n","in dl\n","dt  torch.special.xlog1py\n","before dts\n","in dl\n","dt  torch.floor\n","before dts\n","in dlc\n","before dts\n","in dl\n","dt  torch.as_tensor\n","before dts\n","in dl\n","dt  torch.trapz\n","before dts\n","in dl\n","dt  torch.mul\n","before dts\n","in dl\n","dt  torch.mul\n","before dts\n","in dl\n","dt  torch.logsumexp\n","before dts\n","in dl\n","dt  torch.max\n","before dts\n","in dl\n","dt  torch.max\n","before dts\n","in dl\n","dt  torch.utils.model_zoo.load_url\n","before dts\n","in dl\n","dt  torch.mean\n","before dts\n","in dl\n","dt  torch.mean\n","before dts\n","in dl\n","dt  torch.quantile\n","before dts\n","in dl\n","dt  torch.vdot\n","before dts\n","in dl\n","dt  torch.asinh\n","before dts\n","in dl\n","dt  torch.lstsq\n","before dts\n","in dl\n","dt  torch.lstsq\n","before dts\n","in dl\n","dt  torch.view_as_real\n","before dts\n","in dl\n","dt  torch.isfinite\n","before dts\n","in dl\n","dt  torch.flipud\n","before dts\n","in dl\n","dt  torch.full\n","before dts\n","in dlc\n","dlcdt  torch.Generator\n","before dts\n","in dlc\n","dlcdt  torch.Generator\n","before dts\n","in dl\n","dlmdt  torch.Generator.get_state\n","before dts\n","in dl\n","dlmdt  torch.Generator.initial_seed\n","before dts\n","in dl\n","dlmdt  torch.Generator.manual_seed\n","before dts\n","in dl\n","dlmdt  torch.Generator.seed\n","before dts\n","in dl\n","dlmdt  torch.Generator.set_state\n","before dts\n","in dl\n","dt  torch.deg2rad\n","before dts\n","in dl\n","dt  torch.var_mean\n","before dts\n","in dl\n","dt  torch.complex\n","before dts\n","in dl\n","dt  torch.triangular_solve\n","before dts\n","in dl\n","dt  torch.ones\n","before dts\n","in dl\n","dt  torch.t\n","before dts\n","in dl\n","dt  torch.special.entr\n","before dts\n","in dl\n","dt  torch.special.erf\n","before dts\n","in dl\n","dt  torch.special.erfc\n","before dts\n","in dl\n","dt  torch.special.erfinv\n","before dts\n","in dl\n","dt  torch.special.expit\n","before dts\n","in dl\n","dt  torch.special.expm1\n","before dts\n","in dl\n","dt  torch.special.exp2\n","before dts\n","in dl\n","dt  torch.special.gammaln\n","before dts\n","in dl\n","dt  torch.special.i0e\n","before dts\n","in dl\n","dt  torch.special.logit\n","before dts\n","in dl\n","dt  torch.special.xlog1py\n","before dts\n","in dl\n","dt  torch.gt\n","before dts\n","in dl\n","dt  torch.round\n","before dts\n","in dl\n","dt  torch.pca_lowrank\n","before dts\n","in dl\n","dt  torch.narrow\n","before dts\n","in dl\n","dt  torch.min\n","before dts\n","in dl\n","dt  torch.min\n","before dts\n","in dl\n","dt  torch.multinomial\n","before dts\n","from dpps  To get started, let’s look at a simpler, custom version of PyTorch’sLinearmodule.\n","This module applies an affine transformation to its input.\n","before dts\n","from dpps  This simple module has the following fundamental characteristics of modules:This simple module demonstrates how modules package state and computation together. Instances of this module can be\n","constructed and called:\n","before dts\n","from dpps  Note that the module itself is callable, and that calling it invokes itsforward()function.\n","This name is in reference to the concepts of “forward pass” and “backward pass”, which apply to each module.\n","The “forward pass” is responsible for applying the computation represented by the module\n","to the given input(s) (as shown in the above snippet). The “backward pass” computes gradients of\n","module outputs with respect to its inputs, which can be used for “training” parameters through gradient\n","descent methods. PyTorch’s autograd system automatically takes care of this backward pass computation, so it\n","is not required to manually implement abackward()function for each module. The process of training\n","module parameters through successive forward / backward passes is covered in detail inNeural Network Training with Modules.The full set of parameters registered by the module can be iterated through via a call toparameters()ornamed_parameters(),\n","where the latter includes each parameter’s name:\n","before dts\n","from dpps  Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\n","The simplest way to do this is using theSequentialmodule. It allows us to chain together\n","multiple modules:\n","before dts\n","from dpps  In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\n","full flexibility on how submodules are used for a module’s computation.For example, here’s a simple neural network implemented as a custom module:\n","before dts\n","from dpps  For example, here’s a simple neural network implemented as a custom module:This module is composed of two “children” or “submodules” (l0andl1) that define the layers of\n","the neural network and are utilized for computation within the module’sforward()method. Immediate\n","children of a module can be iterated through via a call tochildren()ornamed_children():\n","before dts\n","from dpps  This module is composed of two “children” or “submodules” (l0andl1) that define the layers of\n","the neural network and are utilized for computation within the module’sforward()method. Immediate\n","children of a module can be iterated through via a call tochildren()ornamed_children():To go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules:\n","before dts\n","from dpps  To go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules:Sometimes, it’s necessary for a module to dynamically define submodules.\n","TheModuleListandModuleDictmodules are useful here; they\n","register submodules from a list or dict:\n","before dts\n","from dpps  Sometimes, it’s necessary for a module to dynamically define submodules.\n","TheModuleListandModuleDictmodules are useful here; they\n","register submodules from a list or dict:For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\n","This means that calls toparameters()andnamed_parameters()will\n","recursively include child parameters, allowing for convenient optimization of all parameters within the network:\n","before dts\n","from dpps  For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\n","This means that calls toparameters()andnamed_parameters()will\n","recursively include child parameters, allowing for convenient optimization of all parameters within the network:It’s also easy to move all parameters to a different device or change their precision usingto():\n","before dts\n","from dpps  Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch’s\n","Optimizers fromtorch.optim:\n","before dts\n","from dpps  In this simplified example, the network learns to simply output zero, as any non-zero output is “penalized” according\n","to its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\n","key parts of training are present:After the above snippet has been run, note that the network’s parameters have changed. In particular, examining the\n","value ofl1’sweightparameter shows that its values are now much closer to 0 (as may be expected):\n","before dts\n","from dpps  In the previous section, we demonstrated training a module’s “parameters”, or learnable aspects of computation.\n","Now, if we want to save the trained model to disk, we can do so by saving itsstate_dict(i.e. “state dictionary”):\n","before dts\n","from dpps  A module’sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\n","module’s parameters. For some modules, it may be useful to have state beyond parameters that affects module\n","computation but is not learnable. For such cases, PyTorch provides the concept of “buffers”, both “persistent”\n","and “non-persistent”. Following is an overview of the various types of state a module can have:As a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want\n","the current value of the running mean to be considered part of the module’sstate_dictso that it will be\n","restored when loading a serialized form of the module, but we don’t want it to be learnable.\n","This snippet shows how to useregister_buffer()to accomplish this:\n","before dts\n","from dpps  As a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want\n","the current value of the running mean to be considered part of the module’sstate_dictso that it will be\n","restored when loading a serialized form of the module, but we don’t want it to be learnable.\n","This snippet shows how to useregister_buffer()to accomplish this:Now, the current value of the running mean is considered part of the module’sstate_dictand will be properly restored when loading the module from disk:\n","before dts\n","from dpps  Now, the current value of the running mean is considered part of the module’sstate_dictand will be properly restored when loading the module from disk:As mentioned previously, buffers can be left out of the module’sstate_dictby marking them as non-persistent:\n","before dts\n","from dpps  As mentioned previously, buffers can be left out of the module’sstate_dictby marking them as non-persistent:Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied withto():\n","before dts\n","from dpps  A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor:\n","before dts\n","from dpps  A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor:A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\n","constructor or tensor creation op:\n","before dts\n","from dpps  For more information about building Tensors, seeCreation OpsThe contents of a tensor can be accessed and modified using Python’s indexing\n","and slicing notation:\n","before dts\n","from dpps  The contents of a tensor can be accessed and modified using Python’s indexing\n","and slicing notation:Usetorch.Tensor.item()to get a Python number from a tensor containing a\n","single value:\n","before dts\n","from dpps  For more information about indexing, seeIndexing, Slicing, Joining, Mutating OpsA tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation.\n","before dts\n","in dl\n","dt  torch.flip\n","before dts\n","in dl\n","dt  torch.special.entr\n","before dts\n","in dl\n","dt  torch.special.erf\n","before dts\n","in dl\n","dt  torch.special.erfc\n","before dts\n","in dl\n","dt  torch.special.erfinv\n","before dts\n","in dl\n","dt  torch.special.expit\n","before dts\n","in dl\n","dt  torch.special.expm1\n","before dts\n","in dl\n","dt  torch.special.exp2\n","before dts\n","in dl\n","dt  torch.special.gammaln\n","before dts\n","in dl\n","dt  torch.special.i0e\n","before dts\n","in dl\n","dt  torch.special.logit\n","before dts\n","in dl\n","dt  torch.special.xlog1py\n","before dts\n","in dl\n","dt  torch.svd\n","before dts\n","in dl\n","dt  torch.svd\n","before dts\n","in dl\n","dt  torch.svd\n","before dts\n","in dl\n","dt  torch.real\n","before dts\n","in dl\n","dt  torch.xlogy\n","before dts\n","in dl\n","dt  torch.quantize_per_channel\n","before dts\n","in dlc\n","dlcdt  torch.enable_grad\n","before dts\n","in dl\n","dt  torch.eig\n","before dts\n","in dl\n","dt  torch.eig\n","before dts\n","in dl\n","dt  torch.eig\n","before dts\n","from dpps  At the heart of PyTorch data loading utility is thetorch.utils.data.DataLoaderclass.  It represents a Python iterable over a dataset, with support forThese options are configured by the constructor arguments of aDataLoader, which has signature:\n","before dts\n","from dpps  After fetching a list of samples using the indices from sampler, the function\n","passed as thecollate_fnargument is used to collate lists of samples\n","into batches.In this case, loading from a map-style dataset is roughly equivalent with:\n","before dts\n","from dpps  In this case, loading from a map-style dataset is roughly equivalent with:and loading from an iterable-style dataset is roughly equivalent with:\n","before dts\n","from dpps  When automatic batching is disabled, the defaultcollate_fnsimply\n","converts NumPy arrays into PyTorch Tensors, and keeps everything else untouched.In this case, loading from a map-style dataset is roughly equivalent with:\n","before dts\n","from dpps  In this case, loading from a map-style dataset is roughly equivalent with:and loading from an iterable-style dataset is roughly equivalent with:\n","before dts\n","from dpps  See the example below.\n","before dts\n","in dlc\n","dlcdt  torch.utils.data.IterableDataset\n","before dts\n","in dlc\n","dlcdt  torch.utils.data.IterableDataset\n","before dts\n","in dl\n","dt  torch.utils.data.random_split\n","before dts\n","in dlc\n","dlcdt  torch.utils.data.WeightedRandomSampler\n","before dts\n","in dlc\n","dlcdt  torch.utils.data.BatchSampler\n","before dts\n","in dlc\n","dlcdt  torch.utils.data.distributed.DistributedSampler\n","before dts\n","in dl\n","dt  torch.sum\n","before dts\n","in dl\n","dt  torch.sum\n","before dts\n","in dl\n","dt  torch.bitwise_not\n","before dts\n","in dl\n","dt  torch.vsplit\n","before dts\n","in dl\n","dt  torch.hsplit\n","before dts\n","in dl\n","dt  torch.special.entr\n","before dts\n","in dl\n","dt  torch.special.erf\n","before dts\n","in dl\n","dt  torch.special.erfc\n","before dts\n","in dl\n","dt  torch.special.erfinv\n","before dts\n","in dl\n","dt  torch.special.expit\n","before dts\n","in dl\n","dt  torch.special.expm1\n","before dts\n","in dl\n","dt  torch.special.exp2\n","before dts\n","in dl\n","dt  torch.special.gammaln\n","before dts\n","in dl\n","dt  torch.special.i0e\n","before dts\n","in dl\n","dt  torch.special.logit\n","before dts\n","in dl\n","dt  torch.special.xlog1py\n","before dts\n","in dl\n","dt  torch.fliplr\n","before dts\n","in dl\n","dt  torch.cosh\n","before dts\n","in dl\n","dt  torch.rad2deg\n","before dts\n","in dl\n","dt  torch.broadcast_shapes\n","before dts\n","in dl\n","dt  torch.get_default_dtype\n","before dts\n","in dl\n","dt  torch.cartesian_prod\n","before dts\n","from dpps  The context managerstorch.no_grad(),torch.enable_grad(), andtorch.set_grad_enabled()are helpful for locally disabling and enabling\n","gradient computation. SeeLocally disabling gradient computationfor more details on\n","their usage.  These context managers are thread local, so they won’t\n","work if you send work to another thread using thethreadingmodule, etc.\n","before dts\n","in dl\n","dt  torch.zeros_like\n","before dts\n","in dl\n","dt  torch.flatten\n","before dts\n","in dl\n","dt  torch.bitwise_and\n","before dts\n","in dl\n","dt  torch.digamma\n","before dts\n","from dpps  The context managerstorch.no_grad(),torch.enable_grad(), andtorch.set_grad_enabled()are helpful for locally disabling and enabling\n","gradient computation. SeeLocally disabling gradient computationfor more details on\n","their usage.  These context managers are thread local, so they won’t\n","work if you send work to another thread using thethreadingmodule, etc.\n","before dts\n","from dpps  This feature is under a Beta release and its API may change.FX is a toolkit for developers to use to transformnn.Moduleinstances. FX consists of three main components: asymbolic tracer,anintermediate representation, andPython code generation. A\n","demonstration of these components in action:\n","before dts\n","from dpps  What is an FX transform? Essentially, it’s a function that looks like this.\n","before dts\n","from dpps  NoteIt is also possible to modify an existingGraphModuleinstead of\n","creating a new one, like so:\n","before dts\n","from dpps  Full treatment of the semantics of graphs can be found in theGraphdocumentation, but we are going to cover the basics here. AGraphis\n","a data structure that represents a method on aGraphModule. The\n","information that this requires is:All three of these concepts are represented withNodeinstances.\n","Let’s see what we mean by that with a short example:\n","before dts\n","from dpps  One approach to building this newGraphis to directly manipulate your old\n","one. To aid in this, we can simply take theGraphwe obtain from symbolic\n","tracing and modify it. For example, let’s say we desire to replacetorch.add()calls withtorch.mul()calls.\n","before dts\n","from dpps  One approach to building this newGraphis to directly manipulate your old\n","one. To aid in this, we can simply take theGraphwe obtain from symbolic\n","tracing and modify it. For example, let’s say we desire to replacetorch.add()calls withtorch.mul()calls.We can also do more involvedGraphrewrites, such as\n","deleting or appending nodes. To aid in these transformations,\n","FX has utility functions for transforming the graph that can\n","be found in theGraphdocumentation. An\n","example of using these APIs to append atorch.relu()call\n","can be found below.\n","before dts\n","from dpps  Another way of manipulatingGraphs is by reusing theProxymachinery used in symbolic tracing. For example, let’s\n","imagine that we wanted to write a transformation that decomposed\n","PyTorch functions into smaller operations. It would transform everyF.relu(x)call into(x>0)*x. One possibility would be to\n","perform the requisite graph rewriting to insert the comparison and\n","multiplication after theF.relu, and then clean up the originalF.relu. However, we can automate this process by usingProxyobjects to automatically record operations into theGraph.To use this method, we write the operations that we want inserted as regular\n","PyTorch code and invoke that code withProxyobjects as arugments.\n","TheseProxyobjects will capture the operations that are performed\n","on them and append them to theGraph.\n","before dts\n","from dpps  A useful code organizational pattern in FX is to loop over all theNodes\n","in aGraphand execute them. This can be used for several things including\n","runtime analysis of values flowing through the graph or transformation of the code\n","via retracing withProxys. For example, suppose we want to run aGraphModuleand record thetorch.Tensorshape and dtype\n","properties on the nodes as we see them at runtime. That might look like:\n","before dts\n","from dpps  Because the output of most deep learning modules consists of floating\n","pointtorch.Tensorinstances, checking for equivalence between\n","the results of twotorch.nn.Moduleis not as straightforward\n","as doing a simple equality check. To motivate this, let’s use an\n","example:\n","before dts\n","from dpps  Because the output of most deep learning modules consists of floating\n","pointtorch.Tensorinstances, checking for equivalence between\n","the results of twotorch.nn.Moduleis not as straightforward\n","as doing a simple equality check. To motivate this, let’s use an\n","example:Here, we’ve tried to check equality of the values of two deep learning\n","models with the==equality operator. However, this is not well-\n","defined both due to the issue of that operator returning a tensor\n","and not a bool, but also because comparison of floating point values\n","should use a margin of error (or epsilon) to account for the\n","non-commutativity of floating point operations (seeherefor more\n","details). We can usetorch.allclose()instead, which will give\n","us an approximate comparison taking into account a relative and\n","absolute tolerance threshold:\n","before dts\n","from dpps  Invokepdbto step into the running program. Although the code that\n","represents theGraphis not in any source file, we can still step\n","into it manually usingpdbwhen the forward pass is invoked.\n","before dts\n","from dpps  If you’d like to run the same code multiple times, then it can be\n","a bit tedious to step to the right code withpdb. In that case, one\n","approach is to simply copy-paste the generatedforwardpass into\n","your code and examine it from there.\n","before dts\n","from dpps  GraphModule.to_folder()is a method inGraphModulethat allows\n","you to dump out the generated FX code to a folder. Although copying the\n","forward pass into the code often suffices as inPrint the Generated Code,\n","it may be easier to examine modules and parameters usingto_folder.\n","before dts\n","from dpps  Now that we’ve identified that a transformation is creating incorrect\n","code, it’s time to debug the transformation itself. First, we’ll check\n","theLimitations of Symbolic Tracingsection in the documentation.\n","Once we verify that tracing is working as expected, the goal\n","becomes figuring out what went wrong during ourGraphModuletransformation. There may be a quick answer inWriting Transformations, but, if not, there are several ways to\n","examine our traced module:\n","before dts\n","from dpps  Using the utility functions above, we can compare our traced Module\n","before and after we’ve applied our transformations. Sometimes, a\n","simple visual comparison is enough to trace down a bug. If it’s still\n","not clear what’s going wrong, a debugger likepdbcan be a good\n","next step.Going off of the example above, consider the following code:\n","before dts\n","from dpps  The main limitation of symbolic tracing is it does not currently supportdynamic control flow. That is, loops orifstatements where the\n","condition may depend on the input values of the program.For example, let’s examine the following program:\n","before dts\n","from dpps  On the other hand, so-calledstatic control flowis supported. Static\n","control flow is loops orifstatements whose value cannot change\n","across invocations. Typically, in PyTorch programs, this control flow\n","arises for code making decisions about a model’s architecture based on\n","hyper-parameters. As a concrete example:\n","before dts\n","from dpps  The if-statementifself.do_activationdoes not depend on any\n","function inputs, thus it is static.do_activationcan be considered\n","to be a hyper-parameter, and the traces of different instances ofMyModulewith different values for that parameter have different\n","code. This is a valid pattern that is supported by symbolic tracing.Many instances of dynamic control flow are semantically static control\n","flow. These instances can be made to support symbolic tracing by\n","removing the data dependencies on input values, for example by moving\n","values toModuleattributes or by binding concrete values to arguments\n","during symbolic tracing:\n","before dts\n","from dpps  FX uses__torch_function__as the mechanism by which it intercepts\n","calls (see thetechnical\n","overviewfor more information about this). Some functions, such as builtin Python\n","functions or those in themathmodule, are not covered by__torch_function__, but we would still like to capture them in\n","symbolic tracing. For example:\n","before dts\n","from dpps  FX uses__torch_function__as the mechanism by which it intercepts\n","calls (see thetechnical\n","overviewfor more information about this). Some functions, such as builtin Python\n","functions or those in themathmodule, are not covered by__torch_function__, but we would still like to capture them in\n","symbolic tracing. For example:The error tells us that the built-in functionlenis not supported.\n","We can make it so that functions like this are recorded in the trace as\n","direct calls using thewrap()API:\n","before dts\n","from dpps  TheTracerclass is the class that underlies the\n","implementation ofsymbolic_trace. The behavior of tracing can be\n","customized by subclassing Tracer, like so:\n","before dts\n","from dpps  Leaf Modules are the modules that appear as calls in the symbolic trace\n","rather than being traced through. The default set of leaf modules is the\n","set of standardtorch.nnmodule instances. For example:\n","before dts\n","before dts\n","in dl\n","dt  torch.fx.symbolic_trace\n","before dts\n","in dl\n","dt  torch.fx.symbolic_trace\n","before dts\n","in dl\n","dt  torch.fx.wrap\n","before dts\n","in dl\n","dt  torch.fx.wrap\n","before dts\n","in dlc\n","dlcdt  For example, the following code\n","before dts\n","in dlc\n","dlcdt  Will produce the following Graph:\n","before dts\n","in dl\n","dlmdt  torch.fx.Graph.eliminate_dead_code\n","before dts\n","in dl\n","dlmdt  torch.fx.Graph.eliminate_dead_code\n","before dts\n","in dl\n","dlmdt  torch.fx.Graph.inserting_after\n","before dts\n","in dl\n","dlmdt  torch.fx.Graph.inserting_before\n","before dts\n","in dl\n","dlmdt  torch.fx.Graph.node_copy\n","before dts\n","in dl\n","dlmdt  torch.fx.Node.prepend\n","before dts\n","in dlc\n","dlcdt  Methods in the Interpreter class can be overridden to customize\n","the behavior of execution. The map of overrideable methods\n","in terms of call hierarchy:\n","before dts\n","in dlc\n","dlcdt  Suppose we want to swap all instances oftorch.negwithtorch.sigmoidand vice versa (including theirTensormethod equivalents). We could subclass Interpreter like so:\n","before dts\n","in dlc\n","dlcdt  Suppose we want to swap all instances oftorch.negwithtorch.sigmoidand vice versa (including theirTensormethod equivalents). We could subclassTransformerlike so:\n","before dts\n","in dl\n","dt  torch.fx.replace_pattern\n","before dts\n","in dl\n","dt  torch.fx.replace_pattern\n","before dts\n","in dl\n","dt  torch.fx.replace_pattern\n","before dts\n","in dl\n","dt  torch.fx.replace_pattern\n","before dts\n","in dl\n","dt  torch.fx.replace_pattern\n","before dts\n","in dl\n","dt  torch.split\n","before dts\n","in dl\n","dt  torch.sign\n","before dts\n","in dl\n","dt  torch.utils.cpp_extension.CppExtension\n","before dts\n","in dl\n","dt  torch.utils.cpp_extension.CUDAExtension\n","before dts\n","in dl\n","dt  torch.utils.cpp_extension.load\n","before dts\n","in dl\n","dt  torch.utils.cpp_extension.load_inline\n","before dts\n","from dpps  The context managerstorch.no_grad(),torch.enable_grad(), andtorch.set_grad_enabled()are helpful for locally disabling and enabling\n","gradient computation. SeeLocally disabling gradient computationfor more details on\n","their usage.  These context managers are thread local, so they won’t\n","work if you send work to another thread using thethreadingmodule, etc.\n","before dts\n","in dl\n","dt  torch.repeat_interleave\n","before dts\n","in dl\n","dt  torch.add\n","before dts\n","in dl\n","dt  torch.add\n","before dts\n","in dl\n","dt  torch.kron\n","before dts\n","in dl\n","dt  torch.polar\n","before dts\n","in dl\n","dt  torch.use_deterministic_algorithms\n","before dts\n","in dl\n","dt  torch.addcmul\n","before dts\n","in dl\n","dt  torch.fmax\n","before dts\n","in dl\n","dt  torch.tensor_split\n","before dts\n","in dl\n","dt  torch.std\n","before dts\n","in dl\n","dt  torch.cos\n","before dts\n","from dpps  This is the simplest to apply form of quantization where the weights are\n","quantized ahead of time but the activations are dynamically quantized\n","during inference. This is used for situations where the model execution time\n","is dominated by loading weights from memory rather than computing the matrix\n","multiplications. This is true for for LSTM and Transformer type models with\n","small batch size.Diagram:\n","before dts\n","from dpps  Diagram:API example:\n","before dts\n","from dpps  Static quantization quantizes the weights and activations of the model.  It\n","fuses activations into preceding layers where possible.  It requires\n","calibration with a representative dataset to determine optimal quantization\n","parameters for activations. Post Training Quantization is typically used when\n","both memory bandwidth and compute savings are important with CNNs being a\n","typical use case.  Static quantization is also known as Post Training\n","Quantization or PTQ.Diagram:\n","before dts\n","from dpps  Diagram:\n","before dts\n","from dpps  Quantization Aware Training models the effects of quantization during training\n","allowing for higher accuracy compared to other quantization methods.  During\n","training, all calculations are done in floating point, with fake_quant modules\n","modeling the effects of quantization by clamping and rounding to simulate the\n","effects of INT8.  After model conversion, weights and\n","activations are quantized, and activations are fused into the preceding layer\n","where possible.  It is commonly used with CNNs and yields a higher accuracy\n","compared to static quantization.  Quantization Aware Training is also known as\n","QAT.Diagram:\n","before dts\n","from dpps  Diagram:\n","before dts\n","from dpps  There are multiple quantization types in post training quantization (weight only, dynamic and static) and the configuration is done throughqconfig_dict(an argument of theprepare_fxfunction).\n","before dts\n","from dpps  If you see an error similar to:\n","before dts\n","from dpps  If you see an error similar to:This means that you are trying to pass a non-quantized Tensor to a quantized\n","kernel. A common workaround is to usetorch.quantization.QuantStubto\n","quantize the tensor.  This needs to be done manually in Eager mode quantization.\n","An e2e example:\n","before dts\n","from dpps  If you see an error similar to:\n","before dts\n","from dpps  If you see an error similar to:This means that you are trying to pass a quantized Tensor to a non-quantized\n","kernel. A common workaround is to usetorch.quantization.DeQuantStubto\n","dequantize the tensor.  This needs to be done manually in Eager mode quantization.\n","An e2e example:\n","before dts\n","in dl\n","dt  torch.fake_quantize_per_tensor_affine\n","before dts\n","in dl\n","dt  torch.baddbmm\n","before dts\n","in dl\n","dt  torch.ge\n","before dts\n","in dl\n","dt  torch.all\n","before dts\n","in dl\n","dt  torch.all\n","before dts\n","in dl\n","dt  torch.dot\n","before dts\n","from dpps  Once you’ve installed TensorBoard, these utilities let you log PyTorch models\n","and metrics into a directory for visualization within the TensorBoard UI.\n","Scalars, images, histograms, graphs, and embedding visualizations are all\n","supported for PyTorch models and tensors as well as Caffe2 nets and blobs.The SummaryWriter class is your main entry to log data for consumption\n","and visualization by TensorBoard. For example:\n","before dts\n","from dpps  The SummaryWriter class is your main entry to log data for consumption\n","and visualization by TensorBoard. For example:This can then be visualized with TensorBoard, which should be installable\n","and runnable with:\n","before dts\n","from dpps  This can then be visualized with TensorBoard, which should be installable\n","and runnable with:Lots of information can be logged for one experiment. To avoid cluttering\n","the UI and have better result clustering, we can group plots by naming them\n","hierarchically. For example, “Loss/train” and “Loss/test” will be grouped\n","together, while “Accuracy/train” and “Accuracy/test” will be grouped separately\n","in the TensorBoard interface.\n","before dts\n","in dl\n","dlmdt  torch.utils.tensorboard.writer.SummaryWriter.__init__\n","before dts\n","in dl\n","dlmdt  torch.utils.tensorboard.writer.SummaryWriter.add_scalar\n","before dts\n","in dl\n","dlmdt  torch.utils.tensorboard.writer.SummaryWriter.add_scalars\n","before dts\n","in dl\n","dlmdt  torch.utils.tensorboard.writer.SummaryWriter.add_histogram\n","before dts\n","in dl\n","dlmdt  torch.utils.tensorboard.writer.SummaryWriter.add_image\n","before dts\n","in dl\n","dlmdt  torch.utils.tensorboard.writer.SummaryWriter.add_images\n","before dts\n","in dl\n","dlmdt  torch.utils.tensorboard.writer.SummaryWriter.add_text\n","before dts\n","in dl\n","dlmdt  torch.utils.tensorboard.writer.SummaryWriter.add_embedding\n","before dts\n","in dl\n","dlmdt  torch.utils.tensorboard.writer.SummaryWriter.add_pr_curve\n","before dts\n","in dl\n","dlmdt  torch.utils.tensorboard.writer.SummaryWriter.add_custom_scalars\n","before dts\n","in dl\n","dlmdt  torch.utils.tensorboard.writer.SummaryWriter.add_mesh\n","before dts\n","in dl\n","dlmdt  torch.utils.tensorboard.writer.SummaryWriter.add_hparams\n","before dts\n","in dl\n","dt  torch.linspace\n","before dts\n","in dl\n","dt  torch.lcm\n","before dts\n","from dpps  The default behavior (letting.grads beNonebefore the firstbackward(), such that their layout is created according to 1 or 2,\n","and retained over time according to 3 or 4) is recommended for best performance.\n","Calls tomodel.zero_grad()oroptimizer.zero_grad()will not affect.gradlayouts.In fact, resetting all.grads toNonebefore each\n","accumulation phase, e.g.:\n","before dts\n","in dlc\n","dlcdt  torch.autograd.Function\n","before dts\n","in dlc\n","dlcdt  torch.autograd.profiler.profile\n","before dts\n","in dlc\n","dlcdt  It is useful when running the program under nvprof:\n","before dts\n","in dlc\n","dlcdt  torch.autograd.profiler.emit_nvtx\n","before dts\n","in dlc\n","dlcdt  torch.autograd.detect_anomaly\n","before dts\n","in dl\n","dt  torch.chain_matmul\n","before dts\n","in dl\n","dt  torch.tanh\n","before dts\n","in dl\n","dt  torch.atan2\n","before dts\n","from dpps  The context managerstorch.no_grad(),torch.enable_grad(), andtorch.set_grad_enabled()are helpful for locally disabling and enabling\n","gradient computation. SeeLocally disabling gradient computationfor more details on\n","their usage.  These context managers are thread local, so they won’t\n","work if you send work to another thread using thethreadingmodule, etc.\n","before dts\n","in dl\n","dt  torch.ne\n","before dts\n","in dl\n","dt  torch.msort\n","before dts\n","from dpps  torch.utils.bottleneckis a tool that can be used as an initial step for\n","debugging bottlenecks in your program. It summarizes runs of your script with\n","the Python profiler and PyTorch’s autograd profiler.Run it on the command line with\n","before dts\n","in dl\n","dt  torch.utils.checkpoint.checkpoint_sequential\n","before dts\n","in dl\n","dt  torch.unbind\n","before dts\n","in dl\n","dt  torch.cholesky_inverse\n","before dts\n","in dl\n","dt  torch.lu\n","before dts\n","in dl\n","dt  torch.igamma\n","before dts\n","in dl\n","dt  torch.load\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9m9lSbRgWUGT","executionInfo":{"status":"ok","timestamp":1628683293004,"user_tz":-330,"elapsed":1720,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}},"outputId":"39256253-def2-46d2-c4f2-41fe8f81ce26"},"source":["qalist , len(qalist)"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["([{'Answer': '>>> a = torch.randn(4).uniform_(-1, 1)\\n>>> a\\ntensor([ -0.9385, 0.2968, -0.8591, -0.1871 ])\\n>>> torch.atanh(a)\\ntensor([ -1.7253, 0.3060, -1.2899, -0.1893 ])\\n',\n","   'Id': 1,\n","   'Question': 'How to use torch.atanh, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.atanh.html#torch.atanh'},\n","  {'Answer': '>>> torch.logaddexp(torch.tensor([-1.0]), torch.tensor([-1.0, -2, -3]))\\ntensor([-0.3069, -0.6867, -0.8731])\\n>>> torch.logaddexp(torch.tensor([-100.0, -200, -300]), torch.tensor([-1.0, -2, -3]))\\ntensor([-1., -2., -3.])\\n>>> torch.logaddexp(torch.tensor([1.0, 2000, 30000]), torch.tensor([-1.0, -2, -3]))\\ntensor([1.1269e+00, 2.0000e+03, 3.0000e+04])\\n',\n","   'Id': 2,\n","   'Question': 'How to use torch.logaddexp, give an example?',\n","   'context': ' This op should be disambiguated withtorch.logsumexp()which performs a\\nreduction on a single tensor.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.logaddexp.html#torch.logaddexp'},\n","  {'Answer': '>>> # tensor to tensor comparison\\n>>> expected = torch.tensor([1e0, 1e-1, 1e-2])\\n>>> actual = torch.acos(torch.cos(expected))\\n>>> torch.testing.assert_close(actual, expected)\\n',\n","   'Id': 3,\n","   'Question': 'How to use torch.testing.assert_close, give an example?',\n","   'context': ' Formax_abs_diffandmax_rel_diffthe type depends on thedtypeof the inputs.',\n","   'source': 'https://pytorch.org/docs/stable/testing.html'},\n","  {'Answer': '>>> # scalar to scalar comparison\\n>>> import math\\n>>> expected = math.sqrt(2.0)\\n>>> actual = 2.0 / math.sqrt(2.0)\\n>>> torch.testing.assert_close(actual, expected)\\n',\n","   'Id': 4,\n","   'Question': 'How  Formax_abs_diffandmax_rel_diffthe type depends on thedtypeof the inputs., give an example?',\n","   'context': ' Formax_abs_diffandmax_rel_diffthe type depends on thedtypeof the inputs.',\n","   'source': 'https://pytorch.org/docs/stable/testing.html'},\n","  {'Answer': '>>> # numpy array to numpy array comparison\\n>>> import numpy as np\\n>>> expected = np.array([1e0, 1e-1, 1e-2])\\n>>> actual = np.arccos(np.cos(expected))\\n>>> torch.testing.assert_close(actual, expected)\\n',\n","   'Id': 5,\n","   'Question': 'How  Formax_abs_diffandmax_rel_diffthe type depends on thedtypeof the inputs., give an example?',\n","   'context': ' Formax_abs_diffandmax_rel_diffthe type depends on thedtypeof the inputs.',\n","   'source': 'https://pytorch.org/docs/stable/testing.html'},\n","  {'Answer': '>>> # sequence to sequence comparison\\n>>> import numpy as np\\n>>> # The types of the sequences do not have to match. They only have to have the same\\n>>> # length and their elements have to match.\\n>>> expected = [torch.tensor([1.0]), 2.0, np.array(3.0)]\\n>>> actual = tuple(expected)\\n>>> torch.testing.assert_close(actual, expected)\\n',\n","   'Id': 6,\n","   'Question': 'How  Formax_abs_diffandmax_rel_diffthe type depends on thedtypeof the inputs., give an example?',\n","   'context': ' Formax_abs_diffandmax_rel_diffthe type depends on thedtypeof the inputs.',\n","   'source': 'https://pytorch.org/docs/stable/testing.html'},\n","  {'Answer': '>>> # mapping to mapping comparison\\n>>> from collections import OrderedDict\\n>>> import numpy as np\\n>>> foo = torch.tensor(1.0)\\n>>> bar = 2.0\\n>>> baz = np.array(3.0)\\n>>> # The types and a possible ordering of mappings do not have to match. They only\\n>>> # have to have the same set of keys and their elements have to match.\\n>>> expected = OrderedDict([(\"foo\", foo), (\"bar\", bar), (\"baz\", baz)])\\n>>> actual = {\"baz\": baz, \"bar\": bar, \"foo\": foo}\\n>>> torch.testing.assert_close(actual, expected)\\n',\n","   'Id': 7,\n","   'Question': 'How  Formax_abs_diffandmax_rel_diffthe type depends on thedtypeof the inputs., give an example?',\n","   'context': ' Formax_abs_diffandmax_rel_diffthe type depends on thedtypeof the inputs.',\n","   'source': 'https://pytorch.org/docs/stable/testing.html'},\n","  {'Answer': \">>> # Different input types are never considered close.\\n>>> expected = torch.tensor([1.0, 2.0, 3.0])\\n>>> actual = expected.numpy()\\n>>> torch.testing.assert_close(actual, expected)\\nAssertionError: Except for scalars, type equality is required, but got\\n<class 'numpy.ndarray'> and <class 'torch.Tensor'> instead.\\n>>> # Scalars of different types are an exception and can be compared with\\n>>> # check_dtype=False.\\n>>> torch.testing.assert_close(1.0, 1, check_dtype=False)\\n\",\n","   'Id': 8,\n","   'Question': 'How  Formax_abs_diffandmax_rel_diffthe type depends on thedtypeof the inputs., give an example?',\n","   'context': ' Formax_abs_diffandmax_rel_diffthe type depends on thedtypeof the inputs.',\n","   'source': 'https://pytorch.org/docs/stable/testing.html'},\n","  {'Answer': '>>> # NaN != NaN by default.\\n>>> expected = torch.tensor(float(\"Nan\"))\\n>>> actual = expected.clone()\\n>>> torch.testing.assert_close(actual, expected)\\nAssertionError: Tensors are not close!\\n>>> torch.testing.assert_close(actual, expected, equal_nan=True)\\n',\n","   'Id': 9,\n","   'Question': 'How  Formax_abs_diffandmax_rel_diffthe type depends on thedtypeof the inputs., give an example?',\n","   'context': ' Formax_abs_diffandmax_rel_diffthe type depends on thedtypeof the inputs.',\n","   'source': 'https://pytorch.org/docs/stable/testing.html'},\n","  {'Answer': '>>> # If equal_nan=True, the real and imaginary NaN\\'s of complex inputs have to match.\\n>>> expected = torch.tensor(complex(float(\"NaN\"), 0))\\n>>> actual = torch.tensor(complex(0, float(\"NaN\")))\\n>>> torch.testing.assert_close(actual, expected, equal_nan=True)\\nAssertionError: Tensors are not close!\\n>>> # If equal_nan=\"relaxed\", however, then complex numbers are treated as NaN if any\\n>>> # of the real or imaginary component is NaN.\\n>>> torch.testing.assert_close(actual, expected, equal_nan=\"relaxed\")\\n',\n","   'Id': 10,\n","   'Question': 'How  Formax_abs_diffandmax_rel_diffthe type depends on thedtypeof the inputs., give an example?',\n","   'context': ' Formax_abs_diffandmax_rel_diffthe type depends on thedtypeof the inputs.',\n","   'source': 'https://pytorch.org/docs/stable/testing.html'},\n","  {'Answer': '>>> expected = torch.tensor([1.0, 2.0, 3.0])\\n>>> actual = torch.tensor([1.0, 4.0, 5.0])\\n>>> # The default mismatch message can be overwritten.\\n>>> torch.testing.assert_close(actual, expected, msg=\"Argh, the tensors are not close!\")\\nAssertionError: Argh, the tensors are not close!\\n>>> # The error message can also created at runtime by passing a callable.\\n>>> def custom_msg(actual, expected, diagnostic_info):\\n...     return (\\n...         f\"Argh, we found {diagnostic_info.total_mismatches} mismatches! \"\\n...         f\"That is {diagnostic_info.mismatch_ratio:.1%}!\"\\n...     )\\n>>> torch.testing.assert_close(actual, expected, msg=custom_msg)\\nAssertionError: Argh, we found 2 mismatches! That is 66.7%!\\n',\n","   'Id': 11,\n","   'Question': 'How  Formax_abs_diffandmax_rel_diffthe type depends on thedtypeof the inputs., give an example?',\n","   'context': ' Formax_abs_diffandmax_rel_diffthe type depends on thedtypeof the inputs.',\n","   'source': 'https://pytorch.org/docs/stable/testing.html'},\n","  {'Answer': '>>> a = torch.randn(3)\\n>>> a\\ntensor([ 0.5950,-0.0872, 2.3298])\\n>>> torch.diag(a)\\ntensor([[ 0.5950, 0.0000, 0.0000],\\n        [ 0.0000,-0.0872, 0.0000],\\n        [ 0.0000, 0.0000, 2.3298]])\\n>>> torch.diag(a, 1)\\ntensor([[ 0.0000, 0.5950, 0.0000, 0.0000],\\n        [ 0.0000, 0.0000,-0.0872, 0.0000],\\n        [ 0.0000, 0.0000, 0.0000, 2.3298],\\n        [ 0.0000, 0.0000, 0.0000, 0.0000]])\\n',\n","   'Id': 12,\n","   'Question': 'How to use torch.diag, give an example?',\n","   'context': ' Get the square matrix where the input vector is the diagonal:',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.diag.html#torch.diag'},\n","  {'Answer': '>>> a = torch.randn(3, 3)\\n>>> a\\ntensor([[-0.4264, 0.0255,-0.1064],\\n        [ 0.8795,-0.2429, 0.1374],\\n        [ 0.1029,-0.6482,-1.6300]])\\n>>> torch.diag(a, 0)\\ntensor([-0.4264,-0.2429,-1.6300])\\n>>> torch.diag(a, 1)\\ntensor([ 0.0255, 0.1374])\\n',\n","   'Id': 13,\n","   'Question': 'How  Get the square matrix where the input vector is the diagonal:Get the k-th diagonal of a given matrix:, give an example?',\n","   'context': ' Get the square matrix where the input vector is the diagonal:Get the k-th diagonal of a given matrix:',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.diag.html#torch.diag'},\n","  {'Answer': '>>> x = torch.zeros(1, requires_grad=True)\\n>>> with torch.no_grad():\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> is_train = False\\n>>> with torch.set_grad_enabled(is_train):\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> torch.set_grad_enabled(True)  # this can also be used as a function\\n>>> y = x * 2\\n>>> y.requires_grad\\nTrue\\n\\n>>> torch.set_grad_enabled(False)\\n>>> y = x * 2\\n>>> y.requires_grad\\nFalse\\n',\n","   'Id': 14,\n","   'Question': 'How to use The context managerstorch.no_grad(),torch.enable_grad(), andtorch.set_grad_enabled()are helpful for locally disabling and enabling\\ngradient computation. SeeLocally disabling gradient computationfor more details on\\ntheir usage.  These context managers are thread local, so they won’t\\nwork if you send work to another thread using thethreadingmodule, etc., give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/torch.html#math-operations'},\n","  {'Answer': '>>> torch.Tensor.as_subclass in torch.overrides.get_ignored_functions()\\nTrue\\n>>> torch.add in torch.overrides.get_ignored_functions()\\nFalse\\n',\n","   'Id': 15,\n","   'Question': 'How to use torch.overrides.get_ignored_functions, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/torch.overrides.html'},\n","  {'Answer': '>>> import inspect\\n>>> my_add = torch.overrides.get_testing_overrides()[torch.add]\\n>>> inspect.signature(my_add)\\n<Signature (input, other, out=None)>\\n',\n","   'Id': 16,\n","   'Question': 'How to use torch.overrides.get_testing_overrides, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/torch.overrides.html'},\n","  {'Answer': '>>> def func(a):\\n...     if type(a) is not torch.Tensor:  # This will make func dispatchable by __torch_function__\\n...         return handle_torch_function(func, (a,), a)\\n...     return a + 0\\n',\n","   'Id': 17,\n","   'Question': 'How to use torch.overrides.handle_torch_function, give an example?',\n","   'context': ' :raises TypeError : if no implementation is found.:',\n","   'source': 'https://pytorch.org/docs/stable/torch.overrides.html'},\n","  {'Answer': '>>> class SubTensor(torch.Tensor): ...\\n>>> is_tensor_like(SubTensor([0]))\\nTrue\\n',\n","   'Id': 18,\n","   'Question': 'How to use torch.overrides.is_tensor_like, give an example?',\n","   'context': ' A subclass of tensor is generally a Tensor-like.',\n","   'source': 'https://pytorch.org/docs/stable/torch.overrides.html'},\n","  {'Answer': '>>> is_tensor_like(6)\\nFalse\\n>>> is_tensor_like(None)\\nFalse\\n>>> class NotATensor: ...\\n>>> is_tensor_like(NotATensor())\\nFalse\\n',\n","   'Id': 19,\n","   'Question': 'How  A subclass of tensor is generally a Tensor-like.Built-in or user types aren’t usually Tensor-like., give an example?',\n","   'context': ' A subclass of tensor is generally a Tensor-like.Built-in or user types aren’t usually Tensor-like.',\n","   'source': 'https://pytorch.org/docs/stable/torch.overrides.html'},\n","  {'Answer': '>>> class TensorLike:\\n...     def __torch_function__(self, func, types, args, kwargs):\\n...         return -1\\n>>> is_tensor_like(TensorLike())\\nTrue\\n',\n","   'Id': 20,\n","   'Question': 'How  Built-in or user types aren’t usually Tensor-like.But, they can be made Tensor-like by implementing __torch_function__., give an example?',\n","   'context': ' Built-in or user types aren’t usually Tensor-like.But, they can be made Tensor-like by implementing __torch_function__.',\n","   'source': 'https://pytorch.org/docs/stable/torch.overrides.html'},\n","  {'Answer': '>>> is_tensor_method_or_property(torch.Tensor.add)\\nTrue\\n>>> is_tensor_method_or_property(torch.add)\\nFalse\\n',\n","   'Id': 21,\n","   'Question': 'How to use torch.overrides.is_tensor_method_or_property, give an example?',\n","   'context': ' This may be needed, in particular, for the following reasons:',\n","   'source': 'https://pytorch.org/docs/stable/torch.overrides.html'},\n","  {'Answer': '>>> def dispatcher(a): # Must have the same signature as func\\n...     return (a,)\\n>>> @torch.overrides.wrap_torch_function(dispatcher)\\n>>> def func(a): # This will make func dispatchable by __torch_function__\\n...     return a + 0\\n',\n","   'Id': 22,\n","   'Question': 'How to use torch.overrides.wrap_torch_function, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/torch.overrides.html'},\n","  {'Answer': '>>> a = torch.empty(2, 3).uniform_(1, 2)\\n>>> a\\ntensor([[1.6835, 1.8474, 1.1929],\\n        [1.0475, 1.7162, 1.4180]])\\n>>> torch.mvlgamma(a, 2)\\ntensor([[0.3928, 0.4007, 0.7586],\\n        [1.0311, 0.3901, 0.5049]])\\n',\n","   'Id': 23,\n","   'Question': 'How to use torch.mvlgamma, give an example?',\n","   'context': ' All elements must be greater thanp−12\\\\frac{p - 1}{2}2p−1\\u200b, otherwise an error would be thrown.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.mvlgamma.html#torch.mvlgamma'},\n","  {'Answer': '>>> x = torch.randn(2, 2, 2)\\n>>> x\\ntensor([[[-0.2525, -0.0466],\\n         [ 0.3491, -0.2168]],\\n\\n        [[-0.5906,  1.6258],\\n         [ 0.6444, -0.0542]]])\\n>>> scales = (torch.randn(2) + 1) * 0.05\\n>>> scales\\ntensor([0.0475, 0.0486])\\n>>> zero_points = torch.zeros(2).to(torch.long)\\n>>> zero_points\\ntensor([0, 0])\\n>>> torch.fake_quantize_per_channel_affine(x, scales, zero_points, 1, 0, 255)\\ntensor([[[0.0000, 0.0000],\\n         [0.3405, 0.0000]],\\n\\n        [[0.0000, 1.6134],\\n        [0.6323, 0.0000]]])\\n',\n","   'Id': 24,\n","   'Question': 'How to use torch.fake_quantize_per_channel_affine, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.fake_quantize_per_channel_affine.html#torch.fake_quantize_per_channel_affine'},\n","  {'Answer': '>>> x = torch.randn(2)\\n>>> x\\ntensor([1.4584, 0.7583])\\n>>> torch.atleast_1d(x)\\ntensor([1.4584, 0.7583])\\n>>> x = torch.tensor(1.)\\n>>> x\\ntensor(1.)\\n>>> torch.atleast_1d(x)\\ntensor([1.])\\n>>> x = torch.tensor(0.5)\\n>>> y = torch.tensor(1.)\\n>>> torch.atleast_1d((x,y))\\n(tensor([0.5000]), tensor([1.]))\\n',\n","   'Id': 25,\n","   'Question': 'How to use torch.atleast_1d, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.atleast_1d.html#torch.atleast_1d'},\n","  {'Answer': '>>> input = torch.tensor([-1.5, 0, 2.0])\\n>>> values = torch.tensor([0.5])\\n>>> torch.heaviside(input, values)\\ntensor([0.0000, 0.5000, 1.0000])\\n>>> values = torch.tensor([1.2, -2.0, 3.5])\\n>>> torch.heaviside(input, values)\\ntensor([0., -2., 1.])\\n',\n","   'Id': 26,\n","   'Question': 'How to use torch.heaviside, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.heaviside.html#torch.heaviside'},\n","  {'Answer': '>>> x = torch.tensor(1.)\\n>>> x\\ntensor(1.)\\n>>> torch.atleast_2d(x)\\ntensor([[1.]])\\n>>> x = torch.randn(2,2)\\n>>> x\\ntensor([[2.2086, 2.5165],\\n        [0.1757, 0.5194]])\\n>>> torch.atleast_2d(x)\\ntensor([[2.2086, 2.5165],\\n        [0.1757, 0.5194]])\\n>>> x = torch.tensor(0.5)\\n>>> y = torch.tensor(1.)\\n>>> torch.atleast_2d((x,y))\\n(tensor([[0.5000]]), tensor([[1.]]))\\n',\n","   'Id': 27,\n","   'Question': 'How to use torch.atleast_2d, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.atleast_2d.html#torch.atleast_2d'},\n","  {'Answer': '>>> torch.ldexp(torch.tensor([1.]), torch.tensor([1]))\\ntensor([2.])\\n>>> torch.ldexp(torch.tensor([1.0]), torch.tensor([1, 2, 3, 4]))\\ntensor([ 2.,  4.,  8., 16.])\\n',\n","   'Id': 28,\n","   'Question': 'How to use torch.ldexp, give an example?',\n","   'context': ' Typically this function is used to construct floating point numbers by multiplying\\nmantissas ininputwith integral powers of two created from the exponents\\nin :attr:’other’.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.ldexp.html#torch.ldexp'},\n","  {'Answer': '>>> torch.promote_types(torch.int32, torch.float32)\\ntorch.float32\\n>>> torch.promote_types(torch.uint8, torch.long)\\ntorch.long\\n',\n","   'Id': 29,\n","   'Question': 'How to use torch.promote_types, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.promote_types.html#torch.promote_types'},\n","  {'Answer': '>>> eps = torch.finfo(torch.float32).eps\\n>>> torch.nextafter(torch.tensor([1.0, 2.0]), torch.tensor([2.0, 1.0])) == torch.tensor([eps + 1, 2 - eps])\\ntensor([True, True])\\n',\n","   'Id': 30,\n","   'Question': 'How to use torch.nextafter, give an example?',\n","   'context': ' The shapes ofinputandothermust bebroadcastable.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.nextafter.html#torch.nextafter'},\n","  {'Answer': '>>> torch.tensor([1.2, 3]).dtype    # initial default for floating point is torch.float32\\ntorch.float32\\n>>> torch.set_default_tensor_type(torch.DoubleTensor)\\n>>> torch.tensor([1.2, 3]).dtype    # a new floating point tensor\\ntorch.float64\\n',\n","   'Id': 31,\n","   'Question': 'How to use torch.set_default_tensor_type, give an example?',\n","   'context': ' The default floating point tensor type is initiallytorch.FloatTensor.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.set_default_tensor_type.html#torch.set_default_tensor_type'},\n","  {'Answer': '>>> a = torch.randn(3)\\n>>> a\\ntensor([-0.2956, -0.9068,  0.1695])\\n>>> torch.diagflat(a)\\ntensor([[-0.2956,  0.0000,  0.0000],\\n        [ 0.0000, -0.9068,  0.0000],\\n        [ 0.0000,  0.0000,  0.1695]])\\n>>> torch.diagflat(a, 1)\\ntensor([[ 0.0000, -0.2956,  0.0000,  0.0000],\\n        [ 0.0000,  0.0000, -0.9068,  0.0000],\\n        [ 0.0000,  0.0000,  0.0000,  0.1695],\\n        [ 0.0000,  0.0000,  0.0000,  0.0000]])\\n\\n>>> a = torch.randn(2, 2)\\n>>> a\\ntensor([[ 0.2094, -0.3018],\\n        [-0.1516,  1.9342]])\\n>>> torch.diagflat(a)\\ntensor([[ 0.2094,  0.0000,  0.0000,  0.0000],\\n        [ 0.0000, -0.3018,  0.0000,  0.0000],\\n        [ 0.0000,  0.0000, -0.1516,  0.0000],\\n        [ 0.0000,  0.0000,  0.0000,  1.9342]])\\n',\n","   'Id': 32,\n","   'Question': 'How to use torch.diagflat, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.diagflat.html#torch.diagflat'},\n","  {'Answer': '>>> A = torch.randn(2, 3, 3)\\n>>> A_LU, pivots = A.lu()\\n>>> P, A_L, A_U = torch.lu_unpack(A_LU, pivots)\\n>>>\\n>>> # can recover A from factorization\\n>>> A_ = torch.bmm(P, torch.bmm(A_L, A_U))\\n\\n>>> # LU factorization of a rectangular matrix:\\n>>> A = torch.randn(2, 3, 2)\\n>>> A_LU, pivots = A.lu()\\n>>> P, A_L, A_U = torch.lu_unpack(A_LU, pivots)\\n>>> P\\ntensor([[[1., 0., 0.],\\n         [0., 1., 0.],\\n         [0., 0., 1.]],\\n\\n        [[0., 0., 1.],\\n         [0., 1., 0.],\\n         [1., 0., 0.]]])\\n>>> A_L\\ntensor([[[ 1.0000,  0.0000],\\n         [ 0.4763,  1.0000],\\n         [ 0.3683,  0.1135]],\\n\\n        [[ 1.0000,  0.0000],\\n         [ 0.2957,  1.0000],\\n         [-0.9668, -0.3335]]])\\n>>> A_U\\ntensor([[[ 2.1962,  1.0881],\\n         [ 0.0000, -0.8681]],\\n\\n        [[-1.0947,  0.3736],\\n         [ 0.0000,  0.5718]]])\\n>>> A_ = torch.bmm(P, torch.bmm(A_L, A_U))\\n>>> torch.norm(A_ - A)\\ntensor(2.9802e-08)\\n',\n","   'Id': 33,\n","   'Question': 'How to use torch.lu_unpack, give an example?',\n","   'context': ' Returns a tuple of tensors as(thePtensor(permutationmatrix),theLtensor,theUtensor).',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.lu_unpack.html#torch.lu_unpack'},\n","  {'Answer': '>>> M = torch.randn(2)\\n>>> mat = torch.randn(2, 3)\\n>>> vec = torch.randn(3)\\n>>> torch.addmv(M, mat, vec)\\ntensor([-0.3768, -5.5565])\\n',\n","   'Id': 34,\n","   'Question': 'How to use torch.addmv, give an example?',\n","   'context': ' For inputs of typeFloatTensororDoubleTensor, argumentsbetaandalphamust be real numbers, otherwise they should be integers',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.addmv.html#torch.addmv'},\n","  {'Answer': \">>> a = torch.tensor([1, float('nan'), 3, 2])\\n>>> a.median()\\ntensor(nan)\\n>>> a.nanmedian()\\ntensor(2.)\\n\",\n","   'Id': 35,\n","   'Question': 'How to use torch.nanmedian, give an example?',\n","   'context': ' This function is identical totorch.median()when there are noNaNvalues ininput.\\nWheninputhas one or moreNaNvalues,torch.median()will always returnNaN,\\nwhile this function will return the median of the non-NaNelements ininput.\\nIf all the elements ininputareNaNit will also returnNaN.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.nanmedian.html#torch.nanmedian'},\n","  {'Answer': \">>> a = torch.tensor([[2, 3, 1], [float('nan'), 1, float('nan')]])\\n>>> a\\ntensor([[2., 3., 1.],\\n        [nan, 1., nan]])\\n>>> a.median(0)\\ntorch.return_types.median(values=tensor([nan, 1., nan]), indices=tensor([1, 1, 1]))\\n>>> a.nanmedian(0)\\ntorch.return_types.nanmedian(values=tensor([2., 1., 1.]), indices=tensor([0, 1, 0]))\\n\",\n","   'Id': 36,\n","   'Question': 'How  This function is identical totorch.median()when there are noNaNvalues in a reduced row. When a reduced row has\\none or moreNaNvalues,torch.median()will always reduce it toNaN, while this function will reduce it to the\\nmedian of the non-NaNelements. If all the elements in a reduced row areNaNthen it will be reduced toNaN, too., give an example?',\n","   'context': ' This function is identical totorch.median()when there are noNaNvalues in a reduced row. When a reduced row has\\none or moreNaNvalues,torch.median()will always reduce it toNaN, while this function will reduce it to the\\nmedian of the non-NaNelements. If all the elements in a reduced row areNaNthen it will be reduced toNaN, too.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.nanmedian.html#torch.nanmedian'},\n","  {'Answer': '>>> a = torch.empty_strided((2, 3), (1, 2))\\n>>> a\\ntensor([[8.9683e-44, 4.4842e-44, 5.1239e+07],\\n        [0.0000e+00, 0.0000e+00, 3.0705e-41]])\\n>>> a.stride()\\n(1, 2)\\n>>> a.size()\\ntorch.Size([2, 3])\\n',\n","   'Id': 37,\n","   'Question': 'How to use torch.empty_strided, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.empty_strided.html#torch.empty_strided'},\n","  {'Answer': \">>> torch.isnan(torch.tensor([1, float('nan'), 2]))\\ntensor([False, True, False])\\n\",\n","   'Id': 38,\n","   'Question': 'How to use torch.isnan, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.isnan.html#torch.isnan'},\n","  {'Answer': '>>> x = torch.tensor(0.5)\\n>>> x\\ntensor(0.5000)\\n>>> torch.atleast_3d(x)\\ntensor([[[0.5000]]])\\n>>> y = torch.randn(2,2)\\n>>> y\\ntensor([[-0.8079,  0.7460],\\n        [-1.1647,  1.4734]])\\n>>> torch.atleast_3d(y)\\ntensor([[[-0.8079],\\n        [ 0.7460]],\\n\\n        [[-1.1647],\\n        [ 1.4734]]])\\n>>> x = torch.randn(1,1,1)\\n>>> x\\ntensor([[[-1.5689]]])\\n>>> torch.atleast_3d(x)\\ntensor([[[-1.5689]]])\\n>>> x = torch.tensor(0.5)\\n>>> y = torch.tensor(1.)\\n>>> torch.atleast_3d((x,y))\\n(tensor([[[0.5000]]]), tensor([[[1.]]]))\\n',\n","   'Id': 39,\n","   'Question': 'How to use torch.atleast_3d, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.atleast_3d.html#torch.atleast_3d'},\n","  {'Answer': '>>> torch.bitwise_xor(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))\\ntensor([-2, -2,  0], dtype=torch.int8)\\n>>> torch.bitwise_xor(torch.tensor([True, True, False]), torch.tensor([False, True, False]))\\ntensor([ True, False, False])\\n',\n","   'Id': 40,\n","   'Question': 'How to use torch.bitwise_xor, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.bitwise_xor.html#torch.bitwise_xor'},\n","  {'Answer': '>>> a = torch.randn(4)\\n>>> a\\ntensor([ 0.5380, -0.8632, -0.1265,  0.9399])\\n>>> torch.sinh(a)\\ntensor([ 0.5644, -0.9744, -0.1268,  1.0845])\\n',\n","   'Id': 41,\n","   'Question': 'How to use torch.sinh, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.sinh.html#torch.sinh'},\n","  {'Answer': '>>> x = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8]).view(4, 2)\\n>>> x\\ntensor([[1, 2],\\n        [3, 4],\\n        [5, 6],\\n        [7, 8]])\\n>>> torch.roll(x, 1, 0)\\ntensor([[7, 8],\\n        [1, 2],\\n        [3, 4],\\n        [5, 6]])\\n>>> torch.roll(x, -1, 0)\\ntensor([[3, 4],\\n        [5, 6],\\n        [7, 8],\\n        [1, 2]])\\n>>> torch.roll(x, shifts=(2, 1), dims=(0, 1))\\ntensor([[6, 5],\\n        [8, 7],\\n        [2, 1],\\n        [4, 3]])\\n',\n","   'Id': 42,\n","   'Question': 'How to use torch.roll, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.roll.html#torch.roll'},\n","  {'Answer': '>>> x = torch.tensor([1, 2, 3])\\n>>> x.tile((2,))\\ntensor([1, 2, 3, 1, 2, 3])\\n>>> y = torch.tensor([[1, 2], [3, 4]])\\n>>> torch.tile(y, (2, 2))\\ntensor([[1, 2, 1, 2],\\n        [3, 4, 3, 4],\\n        [1, 2, 1, 2],\\n        [3, 4, 3, 4]])\\n',\n","   'Id': 43,\n","   'Question': 'How to use torch.tile, give an example?',\n","   'context': ' Analogously, ifinputhas fewer dimensions thanrepsspecifies, theninputis treated as if it were unsqueezed at\\ndimension zero until it has as many dimensions asrepsspecifies.\\nFor example, ifinputhas shape (4, 2) andrepsis (3, 3, 2, 2), theninputis treated as if it had the\\nshape (1, 1, 4, 2).',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile'},\n","  {'Answer': '>>> x = torch.arange(1., 6.)\\n>>> x\\ntensor([ 1.,  2.,  3.,  4.,  5.])\\n>>> torch.topk(x, 3)\\ntorch.return_types.topk(values=tensor([5., 4., 3.]), indices=tensor([4, 3, 2]))\\n',\n","   'Id': 44,\n","   'Question': 'How to use torch.topk, give an example?',\n","   'context': ' The boolean optionsortedifTrue, will make sure that the returnedkelements are themselves sorted',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk'},\n","  {'Answer': '>>> a = torch.randn(4)\\n>>> a\\ntensor([-2.0755,  1.0226,  0.0831,  0.4806])\\n>>> torch.sqrt(a)\\ntensor([    nan,  1.0112,  0.2883,  0.6933])\\n',\n","   'Id': 45,\n","   'Question': 'How to use torch.sqrt, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.sqrt.html#torch.sqrt'},\n","  {'Answer': '>>> a = torch.tensor((1, 2, -1))\\n>>> b = torch.tensor((3, 0, 4))\\n>>> torch.minimum(a, b)\\ntensor([1, 0, -1])\\n',\n","   'Id': 46,\n","   'Question': 'How to use torch.minimum, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.minimum.html#torch.minimum'},\n","  {'Answer': '>>> a = torch.tensor([4.0, 3.0])\\n>>> b = torch.tensor([2.0, 2.0])\\n>>> torch.floor_divide(a, b)\\ntensor([2.0, 1.0])\\n>>> torch.floor_divide(a, 1.4)\\ntensor([2.0, 2.0])\\n',\n","   'Id': 47,\n","   'Question': 'How to use torch.floor_divide, give an example?',\n","   'context': ' Supports broadcasting to a common shape, type promotion, and integer and float inputs.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.floor_divide.html#torch.floor_divide'},\n","  {'Answer': '>>> t = torch.tensor([3+4j, 7-24j, 0, 1+2j])\\n>>> t.sgn()\\ntensor([0.6000+0.8000j, 0.2800-0.9600j, 0.0000+0.0000j, 0.4472+0.8944j])\\n',\n","   'Id': 48,\n","   'Question': 'How to use torch.sgn, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.sgn.html#torch.sgn'},\n","  {'Answer': '>>> input = torch.empty(2, 3)\\n>>> torch.ones_like(input)\\ntensor([[ 1.,  1.,  1.],\\n        [ 1.,  1.,  1.]])\\n',\n","   'Id': 49,\n","   'Question': 'How to use torch.ones_like, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.ones_like.html#torch.ones_like'},\n","  {'Answer': '>>> x = torch.tensor([1], requires_grad=True)\\n>>> with torch.no_grad():\\n...   y = x * 2\\n>>> y.requires_grad\\nFalse\\n>>> @torch.no_grad()\\n... def doubler(x):\\n...     return x * 2\\n>>> z = doubler(x)\\n>>> z.requires_grad\\nFalse\\n',\n","   'Id': 50,\n","   'Question': 'How to use torch.no_grad, give an example?',\n","   'context': ' Also functions as a decorator. (Make sure to instantiate with parenthesis.)',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad'},\n","  {'Answer': '>>> x=torch.randn(4, 2)\\n>>> x\\ntensor([[ 1.6116, -0.5772],\\n        [-1.4606, -0.9120],\\n        [ 0.0786, -1.7497],\\n        [-0.6561, -1.6623]])\\n>>> torch.view_as_complex(x)\\ntensor([(1.6116-0.5772j), (-1.4606-0.9120j), (0.0786-1.7497j), (-0.6561-1.6623j)])\\n',\n","   'Id': 51,\n","   'Question': 'How to use torch.view_as_complex, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.view_as_complex.html#torch.view_as_complex'},\n","  {'Answer': '>>> x = torch.tensor([[[0,1],[2,3]],[[4,5],[6,7]]])\\n>>> x\\ntensor([[[0, 1],\\n        [2, 3]],\\n\\n        [[4, 5],\\n        [6, 7]]])\\n>>> torch.swapaxes(x, 0, 1)\\ntensor([[[0, 1],\\n        [4, 5]],\\n\\n        [[2, 3],\\n        [6, 7]]])\\n>>> torch.swapaxes(x, 0, 2)\\ntensor([[[0, 4],\\n        [2, 6]],\\n\\n        [[1, 5],\\n        [3, 7]]])\\n',\n","   'Id': 52,\n","   'Question': 'How to use torch.swapaxes, give an example?',\n","   'context': ' This function is equivalent to NumPy’s swapaxes function.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.swapaxes.html#torch.swapaxes'},\n","  {'Answer': '>>> torch.logical_xor(torch.tensor([True, False, True]), torch.tensor([True, False, False]))\\ntensor([False, False,  True])\\n>>> a = torch.tensor([0, 1, 10, 0], dtype=torch.int8)\\n>>> b = torch.tensor([4, 0, 1, 0], dtype=torch.int8)\\n>>> torch.logical_xor(a, b)\\ntensor([ True,  True, False, False])\\n>>> torch.logical_xor(a.double(), b.double())\\ntensor([ True,  True, False, False])\\n>>> torch.logical_xor(a.double(), b)\\ntensor([ True,  True, False, False])\\n>>> torch.logical_xor(a, b, out=torch.empty(4, dtype=torch.bool))\\ntensor([ True,  True, False, False])\\n',\n","   'Id': 53,\n","   'Question': 'How to use torch.logical_xor, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.logical_xor.html#torch.logical_xor'},\n","  {'Answer': '>>> float_tensor = torch.ones(1, dtype=torch.float)\\n>>> double_tensor = torch.ones(1, dtype=torch.double)\\n>>> complex_float_tensor = torch.ones(1, dtype=torch.complex64)\\n>>> complex_double_tensor = torch.ones(1, dtype=torch.complex128)\\n>>> int_tensor = torch.ones(1, dtype=torch.int)\\n>>> long_tensor = torch.ones(1, dtype=torch.long)\\n>>> uint_tensor = torch.ones(1, dtype=torch.uint8)\\n>>> double_tensor = torch.ones(1, dtype=torch.double)\\n>>> bool_tensor = torch.ones(1, dtype=torch.bool)\\n# zero-dim tensors\\n>>> long_zerodim = torch.tensor(1, dtype=torch.long)\\n>>> int_zerodim = torch.tensor(1, dtype=torch.int)\\n\\n>>> torch.add(5, 5).dtype\\ntorch.int64\\n# 5 is an int64, but does not have higher category than int_tensor so is not considered.\\n>>> (int_tensor + 5).dtype\\ntorch.int32\\n>>> (int_tensor + long_zerodim).dtype\\ntorch.int32\\n>>> (long_tensor + int_tensor).dtype\\ntorch.int64\\n>>> (bool_tensor + long_tensor).dtype\\ntorch.int64\\n>>> (bool_tensor + uint_tensor).dtype\\ntorch.uint8\\n>>> (float_tensor + double_tensor).dtype\\ntorch.float64\\n>>> (complex_float_tensor + complex_double_tensor).dtype\\ntorch.complex128\\n>>> (bool_tensor + int_tensor).dtype\\ntorch.int32\\n# Since long is a different kind than float, result dtype only needs to be large enough\\n# to hold the float.\\n>>> torch.add(long_tensor, float_tensor).dtype\\ntorch.float32\\n',\n","   'Id': 54,\n","   'Question': 'How to use A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\\nare not yet supported., give an example?',\n","   'context': ' A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\\nare not yet supported.',\n","   'source': 'https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype'},\n","  {'Answer': \"# allowed:\\n>>> float_tensor *= float_tensor\\n>>> float_tensor *= int_tensor\\n>>> float_tensor *= uint_tensor\\n>>> float_tensor *= bool_tensor\\n>>> float_tensor *= double_tensor\\n>>> int_tensor *= long_tensor\\n>>> int_tensor *= uint_tensor\\n>>> uint_tensor *= int_tensor\\n\\n# disallowed (RuntimeError: result type can't be cast to the desired output type):\\n>>> int_tensor *= float_tensor\\n>>> bool_tensor *= int_tensor\\n>>> bool_tensor *= uint_tensor\\n>>> float_tensor *= complex_float_tensor\\n\",\n","   'Id': 55,\n","   'Question': 'How to use torch dtype, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype'},\n","  {'Answer': \">>> torch.device('cuda:0')\\ndevice(type='cuda', index=0)\\n\\n>>> torch.device('cpu')\\ndevice(type='cpu')\\n\\n>>> torch.device('cuda')  # current cuda device\\ndevice(type='cuda')\\n\",\n","   'Id': 56,\n","   'Question': 'How to use Atorch.devicecan be constructed via a string or via a string and device ordinalVia a string:, give an example?',\n","   'context': ' Atorch.devicecan be constructed via a string or via a string and device ordinalVia a string:',\n","   'source': 'https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype'},\n","  {'Answer': \">>> torch.device('cuda', 0)\\ndevice(type='cuda', index=0)\\n\\n>>> torch.device('cpu', 0)\\ndevice(type='cpu', index=0)\\n\",\n","   'Id': 57,\n","   'Question': 'How to use Via a string:Via a string and device ordinal:, give an example?',\n","   'context': ' Via a string:Via a string and device ordinal:',\n","   'source': 'https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype'},\n","  {'Answer': \">>> # Example of a function that takes in a torch.device\\n>>> cuda1 = torch.device('cuda:1')\\n>>> torch.randn((2,3), device=cuda1)\\n\",\n","   'Id': 58,\n","   'Question': 'How to use NoteThetorch.deviceargument in functions can generally be substituted with a string.\\nThis allows for fast prototyping of code., give an example?',\n","   'context': ' Thetorch.deviceargument in functions can generally be substituted with a string.\\nThis allows for fast prototyping of code.',\n","   'source': 'https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype'},\n","  {'Answer': \">>> # You can substitute the torch.device with a string\\n>>> torch.randn((2,3), device='cuda:1')\\n\",\n","   'Id': 59,\n","   'Question': 'How  Thetorch.deviceargument in functions can generally be substituted with a string.\\nThis allows for fast prototyping of code., give an example?',\n","   'context': ' Thetorch.deviceargument in functions can generally be substituted with a string.\\nThis allows for fast prototyping of code.',\n","   'source': 'https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype'},\n","  {'Answer': \">>> torch.device(1)\\ndevice(type='cuda', index=1)\\n\",\n","   'Id': 60,\n","   'Question': 'How to use NoteFor legacy reasons, a device can be constructed via a single device ordinal, which is treated\\nas a cuda device.  This matchesTensor.get_device(), which returns an ordinal for cuda\\ntensors and is not supported for cpu tensors., give an example?',\n","   'context': ' For legacy reasons, a device can be constructed via a single device ordinal, which is treated\\nas a cuda device.  This matchesTensor.get_device(), which returns an ordinal for cuda\\ntensors and is not supported for cpu tensors.',\n","   'source': 'https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype'},\n","  {'Answer': \">>> torch.randn((2,3), device=torch.device('cuda:1'))\\n>>> torch.randn((2,3), device='cuda:1')\\n>>> torch.randn((2,3), device=1)  # legacy\\n\",\n","   'Id': 61,\n","   'Question': 'How to use NoteMethods which take a device will generally accept a (properly formatted) string\\nor (legacy) integer device ordinal, i.e. the following are all equivalent:, give an example?',\n","   'context': ' Methods which take a device will generally accept a (properly formatted) string\\nor (legacy) integer device ordinal, i.e. the following are all equivalent:',\n","   'source': 'https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype'},\n","  {'Answer': '>>> x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\\n>>> x.stride()\\n(5, 1)\\n\\n>>> x.t().stride()\\n(1, 5)\\n',\n","   'Id': 62,\n","   'Question': 'How to use torch.stridedrepresents dense Tensors and is the memory layout that\\nis most commonly used. Each strided tensor has an associatedtorch.Storage, which holds its data. These tensors provide\\nmulti-dimensional,stridedview of a storage. Strides are a list of integers: the k-th stride\\nrepresents the jump in the memory necessary to go from one element to the\\nnext one in the k-th dimension of the Tensor. This concept makes it possible\\nto perform many tensor operations efficiently., give an example?',\n","   'context': ' torch.stridedrepresents dense Tensors and is the memory layout that\\nis most commonly used. Each strided tensor has an associatedtorch.Storage, which holds its data. These tensors provide\\nmulti-dimensional,stridedview of a storage. Strides are a list of integers: the k-th stride\\nrepresents the jump in the memory necessary to go from one element to the\\nnext one in the k-th dimension of the Tensor. This concept makes it possible\\nto perform many tensor operations efficiently.',\n","   'source': 'https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype'},\n","  {'Answer': '>>> a = torch.randn(1, 3)\\n>>> a\\ntensor([[ 1.5219, -1.5212,  0.2202]])\\n>>> torch.median(a)\\ntensor(0.2202)\\n',\n","   'Id': 63,\n","   'Question': 'How to use torch.median, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.median.html#torch.median'},\n","  {'Answer': '>>> a = torch.randn(4, 5)\\n>>> a\\ntensor([[ 0.2505, -0.3982, -0.9948,  0.3518, -1.3131],\\n        [ 0.3180, -0.6993,  1.0436,  0.0438,  0.2270],\\n        [-0.2751,  0.7303,  0.2192,  0.3321,  0.2488],\\n        [ 1.0778, -1.9510,  0.7048,  0.4742, -0.7125]])\\n>>> torch.median(a, 1)\\ntorch.return_types.median(values=tensor([-0.3982,  0.2270,  0.2488,  0.4742]), indices=tensor([1, 4, 4, 3]))\\n',\n","   'Id': 64,\n","   'Question': 'How  IfkeepdimisTrue, the output tensors are of the same size\\nasinputexcept in the dimensiondimwhere they are of size 1.\\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\\nthe outputs tensor having 1 fewer dimension thaninput., give an example?',\n","   'context': ' IfkeepdimisTrue, the output tensors are of the same size\\nasinputexcept in the dimensiondimwhere they are of size 1.\\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\\nthe outputs tensor having 1 fewer dimension thaninput.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.median.html#torch.median'},\n","  {'Answer': 'import torch\\nimport torchvision\\n\\ndummy_input = torch.randn(10, 3, 224, 224, device=\\'cuda\\')\\nmodel = torchvision.models.alexnet(pretrained=True).cuda()\\n\\n# Providing input and output names sets the display names for values\\n# within the model\\'s graph. Setting these does not change the semantics\\n# of the graph; it is only for readability.\\n#\\n# The inputs to the network consist of the flat list of inputs (i.e.\\n# the values you would pass to the forward() method) followed by the\\n# flat list of parameters. You can partially specify names, i.e. provide\\n# a list here shorter than the number of inputs to the model, and we will\\n# only set that subset of names, starting from the beginning.\\ninput_names = [ \"actual_input_1\" ] + [ \"learned_%d\" % i for i in range(16) ]\\noutput_names = [ \"output1\" ]\\n\\ntorch.onnx.export(model, dummy_input, \"alexnet.onnx\", verbose=True, input_names=input_names, output_names=output_names)\\n',\n","   'Id': 65,\n","   'Question': 'How to use Here is a simple script which exports a pretrained AlexNet as defined in\\ntorchvision into ONNX.  It runs a single round of inference and then\\nsaves the resulting traced model toalexnet.onnx:, give an example?',\n","   'context': ' Here is a simple script which exports a pretrained AlexNet as defined in\\ntorchvision into ONNX.  It runs a single round of inference and then\\nsaves the resulting traced model toalexnet.onnx:',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': '# These are the inputs and parameters to the network, which have taken on\\n# the names we specified earlier.\\ngraph(%actual_input_1 : Float(10, 3, 224, 224)\\n      %learned_0 : Float(64, 3, 11, 11)\\n      %learned_1 : Float(64)\\n      %learned_2 : Float(192, 64, 5, 5)\\n      %learned_3 : Float(192)\\n      # ---- omitted for brevity ----\\n      %learned_14 : Float(1000, 4096)\\n      %learned_15 : Float(1000)) {\\n  # Every statement consists of some output tensors (and their types),\\n  # the operator to be run (with its attributes, e.g., kernels, strides,\\n  # etc.), its input tensors (%actual_input_1, %learned_0, %learned_1)\\n  %17 : Float(10, 64, 55, 55) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[11, 11], pads=[2, 2, 2, 2], strides=[4, 4]](%actual_input_1, %learned_0, %learned_1), scope: AlexNet/Sequential[features]/Conv2d[0]\\n  %18 : Float(10, 64, 55, 55) = onnx::Relu(%17), scope: AlexNet/Sequential[features]/ReLU[1]\\n  %19 : Float(10, 64, 27, 27) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%18), scope: AlexNet/Sequential[features]/MaxPool2d[2]\\n  # ---- omitted for brevity ----\\n  %29 : Float(10, 256, 6, 6) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%28), scope: AlexNet/Sequential[features]/MaxPool2d[12]\\n  # Dynamic means that the shape is not known. This may be because of a\\n  # limitation of our implementation (which we would like to fix in a\\n  # future release) or shapes which are truly dynamic.\\n  %30 : Dynamic = onnx::Shape(%29), scope: AlexNet\\n  %31 : Dynamic = onnx::Slice[axes=[0], ends=[1], starts=[0]](%30), scope: AlexNet\\n  %32 : Long() = onnx::Squeeze[axes=[0]](%31), scope: AlexNet\\n  %33 : Long() = onnx::Constant[value={9216}](), scope: AlexNet\\n  # ---- omitted for brevity ----\\n  %output1 : Float(10, 1000) = onnx::Gemm[alpha=1, beta=1, broadcast=1, transB=1](%45, %learned_14, %learned_15), scope: AlexNet/Sequential[classifier]/Linear[6]\\n  return (%output1);\\n}\\n',\n","   'Id': 66,\n","   'Question': 'How to use Here is a simple script which exports a pretrained AlexNet as defined in\\ntorchvision into ONNX.  It runs a single round of inference and then\\nsaves the resulting traced model toalexnet.onnx:The resultingalexnet.onnxis a binary protobuf file which contains both\\nthe network structure and parameters of the model you exported\\n(in this case, AlexNet).  The keyword argumentverbose=Truecauses the\\nexporter to print out a human-readable representation of the network:, give an example?',\n","   'context': ' The resultingalexnet.onnxis a binary protobuf file which contains both\\nthe network structure and parameters of the model you exported\\n(in this case, AlexNet).  The keyword argumentverbose=Truecauses the\\nexporter to print out a human-readable representation of the network:',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': 'conda install -c conda-forge onnx\\n',\n","   'Id': 67,\n","   'Question': 'How to use The resultingalexnet.onnxis a binary protobuf file which contains both\\nthe network structure and parameters of the model you exported\\n(in this case, AlexNet).  The keyword argumentverbose=Truecauses the\\nexporter to print out a human-readable representation of the network:You can also verify the protobuf using theONNXlibrary.\\nYou can installONNXwith conda:, give an example?',\n","   'context': ' The resultingalexnet.onnxis a binary protobuf file which contains both\\nthe network structure and parameters of the model you exported\\n(in this case, AlexNet).  The keyword argumentverbose=Truecauses the\\nexporter to print out a human-readable representation of the network:You can also verify the protobuf using theONNXlibrary.\\nYou can installONNXwith conda:',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': 'import onnx\\n\\n# Load the ONNX model\\nmodel = onnx.load(\"alexnet.onnx\")\\n\\n# Check that the IR is well formed\\nonnx.checker.check_model(model)\\n\\n# Print a human readable representation of the graph\\nonnx.helper.printable_graph(model.graph)\\n',\n","   'Id': 68,\n","   'Question': 'How to use You can also verify the protobuf using theONNXlibrary.\\nYou can installONNXwith conda:Then, you can run:, give an example?',\n","   'context': ' You can also verify the protobuf using theONNXlibrary.\\nYou can installONNXwith conda:Then, you can run:',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': '# ...continuing from above\\nimport caffe2.python.onnx.backend as backend\\nimport numpy as np\\n\\nrep = backend.prepare(model, device=\"CUDA:0\") # or \"CPU\"\\n# For the Caffe2 backend:\\n#     rep.predict_net is the Caffe2 protobuf for the network\\n#     rep.workspace is the Caffe2 workspace for the network\\n#       (see the class caffe2.python.onnx.backend.Workspace)\\noutputs = rep.run(np.random.randn(10, 3, 224, 224).astype(np.float32))\\n# To run networks with more than one input, pass a tuple\\n# rather than a single numpy ndarray.\\nprint(outputs[0])\\n',\n","   'Id': 69,\n","   'Question': 'How to use To run the exported script withcaffe2, you will need to installcaffe2: If you don’t have one already, Pleasefollow the install instructions.Once these are installed, you can use the backend for Caffe2:, give an example?',\n","   'context': ' To run the exported script withcaffe2, you will need to installcaffe2: If you don’t have one already, Pleasefollow the install instructions.Once these are installed, you can use the backend for Caffe2:',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': \"# ...continuing from above\\nimport onnxruntime as ort\\n\\nort_session = ort.InferenceSession('alexnet.onnx')\\n\\noutputs = ort_session.run(None, {'actual_input_1': np.random.randn(10, 3, 224, 224).astype(np.float32)})\\n\\nprint(outputs[0])\\n\",\n","   'Id': 70,\n","   'Question': 'How to use You can also run the exported model withONNX Runtime,\\nyou will need to installONNX Runtime: pleasefollow these instructions.Once these are installed, you can use the backend for ONNX Runtime:, give an example?',\n","   'context': ' You can also run the exported model withONNX Runtime,\\nyou will need to installONNX Runtime: pleasefollow these instructions.Once these are installed, you can use the backend for ONNX Runtime:',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': \"import torch\\n\\n# Trace-based only\\n\\nclass LoopModel(torch.nn.Module):\\n    def forward(self, x, y):\\n        for i in range(y):\\n            x = x + i\\n        return x\\n\\nmodel = LoopModel()\\ndummy_input = torch.ones(2, 3, dtype=torch.long)\\nloop_count = torch.tensor(5, dtype=torch.long)\\n\\ntorch.onnx.export(model, (dummy_input, loop_count), 'loop.onnx', verbose=True)\\n\",\n","   'Id': 71,\n","   'Question': 'How to use The ONNX exporter can be bothtrace-basedandscript-basedexporter.We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\\nof a part of a model.  Checkout this example:, give an example?',\n","   'context': ' We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\\nof a part of a model.  Checkout this example:',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': 'graph(%0 : Long(2, 3),\\n      %1 : Long()):\\n  %2 : Tensor = onnx::Constant[value={1}]()\\n  %3 : Tensor = onnx::Add(%0, %2)\\n  %4 : Tensor = onnx::Constant[value={2}]()\\n  %5 : Tensor = onnx::Add(%3, %4)\\n  %6 : Tensor = onnx::Constant[value={3}]()\\n  %7 : Tensor = onnx::Add(%5, %6)\\n  %8 : Tensor = onnx::Constant[value={4}]()\\n  %9 : Tensor = onnx::Add(%7, %8)\\n  return (%9)\\n',\n","   'Id': 72,\n","   'Question': 'How to use We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\\nof a part of a model.  Checkout this example:Withtrace-basedexporter, we get the result ONNX graph which unrolls the for loop:, give an example?',\n","   'context': ' We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\\nof a part of a model.  Checkout this example:Withtrace-basedexporter, we get the result ONNX graph which unrolls the for loop:',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': \"# Mixing tracing and scripting\\n\\n@torch.jit.script\\ndef loop(x, y):\\n    for i in range(int(y)):\\n        x = x + i\\n    return x\\n\\nclass LoopModel2(torch.nn.Module):\\n    def forward(self, x, y):\\n        return loop(x, y)\\n\\nmodel = LoopModel2()\\ndummy_input = torch.ones(2, 3, dtype=torch.long)\\nloop_count = torch.tensor(5, dtype=torch.long)\\ntorch.onnx.export(model, (dummy_input, loop_count), 'loop.onnx', verbose=True,\\n                  input_names=['input_data', 'loop_range'])\\n\",\n","   'Id': 73,\n","   'Question': 'How to use Withtrace-basedexporter, we get the result ONNX graph which unrolls the for loop:To utilizescript-basedexporter for capturing the dynamic loop,\\nwe can write the loop in script, and call it from the regular nn.Module:, give an example?',\n","   'context': ' Withtrace-basedexporter, we get the result ONNX graph which unrolls the for loop:To utilizescript-basedexporter for capturing the dynamic loop,\\nwe can write the loop in script, and call it from the regular nn.Module:',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': 'graph(%input_data : Long(2, 3),\\n      %loop_range : Long()):\\n  %2 : Long() = onnx::Constant[value={1}](), scope: LoopModel2/loop\\n  %3 : Tensor = onnx::Cast[to=9](%2)\\n  %4 : Long(2, 3) = onnx::Loop(%loop_range, %3, %input_data), scope: LoopModel2/loop\\n    block0(%i.1 : Long(), %cond : bool, %x.6 : Long(2, 3)):\\n      %8 : Long(2, 3) = onnx::Add(%x.6, %i.1), scope: LoopModel2/loop\\n      %9 : Tensor = onnx::Cast[to=9](%2)\\n      -> (%9, %8)\\n  return (%4)\\n',\n","   'Id': 74,\n","   'Question': 'How to use To utilizescript-basedexporter for capturing the dynamic loop,\\nwe can write the loop in script, and call it from the regular nn.Module:Now the exported ONNX graph becomes:, give an example?',\n","   'context': ' To utilizescript-basedexporter for capturing the dynamic loop,\\nwe can write the loop in script, and call it from the regular nn.Module:Now the exported ONNX graph becomes:',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': \"import caffe2.python.onnx.backend as backend\\nimport numpy as np\\nimport onnx\\nmodel = onnx.load('loop.onnx')\\n\\nrep = backend.prepare(model)\\noutputs = rep.run((dummy_input.numpy(), np.array(9).astype(np.int64)))\\nprint(outputs[0])\\n#[[37 37 37]\\n# [37 37 37]]\\n\\n\\nimport onnxruntime as ort\\nort_sess = ort.InferenceSession('loop.onnx')\\noutputs = ort_sess.run(None, {'input_data': dummy_input.numpy(),\\n                              'loop_range': np.array(9).astype(np.int64)})\\nprint(outputs)\\n#[array([[37, 37, 37],\\n#       [37, 37, 37]], dtype=int64)]\\n\",\n","   'Id': 75,\n","   'Question': 'How to use Now the exported ONNX graph becomes:The dynamic control flow is captured correctly. We can verify in backends with different loop range., give an example?',\n","   'context': ' Now the exported ONNX graph becomes:The dynamic control flow is captured correctly. We can verify in backends with different loop range.',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': \"class LoopModel(torch.nn.Module):\\n    def forward(self, x, y):\\n        res = []\\n        arr = x.split(2, 0)\\n        for i in range(int(y)):\\n            res += [arr[i].sum(0, False)]\\n        return torch.stack(res)\\n\\nmodel = torch.jit.script(LoopModel())\\ninputs = (torch.randn(16), torch.tensor(8))\\n\\nout = model(*inputs)\\ntorch.onnx.export(model, inputs, 'loop_and_list.onnx', opset_version=11, example_outputs=out)\\n\",\n","   'Id': 76,\n","   'Question': 'How to use The dynamic control flow is captured correctly. We can verify in backends with different loop range.To avoid exporting a variable scalar tensor as a fixed value constant as part of the ONNX model, please\\navoid use oftorch.Tensor.item(). Torch supports implicit cast of single-element tensors to numbers.\\nE.g.:, give an example?',\n","   'context': ' The dynamic control flow is captured correctly. We can verify in backends with different loop range.To avoid exporting a variable scalar tensor as a fixed value constant as part of the ONNX model, please\\navoid use oftorch.Tensor.item(). Torch supports implicit cast of single-element tensors to numbers.\\nE.g.:',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': 'import torch\\n\\nclass Module(torch.nn.Module):\\n    def forward(self, x, tup):\\n        # type: (int, Tuple[Tensor, Tensor]) -> Tensor\\n        t0, t1 = tup\\n        return t0 + t1 + x\\n',\n","   'Id': 77,\n","   'Question': 'How to use TorchScript only supports a subset of Python types. You can find more details about type annotationhere.Due to optimization purposes, TorchScript only supports variables with single static types for script functions.\\nBy default, each variable is assumed to be Tensor. If an argument to a ScriptModule function is not Tensor,\\nits type should be specified using MyPy-style annotations., give an example?',\n","   'context': ' Due to optimization purposes, TorchScript only supports variables with single static types for script functions.\\nBy default, each variable is assumed to be Tensor. If an argument to a ScriptModule function is not Tensor,\\nits type should be specified using MyPy-style annotations.',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': 'RuntimeError:\\nTensor (inferred) cannot be used as a tuple:\\n  File <filename>\\n        def forward(self, x, tup):\\n            t0, t1 = tup\\n                     ~~~ <--- HERE\\n            return t0 + t1 + x\\n',\n","   'Id': 78,\n","   'Question': 'How to use Due to optimization purposes, TorchScript only supports variables with single static types for script functions.\\nBy default, each variable is assumed to be Tensor. If an argument to a ScriptModule function is not Tensor,\\nits type should be specified using MyPy-style annotations.If the type annotation is not specified, TorchScript compiler fails with the runtime error below., give an example?',\n","   'context': ' Due to optimization purposes, TorchScript only supports variables with single static types for script functions.\\nBy default, each variable is assumed to be Tensor. If an argument to a ScriptModule function is not Tensor,\\nits type should be specified using MyPy-style annotations.If the type annotation is not specified, TorchScript compiler fails with the runtime error below.',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': 'np.concatenate((x, y, z), axis=1)\\n',\n","   'Id': 79,\n","   'Question': 'How to use PyTorch models can be written using numpy manipulations, but this is not proper when we convert to the ONNX model.\\nFor the trace-based exporter, tracing treats the numpy values as the constant node,\\ntherefore it calculates the wrong result if we change the input.\\nSo the PyTorch model need implement using torch operators.\\nFor example, do not use numpy operators on numpy tensors:, give an example?',\n","   'context': ' PyTorch models can be written using numpy manipulations, but this is not proper when we convert to the ONNX model.\\nFor the trace-based exporter, tracing treats the numpy values as the constant node,\\ntherefore it calculates the wrong result if we change the input.\\nSo the PyTorch model need implement using torch operators.\\nFor example, do not use numpy operators on numpy tensors:',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': 'y = x.astype(np.int)\\n',\n","   'Id': 80,\n","   'Question': 'How to use PyTorch models can be written using numpy manipulations, but this is not proper when we convert to the ONNX model.\\nFor the trace-based exporter, tracing treats the numpy values as the constant node,\\ntherefore it calculates the wrong result if we change the input.\\nSo the PyTorch model need implement using torch operators.\\nFor example, do not use numpy operators on numpy tensors:do not convert to numpy types:, give an example?',\n","   'context': ' do not convert to numpy types:',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': 'class MyModule(nn.Module):\\n    def __init__(self):\\n        self.dropout = nn.Dropout(0.5)\\n\\n    def forward(self, x):\\n        x = self.dropout(x)\\n',\n","   'Id': 81,\n","   'Question': 'How to use do not convert to numpy types:Always use torch tensors and torch operators: torch.concat, etc.\\nIn addition, Dropout layer need defined in init function so that inferencing can handle it properly, i.e.,, give an example?',\n","   'context': ' do not convert to numpy types:Always use torch tensors and torch operators: torch.concat, etc.\\nIn addition, Dropout layer need defined in init function so that inferencing can handle it properly, i.e.,',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': 'class Model(torch.nn.Module):\\n  def forward(self, x, y=None, z=None):\\n    if y is not None:\\n      return x + y\\n    if z is not None:\\n      return x + z\\n    return x\\nm = Model()\\nx = torch.randn(2, 3)\\nz = torch.randn(2, 3)\\n',\n","   'Id': 82,\n","   'Question': 'How to use There are two ways to handle models which consist of named parameters or keyword arguments as inputs:For example, in the model:, give an example?',\n","   'context': ' For example, in the model:',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': 'torch.onnx.export(model, (x, None, z), ‘test.onnx’)\\n',\n","   'Id': 83,\n","   'Question': 'How to use Not using a dictionary for the keyword arguments and passing all the inputs in the same order\\nas required by the model, give an example?',\n","   'context': ' Not using a dictionary for the keyword arguments and passing all the inputs in the same order\\nas required by the model',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': \"torch.onnx.export(model, (x, {'y': None, 'z': z}), ‘test.onnx’)\\n\",\n","   'Id': 84,\n","   'Question': 'How to use Using a dictionary to represent the keyword arguments. This dictionary is always passed in\\naddition to the non-keyword arguments and is always the last argument in the args tuple., give an example?',\n","   'context': ' Using a dictionary to represent the keyword arguments. This dictionary is always passed in\\naddition to the non-keyword arguments and is always the last argument in the args tuple.',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': 'torch.onnx.export(model, (x, {}), ‘test.onnx’)\\nor\\ntorch.onnx.export(model, (x, ), ‘test.onnx’)\\n',\n","   'Id': 85,\n","   'Question': 'How to use There are two ways of exporting the model:For cases in which there are no keyword arguments, models can be exported with either an\\nempty or no dictionary. For example,, give an example?',\n","   'context': ' There are two ways of exporting the model:For cases in which there are no keyword arguments, models can be exported with either an\\nempty or no dictionary. For example,',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': 'class Model(torch.nn.Module):\\n  def forward(self, k, x):\\n    ...\\n    return x\\nm = Model()\\nk =\\u202ftorch.randn(2, 3)\\nx = {torch.tensor(1.):\\u202ftorch.randn(2, 3)}\\n',\n","   'Id': 86,\n","   'Question': 'How to use For cases in which there are no keyword arguments, models can be exported with either an\\nempty or no dictionary. For example,An exception to this rule are cases in which the last input is also of a dictionary type.\\nIn these cases it is mandatory to have an empty dictionary as the last argument in the\\nargs tuple. For example,, give an example?',\n","   'context': ' For cases in which there are no keyword arguments, models can be exported with either an\\nempty or no dictionary. For example,An exception to this rule are cases in which the last input is also of a dictionary type.\\nIn these cases it is mandatory to have an empty dictionary as the last argument in the\\nargs tuple. For example,',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': 'torch.onnx.export(model, (k, x, {}), ‘test.onnx’)\\n',\n","   'Id': 87,\n","   'Question': 'How to use An exception to this rule are cases in which the last input is also of a dictionary type.\\nIn these cases it is mandatory to have an empty dictionary as the last argument in the\\nargs tuple. For example,Without the presence of the empty dictionary, the export call assumes that the\\n‘x’ input is intended to represent the optional dictionary consisting of named arguments.\\nIn order to prevent this from being an issue a constraint is placed to provide an empty\\ndictionary as the last input in the tuple args in such cases.\\nThe new call would look like this., give an example?',\n","   'context': ' An exception to this rule are cases in which the last input is also of a dictionary type.\\nIn these cases it is mandatory to have an empty dictionary as the last argument in the\\nargs tuple. For example,Without the presence of the empty dictionary, the export call assumes that the\\n‘x’ input is intended to represent the optional dictionary consisting of named arguments.\\nIn order to prevent this from being an issue a constraint is placed to provide an empty\\ndictionary as the last input in the tuple args in such cases.\\nThe new call would look like this.',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': \"data = torch.randn(3, 4)\\nindex = torch.tensor([1, 2])\\n\\n# RHS indexing is supported in ONNX opset >= 11.\\nclass RHSIndexing(torch.nn.Module):\\n    def forward(self, data, index):\\n        return data[index]\\n\\nout = RHSIndexing()(data, index)\\n\\ntorch.onnx.export(RHSIndexing(), (data, index), 'indexing.onnx', opset_version=9)\\n\\n# onnxruntime\\nimport onnxruntime\\nsess = onnxruntime.InferenceSession('indexing.onnx')\\nout_ort = sess.run(None, {\\n    sess.get_inputs()[0].name: data.numpy(),\\n    sess.get_inputs()[1].name: index.numpy(),\\n})\\n\\nassert torch.all(torch.eq(out, torch.tensor(out_ort)))\\n\",\n","   'Id': 88,\n","   'Question': 'How to use This type of indexing occurs on the RHS. Export is supported for ONNX opset version >= 9. E.g.:, give an example?',\n","   'context': ' This type of indexing occurs on the RHS. Export is supported for ONNX opset version >= 9. E.g.:',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': '# Scalar indices\\ndata[0, 1]\\n\\n# Slice indices\\ndata[:3]\\n\\n# Tensor indices\\ndata[torch.tensor([[1, 2], [2, 3]])]\\ndata[torch.tensor([2, 3]), torch.tensor([1, 2])]\\ndata[torch.tensor([[1, 2], [2, 3]]), torch.tensor([2, 3])]\\ndata[torch.tensor([2, 3]), :, torch.tensor([1, 2])]\\n\\n# Ellipsis followed by tensor indexing\\n# Not supported in scripting\\n# i.e. torch.jit.script(model) will fail if model contains this pattern.\\n# Export is supported under tracing\\n# i.e. torch.onnx.export(model)\\ndata[..., torch.tensor([2, 1])]\\n\\n# The combination of above\\ndata[2, ..., torch.tensor([2, 1, 3]), 2:4, torch.tensor([[1], [2]])]\\n\\n# Boolean mask (supported for ONNX opset version >= 11)\\ndata[data != 1]\\n',\n","   'Id': 89,\n","   'Question': 'How to use This type of indexing occurs on the RHS. Export is supported for ONNX opset version >= 9. E.g.:Below is the list of supported patterns for RHS indexing., give an example?',\n","   'context': ' Below is the list of supported patterns for RHS indexing.',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': '# Tensor indices that includes negative values.\\ndata[torch.tensor([[1, 2], [2, -3]]), torch.tensor([-2, 3])]\\n',\n","   'Id': 90,\n","   'Question': 'How to use Below is the list of supported patterns for RHS indexing.And below is the list of unsupported patterns for RHS indexing., give an example?',\n","   'context': ' Below is the list of supported patterns for RHS indexing.And below is the list of unsupported patterns for RHS indexing.',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': \"data = torch.zeros(3, 4)\\nnew_data = torch.arange(4).to(torch.float32)\\n\\n# LHS indexing is supported in ONNX opset >= 11.\\nclass LHSIndexing(torch.nn.Module):\\n    def forward(self, data, new_data):\\n        data[1] = new_data\\n        return data\\n\\nout = LHSIndexing()(data, new_data)\\n\\ndata = torch.zeros(3, 4)\\nnew_data = torch.arange(4).to(torch.float32)\\ntorch.onnx.export(LHSIndexing(), (data, new_data), 'inplace_assign.onnx', opset_version=11)\\n\\n# onnxruntime\\nimport onnxruntime\\nsess = onnxruntime.InferenceSession('inplace_assign.onnx')\\nout_ort = sess.run(None, {\\n    sess.get_inputs()[0].name: torch.zeros(3, 4).numpy(),\\n    sess.get_inputs()[1].name: new_data.numpy(),\\n})\\n\\nassert torch.all(torch.eq(out, torch.tensor(out_ort)))\\n\",\n","   'Id': 91,\n","   'Question': 'How to use In code, this type of indexing occurs on the LHS.\\nExport is supported for ONNX opset version >= 11. E.g.:, give an example?',\n","   'context': ' In code, this type of indexing occurs on the LHS.\\nExport is supported for ONNX opset version >= 11. E.g.:',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': '# Scalar indices\\ndata[0, 1] = new_data\\n\\n# Slice indices\\ndata[:3] = new_data\\n\\n# Tensor indices\\n# If more than one tensor are used as indices, only consecutive 1-d tensor indices are supported.\\ndata[torch.tensor([[1, 2], [2, 3]])] = new_data\\ndata[torch.tensor([2, 3]), torch.tensor([1, 2])] = new_data\\n\\n# Ellipsis followed by tensor indexing\\n# Not supported to export in script modules\\n# i.e. torch.onnx.export(torch.jit.script(model)) will fail if model contains this pattern.\\n# Export is supported under tracing\\n# i.e. torch.onnx.export(model)\\ndata[..., torch.tensor([2, 1])] = new_data\\n\\n# The combination of above\\ndata[2, ..., torch.tensor([2, 1, 3]), 2:4] += update\\n\\n# Boolean mask\\ndata[data != 1] = new_data\\n',\n","   'Id': 92,\n","   'Question': 'How to use In code, this type of indexing occurs on the LHS.\\nExport is supported for ONNX opset version >= 11. E.g.:Below is the list of supported patterns for LHS indexing., give an example?',\n","   'context': ' Below is the list of supported patterns for LHS indexing.',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': '# Multiple tensor indices if any has rank >= 2\\ndata[torch.tensor([[1, 2], [2, 3]]), torch.tensor([2, 3])] = new_data\\n\\n# Multiple tensor indices that are not consecutive\\ndata[torch.tensor([2, 3]), :, torch.tensor([1, 2])] = new_data\\n\\n# Tensor indices that includes negative values.\\ndata[torch.tensor([1, -2]), torch.tensor([-2, 3])] = new_data\\n',\n","   'Id': 93,\n","   'Question': 'How to use Below is the list of supported patterns for LHS indexing.And below is the list of unsupported patterns for LHS indexing., give an example?',\n","   'context': ' Below is the list of supported patterns for LHS indexing.And below is the list of unsupported patterns for LHS indexing.',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': 'def operator/symbolic(g, *inputs):\\n  \"\"\"\\n  Modifies Graph (e.g., using \"op\"), adding the ONNX operations representing\\n  this PyTorch function, and returning a Value or tuple of Values specifying the\\n  ONNX outputs whose values correspond to the original PyTorch return values\\n  of the autograd Function (or None if an output is not supported by ONNX).\\n\\n  Args:\\n    g (Graph): graph to write the ONNX representation into\\n    inputs (Value...): list of values representing the variables which contain\\n        the inputs for this function\\n  \"\"\"\\n\\nclass Value(object):\\n  \"\"\"Represents an intermediate tensor value computed in ONNX.\"\"\"\\n  def type(self):\\n    \"\"\"Returns the Type of the value.\"\"\"\\n\\nclass Type(object):\\n  def sizes(self):\\n    \"\"\"Returns a tuple of ints representing the shape of a tensor this describes.\"\"\"\\n\\nclass Graph(object):\\n  def op(self, opname, *inputs, **attrs):\\n    \"\"\"\\n    Create an ONNX operator \\'opname\\', taking \\'args\\' as inputs\\n    and attributes \\'kwargs\\' and add it as a node to the current graph,\\n    returning the value representing the single output of this\\n    operator (see the `outputs` keyword argument for multi-return\\n    nodes).\\n\\n    The set of operators and the inputs/attributes they take\\n    is documented at https://github.com/onnx/onnx/blob/master/docs/Operators.md\\n\\n    Args:\\n        opname (string): The ONNX operator name, e.g., `Abs` or `Add`.\\n        args (Value...): The inputs to the operator; usually provided\\n            as arguments to the `symbolic` definition.\\n        kwargs: The attributes of the ONNX operator, with keys named\\n            according to the following convention: `alpha_f` indicates\\n            the `alpha` attribute with type `f`.  The valid type specifiers are\\n            `f` (float), `i` (int), `s` (string) or `t` (Tensor).  An attribute\\n            specified with type float accepts either a single float, or a\\n            list of floats (e.g., you would say `dims_i` for a `dims` attribute\\n            that takes a list of integers).\\n        outputs (int, optional):  The number of outputs this operator returns;\\n            by default an operator is assumed to return a single output.\\n            If `outputs` is greater than one, this functions returns a tuple\\n            of output `Value`, representing each output of the ONNX operator\\n            in positional.\\n    \"\"\"\\n',\n","   'Id': 94,\n","   'Question': 'How to use If the operator is a non-ATen operator, the symbolic function has to be\\nadded in the corresponding PyTorch Function class. Please read the following\\ninstructions:Symbolic functions should be implemented in Python. All of these functions interact\\nwith Python methods which are implemented via C++-Python bindings,\\nbut intuitively the interface they provide looks like this:, give an example?',\n","   'context': ' Symbolic functions should be implemented in Python. All of these functions interact\\nwith Python methods which are implemented via C++-Python bindings,\\nbut intuitively the interface they provide looks like this:',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': \"UserWarning: ONNX export failed on elu because torch.onnx.symbolic_opset9.elu does not exist\\nRuntimeError: ONNX export failed: Couldn't export operator elu\\n\",\n","   'Id': 95,\n","   'Question': 'How to use The ONNX graph C++ definition is intorch/csrc/jit/ir/ir.h.Here is an example of handling missing symbolic function foreluoperator.\\nWe try to export the model and see the error message as below:, give an example?',\n","   'context': ' The ONNX graph C++ definition is intorch/csrc/jit/ir/ir.h.Here is an example of handling missing symbolic function foreluoperator.\\nWe try to export the model and see the error message as below:',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': 'def elu(g, input, alpha, inplace=False):\\n    return g.op(\"Elu\", input, alpha_f=_scalar(alpha))\\n',\n","   'Id': 96,\n","   'Question': 'How to use Here is an example of handling missing symbolic function foreluoperator.\\nWe try to export the model and see the error message as below:The export fails because PyTorch does not support exportingeluoperator.\\nWe findvirtualTensorelu(constTensor&input,Scalaralpha,boolinplace)constoverride;inVariableType.h. This meanseluis an ATen operator.\\nWe check theONNX operator list,\\nand confirm thatEluis standardized in ONNX.\\nWe add the following lines tosymbolic_opset9.py:, give an example?',\n","   'context': ' Here is an example of handling missing symbolic function foreluoperator.\\nWe try to export the model and see the error message as below:The export fails because PyTorch does not support exportingeluoperator.\\nWe findvirtualTensorelu(constTensor&input,Scalaralpha,boolinplace)constoverride;inVariableType.h. This meanseluis an ATen operator.\\nWe check theONNX operator list,\\nand confirm thatEluis standardized in ONNX.\\nWe add the following lines tosymbolic_opset9.py:',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': '# Create custom symbolic function\\nfrom torch.onnx.symbolic_helper import parse_args\\n@parse_args(\\'v\\', \\'v\\', \\'f\\', \\'i\\')\\ndef symbolic_foo_forward(g, input1, input2, attr1, attr2):\\n    return g.op(\"Foo\", input1, input2, attr1_f=attr1, attr2_i=attr2)\\n\\n# Register custom symbolic function\\nfrom torch.onnx import register_custom_op_symbolic\\nregister_custom_op_symbolic(\\'custom_ops::foo_forward\\', symbolic_foo_forward, 9)\\n\\nclass FooModel(torch.nn.Module):\\n    def __init__(self, attr1, attr2):\\n        super(FooModule, self).__init__()\\n        self.attr1 = attr1\\n        self.attr2 = attr2\\n\\n    def forward(self, input1, input2):\\n        # Calling custom op\\n        return torch.ops.custom_ops.foo_forward(input1, input2, self.attr1, self.attr2)\\n\\nmodel = FooModel(attr1, attr2)\\ntorch.onnx.export(model, (dummy_input1, dummy_input2), \\'model.onnx\\', custom_opsets={\"custom_domain\": 2})\\n',\n","   'Id': 97,\n","   'Question': 'How to use Following this tutorialExtending TorchScript with Custom C++ Operators,\\nyou can create and register your own custom ops implementation in PyTorch. Here’s how to export such model to ONNX.:, give an example?',\n","   'context': ' Following this tutorialExtending TorchScript with Custom C++ Operators,\\nyou can create and register your own custom ops implementation in PyTorch. Here’s how to export such model to ONNX.:',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': 'Example torch ir graph:\\n\\n  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1])):\\n    %3 : Float(2, 3, 4, strides=[12, 4, 1]) = aten:exp(%0)\\n    %4 : Float(2, 3, 4, strides=[12, 4, 1]) = aten:div(%0, %3)\\n    return (%4)\\n\\nIs exported as:\\n\\n  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1])):\\n    %1 : Float(2, 3, 4, strides=[12, 4, 1]) = onnx:Exp(%0)\\n    %2 : Float(2, 3, 4, strides=[12, 4, 1]) = onnx:Div(%0, %1)\\n    return (%2)\\n',\n","   'Id': 98,\n","   'Question': 'How to use This mode is used to export all operators as regular ONNX operators. This is the defaultoperator_export_typemode., give an example?',\n","   'context': ' This mode is used to export all operators as regular ONNX operators. This is the defaultoperator_export_typemode.',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': 'Example torch ir graph:\\n\\n  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1])):\\n    %3 : Float(2, 3, 4, strides=[12, 4, 1]) = aten::exp(%0)\\n    %4 : Float(2, 3, 4, strides=[12, 4, 1]) = aten::div(%0, %3)\\n    return (%4)\\n\\nIs exported as:\\n\\n  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1])):\\n    %1 : Float(2, 3, 4, strides=[12, 4, 1]) = aten::ATen[operator=\"exp\"](%0)\\n    %2 : Float(2, 3, 4, strides=[12, 4, 1]) = aten::ATen[operator=\"div\"](%0, %1)\\n    return (%2)\\n',\n","   'Id': 99,\n","   'Question': 'How to use This mode is used to export all operators as ATen ops, and avoid conversion to ONNX., give an example?',\n","   'context': ' This mode is used to export all operators as ATen ops, and avoid conversion to ONNX.',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': 'Example torch ir graph:\\n\\n  graph(%0 : Float):\\n    %3 : int = prim::Constant[value=0]()\\n    %4 : Float = aten::triu(%0, %3) # unsupported op\\n    %5 : Float = aten::mul(%4, %0) # registered op\\n    return (%5)\\n\\nis exported as:\\n\\n  graph(%0 : Float):\\n    %1 : Long() = onnx::Constant[value={0}]()\\n    %2 : Float = aten::ATen[operator=\"triu\"](%0, %1) # unsupported op\\n    %3 : Float = onnx::Mul(%2, %0) # registered op\\n    return (%3)\\n',\n","   'Id': 100,\n","   'Question': 'How to use To fallback on unsupported ATen operators in ONNX. Supported operators are exported to ONNX regularly.\\nIn the following example, aten::triu is not supported in ONNX. Exporter falls back on this operator., give an example?',\n","   'context': ' To fallback on unsupported ATen operators in ONNX. Supported operators are exported to ONNX regularly.\\nIn the following example, aten::triu is not supported in ONNX. Exporter falls back on this operator.',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': 'Example torch ir graph:\\n\\n  graph(%x.1 : Float(1, strides=[1])):\\n    %1 : Tensor = aten::exp(%x.1)\\n    %2 : Tensor = aten::div(%x.1, %1)\\n    %y.1 : Tensor[] = prim::ListConstruct(%2)\\n    return (%y.1)\\n\\nis exported as:\\n\\n  graph(%x.1 : Float(1, strides=[1])):\\n    %1 : Tensor = aten::exp(%x.1)\\n    %2 : Tensor = aten::div(%x.1, %1)\\n    %y.1 : Tensor[] = prim::ListConstruct(%2)\\n    return (%y.1)\\n',\n","   'Id': 101,\n","   'Question': 'How to use To export a raw ir., give an example?',\n","   'context': ' To export a raw ir.',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': 'Example torch ir graph:\\n\\n  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1]),\\n        %1 : Float(2, 3, 4, strides=[12, 4, 1])):\\n    %6 : Float(2, 3, 4, strides=[12, 4, 1]) = foo_namespace::bar(%0, %1) # custom op\\n    %7 : Float(2, 3, 4, strides=[12, 4, 1]) = aten::div(%6, %0) # registered op\\n    return (%7))\\n\\nis exported as:\\n\\n  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1]),\\n        %1 : Float(2, 3, 4, strides=[12, 4, 1])):\\n    %2 : Float(2, 3, 4, strides=[12, 4, 1]) = foo_namespace::bar(%0, %1) # custom op\\n    %3 : Float(2, 3, 4, strides=[12, 4, 1]) = onnx::Div(%2, %0) # registered op\\n    return (%3\\n',\n","   'Id': 102,\n","   'Question': 'How to use This mode can be used to export any operator (ATen or non-ATen) that is not registered and supported in ONNX.\\nExported falls through and exports the operator as is, as custom op. Exporting custom operators\\nenables users to register and implement the operator as part of their runtime backend., give an example?',\n","   'context': ' This mode can be used to export any operator (ATen or non-ATen) that is not registered and supported in ONNX.\\nExported falls through and exports the operator as is, as custom op. Exporting custom operators\\nenables users to register and implement the operator as part of their runtime backend.',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': \"layer_count = 4\\n\\nmodel = nn.LSTM(10, 20, num_layers=layer_count, bidirectional=True)\\nmodel.eval()\\n\\nwith torch.no_grad():\\n    input = torch.randn(5, 3, 10)\\n    h0 = torch.randn(layer_count * 2, 3, 20)\\n    c0 = torch.randn(layer_count * 2, 3, 20)\\n    output, (hn, cn) = model(input, (h0, c0))\\n\\n    # default export\\n    torch.onnx.export(model, (input, (h0, c0)), 'lstm.onnx')\\n    onnx_model = onnx.load('lstm.onnx')\\n    # input shape [5, 3, 10]\\n    print(onnx_model.graph.input[0])\\n\\n    # export with `dynamic_axes`\\n    torch.onnx.export(model, (input, (h0, c0)), 'lstm.onnx',\\n                    input_names=['input', 'h0', 'c0'],\\n                    output_names=['output', 'hn', 'cn'],\\n                    dynamic_axes={'input': {0: 'sequence'}, 'output': {0: 'sequence'}})\\n    onnx_model = onnx.load('lstm.onnx')\\n    # input shape ['sequence', 3, 10]\\n    print(onnx_model.graph.input[0])\\n\",\n","   'Id': 103,\n","   'Question': 'How to use The tracer records the example inputs shape in the graph. In case the model should accept\\ninputs of dynamic shape, you can utilize the parameterdynamic_axesin export api., give an example?',\n","   'context': ' The tracer records the example inputs shape in the graph. In case the model should accept\\ninputs of dynamic shape, you can utilize the parameterdynamic_axesin export api.',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': \"class ImplicitCastType(torch.jit.ScriptModule):\\n    @torch.jit.script_method\\n    def forward(self, x):\\n        # Exporter knows x is float32, will export '2' as float32 as well.\\n        y = x + 2\\n        # Without type propagation, exporter doesn't know the datatype of y.\\n        # Thus '3' is exported as int64 by default.\\n        return y + 3\\n        # The following will export correctly.\\n        # return y + torch.tensor([3], dtype=torch.float32)\\n\\nx = torch.tensor([1.0], dtype=torch.float32)\\ntorch.onnx.export(ImplicitCastType(), x, 'models/implicit_cast.onnx',\\n                  example_outputs=ImplicitCastType()(x))\\n\",\n","   'Id': 104,\n","   'Question': 'How to use No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\\nwhere the datatypes are not recorded.  We are trying to improve the datatype\\npropagation in the exporter such that manual changes are not required in the future., give an example?',\n","   'context': ' No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\\nwhere the datatypes are not recorded.  We are trying to improve the datatype\\npropagation in the exporter such that manual changes are not required in the future.',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': \"class ListLoopModel(torch.nn.Module):\\n    def forward(self, x):\\n        res = []\\n        res1 = []\\n        arr = x.split(2, 0)\\n        res2 = torch.zeros(3, 4, dtype=torch.long)\\n        for i in range(len(arr)):\\n            res += [arr[i].sum(0, False)]\\n            res1 += [arr[-1 - i].sum(0, False)]\\n            res2 += 1\\n        return torch.stack(res), torch.stack(res1), res2\\n\\nmodel = torch.jit.script(ListLoopModel())\\ninputs = torch.randn(16)\\n\\nout = model(inputs)\\ntorch.onnx.export(model, (inputs, ), 'loop_and_list.onnx', opset_version=11, example_outputs=out)\\n\\n# onnxruntime\\nimport onnxruntime\\nsess = onnxruntime.InferenceSession('loop_and_list.onnx')\\nout_ort = sess.run(None, {\\n    sess.get_inputs()[0].name: inputs.numpy(),\\n})\\n\\nassert [torch.allclose(o, torch.tensor(o_ort)) for o, o_ort in zip(out, out_ort)]\\n\",\n","   'Id': 105,\n","   'Question': 'How to use Yes, this is supported now for ONNX opset version >= 11. ONNX introduced the concept of Sequence in opset 11.\\nSimilar to list, Sequence is a data type that contains arbitrary number of Tensors.\\nAssociated operators are also introduced in ONNX, such as SequenceInsert, SequenceAt, etc.\\nHowever, in-place list append within loops is not exportable to ONNX. To implement this, please use inplace\\nadd operator.\\nE.g.:, give an example?',\n","   'context': ' Yes, this is supported now for ONNX opset version >= 11. ONNX introduced the concept of Sequence in opset 11.\\nSimilar to list, Sequence is a data type that contains arbitrary number of Tensors.\\nAssociated operators are also introduced in ONNX, such as SequenceInsert, SequenceAt, etc.\\nHowever, in-place list append within loops is not exportable to ONNX. To implement this, please use inplace\\nadd operator.\\nE.g.:',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': \"model = torchvision.models.mobilenet_v2(pretrained=True)\\ninput = torch.randn(2, 3, 224, 224, requires_grad=True)\\ntorch.onnx.export(model, (input, ), './large_model.onnx', use_external_data_format=True)\\n\",\n","   'Id': 106,\n","   'Question': 'How to use use_external_data_formatargument in export API enables export of models in ONNX external\\ndata format. With this option enabled, the exporter stores some model parameters in external\\nbinary files, rather than the ONNX file itself. These external binary files are stored in the\\nsame location as the ONNX file. Argument ‘f’ must be a string specifying the location of the model., give an example?',\n","   'context': ' use_external_data_formatargument in export API enables export of models in ONNX external\\ndata format. With this option enabled, the exporter stores some model parameters in external\\nbinary files, rather than the ONNX file itself. These external binary files are stored in the\\nsame location as the ONNX file. Argument ‘f’ must be a string specifying the location of the model.',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': '\"args = (x, y, z)\"\\n',\n","   'Id': 107,\n","   'Question': 'How to use torch.onnx.export, give an example?',\n","   'context': ' ONLY A TUPLE OF ARGUMENTS or torch.Tensor:',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': '\"args = (x,\\n        {\\n        \\'y\\': input_y,\\n        \\'z\\': input_z\\n        })\"\\n',\n","   'Id': 108,\n","   'Question': 'How  A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS:, give an example?',\n","   'context': ' A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS:',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': 'graph(%0 : Float)::\\n  %3 : int = prim::Constant[value=0]()\\n  %4 : Float = aten::triu(%0, %3) # missing op\\n  %5 : Float = aten::mul(%4, %0) # registered op\\n  return (%5)\\n',\n","   'Id': 109,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': 'graph(%0 : Float)::\\n  %1 : Long() = onnx::Constant[value={0}]()\\n  %2 : Float = aten::ATen[operator=\"triu\"](%0, %1)  # missing op\\n  %3 : Float = onnx::Mul(%2, %0) # registered op\\n  return (%3)\\n',\n","   'Id': 110,\n","   'Question': 'How  is exported as:, give an example?',\n","   'context': ' is exported as:',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': 'graph(%x.1 : Long(1, strides=[1]))::\\n  %1 : None = prim::Constant()\\n  %2 : Tensor = aten::sum(%x.1, %1)\\n  %y.1 : Tensor[] = prim::ListConstruct(%2)\\n  return (%y.1)\\n',\n","   'Id': 111,\n","   'Question': 'How  is exported as:, give an example?',\n","   'context': ' is exported as:',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': 'graph(%x.1 : Long(1, strides=[1]))::\\n  %1 : Tensor = onnx::ReduceSum[keepdims=0](%x.1)\\n  %y.1 : Long() = prim::ListConstruct(%1)\\n  return (%y.1)\\n',\n","   'Id': 112,\n","   'Question': 'How  is exported as:, give an example?',\n","   'context': ' is exported as:',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': \"shape(input_1) = ('b', 3, 'w', 'h')\\nand shape(input_2) = ('b', 4)\\nand shape(output)  = ('b', 'd', 5)\\n\",\n","   'Id': 113,\n","   'Question': 'How to use Functions, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': \"``dynamic_axes = {'input_1':[0, 2, 3],\\n                  'input_2':[0],\\n                  'output':[0, 1]}``\\nwhere automatic names will be generated for exported dynamic axes\\n\",\n","   'Id': 114,\n","   'Question': 'How  ONLY INDICES:, give an example?',\n","   'context': ' ONLY INDICES:',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': \"``dynamic_axes = {'input_1':{0:'batch',\\n                             1:'width',\\n                             2:'height'},\\n                  'input_2':{0:'batch'},\\n                  'output':{0:'batch',\\n                            1:'detections'}}``\\nwhere provided names will be applied to exported dynamic axes\\n\",\n","   'Id': 115,\n","   'Question': 'How  INDICES WITH CORRESPONDING NAMES:, give an example?',\n","   'context': ' INDICES WITH CORRESPONDING NAMES:',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': \"``dynamic_axes = {'input_1':[0, 2, 3],\\n                  'input_2':{0:'batch'},\\n                  'output':[0,1]}``\\n\",\n","   'Id': 116,\n","   'Question': 'How  MIXED MODE OF (1) and (2):, give an example?',\n","   'context': ' MIXED MODE OF (1) and (2):',\n","   'source': 'https://pytorch.org/docs/stable/onnx.html'},\n","  {'Answer': '>>> a = torch.tril_indices(3, 3)\\n>>> a\\ntensor([[0, 1, 1, 2, 2, 2],\\n        [0, 0, 1, 0, 1, 2]])\\n\\n>>> a = torch.tril_indices(4, 3, -1)\\n>>> a\\ntensor([[1, 2, 2, 3, 3, 3],\\n        [0, 0, 1, 0, 1, 2]])\\n\\n>>> a = torch.tril_indices(4, 3, 1)\\n>>> a\\ntensor([[0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3],\\n        [0, 1, 0, 1, 2, 0, 1, 2, 0, 1, 2]])\\n',\n","   'Id': 117,\n","   'Question': 'How to use torch.tril_indices, give an example?',\n","   'context': ' The argumentoffsetcontrols which diagonal to consider. Ifoffset= 0, all elements on and below the main diagonal are\\nretained. A positive value includes just as many diagonals above the main\\ndiagonal, and similarly a negative value excludes just as many diagonals below\\nthe main diagonal. The main diagonal are the set of indices{(i,i)}\\\\lbrace (i, i) \\\\rbrace{(i,i)}fori∈[0,min\\u2061{d1,d2}−1]i \\\\in [0, \\\\min\\\\{d_{1}, d_{2}\\\\} - 1]i∈[0,min{d1\\u200b,d2\\u200b}−1]whered1,d2d_{1}, d_{2}d1\\u200b,d2\\u200bare the dimensions of the matrix.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.tril_indices.html#torch.tril_indices'},\n","  {'Answer': '>>> a = torch.tensor((1, 2))\\n>>> b = torch.tensor((0, 1))\\n>>> torch.sub(a, b, alpha=2)\\ntensor([1, 0])\\n',\n","   'Id': 118,\n","   'Question': 'How to use torch.sub, give an example?',\n","   'context': ' Supportsbroadcasting to a common shape,type promotion, and integer, float, and complex inputs.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.sub.html#torch.sub'},\n","  {'Answer': '# Creates model and optimizer in default precision\\nmodel = Net().cuda()\\noptimizer = optim.SGD(model.parameters(), ...)\\n\\nfor input, target in data:\\n    optimizer.zero_grad()\\n\\n    # Enables autocasting for the forward pass (model + loss)\\n    with autocast():\\n        output = model(input)\\n        loss = loss_fn(output, target)\\n\\n    # Exits the context manager before backward()\\n    loss.backward()\\n    optimizer.step()\\n',\n","   'Id': 119,\n","   'Question': 'How to use torch.cuda.amp.autocast, give an example?',\n","   'context': ' autocastshould wrap only the forward pass(es) of your network, including the loss\\ncomputation(s).  Backward passes under autocast are not recommended.\\nBackward ops run in the same type that autocast used for corresponding forward ops.',\n","   'source': 'https://pytorch.org/docs/stable/amp.html'},\n","  {'Answer': 'class AutocastModel(nn.Module):\\n    ...\\n    @autocast()\\n    def forward(self, input):\\n        ...\\n',\n","   'Id': 120,\n","   'Question': 'How to use autocastcan also be used as a decorator, e.g., on theforwardmethod of your model:, give an example?',\n","   'context': ' See theAutomatic Mixed Precision examplesfor usage (along with gradient scaling)\\nin more complex scenarios (e.g., gradient penalty, multiple models/losses, custom autograd functions).autocastcan also be used as a decorator, e.g., on theforwardmethod of your model:',\n","   'source': 'https://pytorch.org/docs/stable/amp.html'},\n","  {'Answer': '# Creates some tensors in default dtype (here assumed to be float32)\\na_float32 = torch.rand((8, 8), device=\"cuda\")\\nb_float32 = torch.rand((8, 8), device=\"cuda\")\\nc_float32 = torch.rand((8, 8), device=\"cuda\")\\nd_float32 = torch.rand((8, 8), device=\"cuda\")\\n\\nwith autocast():\\n    # torch.mm is on autocast\\'s list of ops that should run in float16.\\n    # Inputs are float32, but the op runs in float16 and produces float16 output.\\n    # No manual casts are required.\\n    e_float16 = torch.mm(a_float32, b_float32)\\n    # Also handles mixed input types\\n    f_float16 = torch.mm(d_float32, e_float16)\\n\\n# After exiting autocast, calls f_float16.float() to use with d_float32\\ng_float32 = torch.mm(d_float32, f_float16.float())\\n',\n","   'Id': 121,\n","   'Question': 'How to use Floating-point Tensors produced in an autocast-enabled region may befloat16.\\nAfter returning to an autocast-disabled region, using them with floating-point\\nTensors of different dtypes may cause type mismatch errors.  If so, cast the Tensor(s)\\nproduced in the autocast region back tofloat32(or other dtype if desired).\\nIf a Tensor from the autocast region is alreadyfloat32, the cast is a no-op,\\nand incurs no additional overhead.  Example:, give an example?',\n","   'context': ' autocastcan also be used as a decorator, e.g., on theforwardmethod of your model:',\n","   'source': 'https://pytorch.org/docs/stable/amp.html'},\n","  {'Answer': '# Creates some tensors in default dtype (here assumed to be float32)\\na_float32 = torch.rand((8, 8), device=\"cuda\")\\nb_float32 = torch.rand((8, 8), device=\"cuda\")\\nc_float32 = torch.rand((8, 8), device=\"cuda\")\\nd_float32 = torch.rand((8, 8), device=\"cuda\")\\n\\nwith autocast():\\n    e_float16 = torch.mm(a_float32, b_float32)\\n\\n    with autocast(enabled=False):\\n        # Calls e_float16.float() to ensure float32 execution\\n        # (necessary because e_float16 was created in an autocasted region)\\n        f_float32 = torch.mm(c_float32, e_float16.float())\\n\\n    # No manual casts are required when re-entering the autocast-enabled region.\\n    # torch.mm again runs in float16 and produces float16 output, regardless of input types.\\n    g_float16 = torch.mm(d_float32, f_float32)\\n',\n","   'Id': 122,\n","   'Question': 'How to use autocast(enabled=False)subregions can be nested in autocast-enabled regions.\\nLocally disabling autocast can be useful, for example, if you want to force a subregion\\nto run in a particulardtype.  Disabling autocast gives you explicit control over\\nthe execution type.  In the subregion, inputs from the surrounding region\\nshould be cast todtypebefore use:, give an example?',\n","   'context': ' Type mismatch errorsinan autocast-enabled region are a bug; if this is what you observe,\\nplease file an issue.autocast(enabled=False)subregions can be nested in autocast-enabled regions.\\nLocally disabling autocast can be useful, for example, if you want to force a subregion\\nto run in a particulardtype.  Disabling autocast gives you explicit control over\\nthe execution type.  In the subregion, inputs from the surrounding region\\nshould be cast todtypebefore use:',\n","   'source': 'https://pytorch.org/docs/stable/amp.html'},\n","  {'Answer': '...\\nscaler.scale(loss).backward()\\nscaler.unscale_(optimizer)\\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\\nscaler.step(optimizer)\\nscaler.update()\\n',\n","   'Id': 123,\n","   'Question': 'How to use torch.cuda.amp.GradScaler.unscale_, give an example?',\n","   'context': ' unscale_()is optional, serving cases where you need tomodify or inspect gradientsbetween the backward pass(es) andstep().\\nIfunscale_()is not called explicitly,  gradients will be unscaled  automatically duringstep().Simple example, usingunscale_()to enable clipping of unscaled gradients:',\n","   'source': 'https://pytorch.org/docs/stable/amp.html'},\n","  {'Answer': '>>> A = torch.randn(3, 3)\\n>>> torch.det(A)\\ntensor(0.2611)\\n>>> torch.logdet(A)\\ntensor(-1.3430)\\n>>> A\\ntensor([[[ 0.9254, -0.6213],\\n         [-0.5787,  1.6843]],\\n\\n        [[ 0.3242, -0.9665],\\n         [ 0.4539, -0.0887]],\\n\\n        [[ 1.1336, -0.4025],\\n         [-0.7089,  0.9032]]])\\n>>> A.det()\\ntensor([1.1990, 0.4099, 0.7386])\\n>>> A.det().log()\\ntensor([ 0.1815, -0.8917, -0.3031])\\n',\n","   'Id': 124,\n","   'Question': 'How to use torch.logdet, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.logdet.html#torch.logdet'},\n","  {'Answer': '>>> torch.lt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\\ntensor([[False, False], [True, False]])\\n',\n","   'Id': 125,\n","   'Question': 'How to use torch.lt, give an example?',\n","   'context': ' The second argument can be a number or a tensor whose shape isbroadcastablewith the first argument.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.lt.html#torch.lt'},\n","  {'Answer': '>>> torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8)\\ntensor([-1.,  0.,  1.,  2.], size=(4,), dtype=torch.quint8,\\n       quantization_scheme=torch.per_tensor_affine, scale=0.1, zero_point=10)\\n>>> torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8).int_repr()\\ntensor([ 0, 10, 20, 30], dtype=torch.uint8)\\n',\n","   'Id': 126,\n","   'Question': 'How to use torch.quantize_per_tensor, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.quantize_per_tensor.html#torch.quantize_per_tensor'},\n","  {'Answer': '>>> a = torch.randn(4)\\n>>> a\\ntensor([-2.0755,  1.0226,  0.0831,  0.4806])\\n>>> torch.square(a)\\ntensor([ 4.3077,  1.0457,  0.0069,  0.2310])\\n',\n","   'Id': 127,\n","   'Question': 'How to use torch.square, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.square.html#torch.square'},\n","  {'Answer': '>>> src = torch.tensor([[4, 3, 5],\\n...                     [6, 7, 8]])\\n>>> torch.take(src, torch.tensor([0, 2, 5]))\\ntensor([ 4,  5,  8])\\n',\n","   'Id': 128,\n","   'Question': 'How to use torch.take, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.take.html#torch.take'},\n","  {'Answer': '>>> x = torch.zeros(1, requires_grad=True)\\n>>> with torch.no_grad():\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> is_train = False\\n>>> with torch.set_grad_enabled(is_train):\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> torch.set_grad_enabled(True)  # this can also be used as a function\\n>>> y = x * 2\\n>>> y.requires_grad\\nTrue\\n\\n>>> torch.set_grad_enabled(False)\\n>>> y = x * 2\\n>>> y.requires_grad\\nFalse\\n',\n","   'Id': 129,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/torch.html#torch.torch.default_generator'},\n","  {'Answer': '>>> torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1))\\ntensor([  1.0425,   3.5672,   2.7969,   4.2925,   4.7229,   6.2134,\\n          8.0505,   8.1408,   9.0563,  10.0566])\\n',\n","   'Id': 130,\n","   'Question': 'How to use torch.normal, give an example?',\n","   'context': ' The shapes ofmeanandstddon’t need to match, but the\\ntotal number of elements in each tensor need to be the same.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.normal.html#torch.normal'},\n","  {'Answer': '>>> torch.normal(mean=0.5, std=torch.arange(1., 6.))\\ntensor([-1.2793, -1.0732, -2.0687,  5.1177, -1.2303])\\n',\n","   'Id': 131,\n","   'Question': 'How  Similar to the function above, but the means are shared among all drawn\\nelements., give an example?',\n","   'context': ' Similar to the function above, but the means are shared among all drawn\\nelements.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.normal.html#torch.normal'},\n","  {'Answer': '>>> torch.normal(mean=torch.arange(1., 6.))\\ntensor([ 1.1552,  2.6148,  2.6535,  5.8318,  4.2361])\\n',\n","   'Id': 132,\n","   'Question': 'How  Similar to the function above, but the standard deviations are shared among\\nall drawn elements., give an example?',\n","   'context': ' Similar to the function above, but the standard deviations are shared among\\nall drawn elements.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.normal.html#torch.normal'},\n","  {'Answer': '>>> torch.normal(2, 3, size=(1, 4))\\ntensor([[-1.3987, -1.9544,  3.6048,  0.7909]])\\n',\n","   'Id': 133,\n","   'Question': 'How  Similar to the function above, but the means and standard deviations are shared\\namong all drawn elements. The resulting tensor has size given bysize., give an example?',\n","   'context': ' Similar to the function above, but the means and standard deviations are shared\\namong all drawn elements. The resulting tensor has size given bysize.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.normal.html#torch.normal'},\n","  {'Answer': '>>> a = torch.tensor([[-0.8166, -1.3802, -0.3560]])\\n>>> torch.std_mean(a, unbiased=False)\\n(tensor(0.4188), tensor(-0.8509))\\n',\n","   'Id': 134,\n","   'Question': 'How to use torch.std_mean, give an example?',\n","   'context': ' IfunbiasedisTrue, Bessel’s correction will be used.\\nOtherwise, the sample deviation is calculated, without any correction.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.std_mean.html#torch.std_mean'},\n","  {'Answer': '>>> x = torch.tensor([[[0,1],[2,3]],[[4,5],[6,7]]])\\n>>> x\\ntensor([[[0, 1],\\n        [2, 3]],\\n\\n        [[4, 5],\\n        [6, 7]]])\\n>>> torch.swapdims(x, 0, 1)\\ntensor([[[0, 1],\\n        [4, 5]],\\n\\n        [[2, 3],\\n        [6, 7]]])\\n>>> torch.swapdims(x, 0, 2)\\ntensor([[[0, 4],\\n        [2, 6]],\\n\\n        [[1, 5],\\n        [3, 7]]])\\n',\n","   'Id': 135,\n","   'Question': 'How to use torch.swapdims, give an example?',\n","   'context': ' This function is equivalent to NumPy’s swapaxes function.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.swapdims.html#torch.swapdims'},\n","  {'Answer': '>>> a = torch.tensor([1, 2, 3])\\n>>> b = torch.tensor([4, 5, 6])\\n>>> torch.dstack((a,b))\\ntensor([[[1, 4],\\n         [2, 5],\\n         [3, 6]]])\\n>>> a = torch.tensor([[1],[2],[3]])\\n>>> b = torch.tensor([[4],[5],[6]])\\n>>> torch.dstack((a,b))\\ntensor([[[1, 4]],\\n        [[2, 5]],\\n        [[3, 6]]])\\n',\n","   'Id': 136,\n","   'Question': 'How to use torch.dstack, give an example?',\n","   'context': ' This is equivalent to concatenation along the third axis after 1-D and 2-D tensors have been reshaped bytorch.atleast_3d().',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.dstack.html#torch.dstack'},\n","  {'Answer': '>>> a = torch.arange(-0.5, 1, 0.5)\\n>>> a\\ntensor([-0.5000,  0.0000,  0.5000])\\n>>> torch.special.entr(a)\\ntensor([  -inf, 0.0000, 0.3466])\\n',\n","   'Id': 137,\n","   'Question': 'How to use torch.special.entr, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.erfinv'},\n","  {'Answer': '>>> torch.special.erf(torch.tensor([0, -1., 10.]))\\ntensor([ 0.0000, -0.8427,  1.0000])\\n',\n","   'Id': 138,\n","   'Question': 'How to use torch.special.erf, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.erfinv'},\n","  {'Answer': '>>> torch.special.erfc(torch.tensor([0, -1., 10.]))\\ntensor([ 1.0000, 1.8427,  0.0000])\\n',\n","   'Id': 139,\n","   'Question': 'How to use torch.special.erfc, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.erfinv'},\n","  {'Answer': '>>> torch.special.erfinv(torch.tensor([0, 0.5, -1.]))\\ntensor([ 0.0000,  0.4769,    -inf])\\n',\n","   'Id': 140,\n","   'Question': 'How to use torch.special.erfinv, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.erfinv'},\n","  {'Answer': '>>> t = torch.randn(4)\\n>>> t\\ntensor([ 0.9213,  1.0887, -0.8858, -1.7683])\\n>>> torch.special.expit(t)\\ntensor([ 0.7153,  0.7481,  0.2920,  0.1458])\\n',\n","   'Id': 141,\n","   'Question': 'How to use torch.special.expit, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.erfinv'},\n","  {'Answer': '>>> torch.special.expm1(torch.tensor([0, math.log(2.)]))\\ntensor([ 0.,  1.])\\n',\n","   'Id': 142,\n","   'Question': 'How to use torch.special.expm1, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.erfinv'},\n","  {'Answer': '>>> torch.special.exp2(torch.tensor([0, math.log2(2.), 3, 4]))\\ntensor([ 1.,  2.,  8., 16.])\\n',\n","   'Id': 143,\n","   'Question': 'How to use torch.special.exp2, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.erfinv'},\n","  {'Answer': '>>> a = torch.arange(0.5, 2, 0.5)\\n>>> torch.special.gammaln(a)\\ntensor([ 0.5724,  0.0000, -0.1208])\\n',\n","   'Id': 144,\n","   'Question': 'How to use torch.special.gammaln, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.erfinv'},\n","  {'Answer': '>>> torch.special.i0e(torch.arange(5, dtype=torch.float32))\\ntensor([1.0000, 0.4658, 0.3085, 0.2430, 0.2070])\\n',\n","   'Id': 145,\n","   'Question': 'How to use torch.special.i0e, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.erfinv'},\n","  {'Answer': '>>> a = torch.rand(5)\\n>>> a\\ntensor([0.2796, 0.9331, 0.6486, 0.1523, 0.6516])\\n>>> torch.special.logit(a, eps=1e-6)\\ntensor([-0.9466,  2.6352,  0.6131, -1.7169,  0.6261])\\n',\n","   'Id': 146,\n","   'Question': 'How to use torch.special.logit, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.erfinv'},\n","  {'Answer': \">>> x = torch.zeros(5,)\\n>>> y = torch.tensor([-1, 0, 1, float('inf'), float('nan')])\\n>>> torch.special.xlog1py(x, y)\\ntensor([0., 0., 0., 0., nan])\\n>>> x = torch.tensor([1, 2, 3])\\n>>> y = torch.tensor([3, 2, 1])\\n>>> torch.special.xlog1py(x, y)\\ntensor([1.3863, 2.1972, 2.0794])\\n>>> torch.special.xlog1py(x, 4)\\ntensor([1.6094, 3.2189, 4.8283])\\n>>> torch.special.xlog1py(2, y)\\ntensor([2.7726, 2.1972, 1.3863])\\n\",\n","   'Id': 147,\n","   'Question': 'How to use torch.special.xlog1py, give an example?',\n","   'context': ' Similar to SciPy’sscipy.special.xlog1py.',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.erfinv'},\n","  {'Answer': \">>> a = torch.tensor([-float('inf'), float('inf'), 1.2])\\n>>> torch.isneginf(a)\\ntensor([ True, False, False])\\n\",\n","   'Id': 148,\n","   'Question': 'How to use torch.isneginf, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.isneginf.html#torch.isneginf'},\n","  {'Answer': \">>> a = torch.arange(60.).reshape(3, 4, 5)\\n>>> b = torch.arange(24.).reshape(4, 3, 2)\\n>>> torch.tensordot(a, b, dims=([1, 0], [0, 1]))\\ntensor([[4400., 4730.],\\n        [4532., 4874.],\\n        [4664., 5018.],\\n        [4796., 5162.],\\n        [4928., 5306.]])\\n\\n>>> a = torch.randn(3, 4, 5, device='cuda')\\n>>> b = torch.randn(4, 5, 6, device='cuda')\\n>>> c = torch.tensordot(a, b, dims=2).cpu()\\ntensor([[ 8.3504, -2.5436,  6.2922,  2.7556, -1.0732,  3.2741],\\n        [ 3.3161,  0.0704,  5.0187, -0.4079, -4.3126,  4.8744],\\n        [ 0.8223,  3.9445,  3.2168, -0.2400,  3.4117,  1.7780]])\\n\\n>>> a = torch.randn(3, 5, 4, 6)\\n>>> b = torch.randn(6, 4, 5, 3)\\n>>> torch.tensordot(a, b, dims=([2, 1, 3], [1, 2, 0]))\\ntensor([[  7.7193,  -2.4867, -10.3204],\\n        [  1.5513, -14.4737,  -6.5113],\\n        [ -0.2850,   4.2573,  -3.5997]])\\n\",\n","   'Id': 149,\n","   'Question': 'How to use torch.tensordot, give an example?',\n","   'context': ' When called withdimsof the list form, the given dimensions will be contracted\\nin place of the lastdddofaand the firstdddofbbb. The sizes\\nin these dimensions must match, buttensordot()will deal with broadcasted\\ndimensions.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.tensordot.html#torch.tensordot'},\n","  {'Answer': '>>> a = torch.randn(4)\\n>>> a\\ntensor([-1.7120,  0.1734, -0.0478, -0.0922])\\n>>> torch.clamp(a, min=-0.5, max=0.5)\\ntensor([-0.5000,  0.1734, -0.0478, -0.0922])\\n\\n>>> min = torch.linspace(-1, 1, steps=4)\\n>>> torch.clamp(a, min=min)\\ntensor([-1.0000,  0.1734,  0.3333,  1.0000])\\n',\n","   'Id': 150,\n","   'Question': 'How to use torch.clamp, give an example?',\n","   'context': ' IfminisNone, there is no lower bound.\\nOr, ifmaxisNonethere is no upper bound.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.clamp.html#torch.clamp'},\n","  {'Answer': '>>> x = torch.ones(3, 3)\\n>>> x[1].fill_(2)\\ntensor([ 2.,  2.,  2.])\\n>>> x[2].fill_(3)\\ntensor([ 3.,  3.,  3.])\\n>>> x\\ntensor([[ 1.,  1.,  1.],\\n        [ 2.,  2.,  2.],\\n        [ 3.,  3.,  3.]])\\n>>> torch.renorm(x, 1, 0, 5)\\ntensor([[ 1.0000,  1.0000,  1.0000],\\n        [ 1.6667,  1.6667,  1.6667],\\n        [ 1.6667,  1.6667,  1.6667]])\\n',\n","   'Id': 151,\n","   'Question': 'How to use torch.renorm, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.renorm.html#torch.renorm'},\n","  {'Answer': '>>> a = torch.randn(10)\\n>>> torch.logcumsumexp(a, dim=0)\\ntensor([-0.42296738, -0.04462666,  0.86278635,  0.94622083,  1.05277811,\\n         1.39202815,  1.83525007,  1.84492621,  2.06084887,  2.06844475]))\\n',\n","   'Id': 152,\n","   'Question': 'How to use torch.logcumsumexp, give an example?',\n","   'context': ' For summation indexjjjgiven bydimand other indicesiii, the result is',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.logcumsumexp.html#torch.logcumsumexp'},\n","  {'Answer': '>>> x = torch.randn(2, 3)\\n>>> x\\ntensor([[ 0.6580, -1.0969, -0.4614],\\n        [-0.1034, -0.5790,  0.1497]])\\n>>> torch.cat((x, x, x), 0)\\ntensor([[ 0.6580, -1.0969, -0.4614],\\n        [-0.1034, -0.5790,  0.1497],\\n        [ 0.6580, -1.0969, -0.4614],\\n        [-0.1034, -0.5790,  0.1497],\\n        [ 0.6580, -1.0969, -0.4614],\\n        [-0.1034, -0.5790,  0.1497]])\\n>>> torch.cat((x, x, x), 1)\\ntensor([[ 0.6580, -1.0969, -0.4614,  0.6580, -1.0969, -0.4614,  0.6580,\\n         -1.0969, -0.4614],\\n        [-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034,\\n         -0.5790,  0.1497]])\\n',\n","   'Id': 153,\n","   'Question': 'How to use torch.cat, give an example?',\n","   'context': ' torch.cat()can be best understood via examples.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat'},\n","  {'Answer': '>>> a = torch.tensor([[0.9041,  0.0196], [-0.3108, -2.4423], [-0.4821,  1.059]])\\n>>> a\\ntensor([[ 0.9041,  0.0196],\\n        [-0.3108, -2.4423],\\n        [-0.4821,  1.0590]])\\n>>> b = torch.tensor([[-2.1763, -0.4713], [-0.6986,  1.3702]])\\n>>> b\\ntensor([[-2.1763, -0.4713],\\n        [-0.6986,  1.3702]])\\n>>> torch.cdist(a, b, p=2)\\ntensor([[3.1193, 2.0959],\\n        [2.7138, 3.8322],\\n        [2.2830, 0.3791]])\\n',\n","   'Id': 154,\n","   'Question': 'How to use torch.cdist, give an example?',\n","   'context': ' This function is equivalent toscipy.spatial.distance.cdist(input,’minkowski’, p=p)ifp∈(0,∞)p \\\\in (0, \\\\infty)p∈(0,∞). Whenp=0p = 0p=0it is equivalent toscipy.spatial.distance.cdist(input, ‘hamming’) * M. Whenp=∞p = \\\\inftyp=∞, the closest\\nscipy function isscipy.spatial.distance.cdist(xn, lambda x, y: np.abs(x - y).max()).',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.cdist.html#torch.cdist'},\n","  {'Answer': '>>> x = torch.randn(3, 4)\\n>>> x\\ntensor([[ 0.1427,  0.0231, -0.5414, -1.0009],\\n        [-0.4664,  0.2647, -0.1228, -1.1068],\\n        [-1.1734, -0.6571,  0.7230, -0.6004]])\\n>>> indices = torch.tensor([0, 2])\\n>>> torch.index_select(x, 0, indices)\\ntensor([[ 0.1427,  0.0231, -0.5414, -1.0009],\\n        [-1.1734, -0.6571,  0.7230, -0.6004]])\\n>>> torch.index_select(x, 1, indices)\\ntensor([[ 0.1427, -0.5414],\\n        [-0.4664, -0.1228],\\n        [-1.1734,  0.7230]])\\n',\n","   'Id': 155,\n","   'Question': 'How to use torch.index_select, give an example?',\n","   'context': ' The returned tensor has the same number of dimensions as the original tensor\\n(input).  Thedimth dimension has the same size as the length\\nofindex; other dimensions have the same size as in the original tensor.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.index_select.html#torch.index_select'},\n","  {'Answer': '>>> float_tensor = torch.ones(1, dtype=torch.float)\\n>>> double_tensor = torch.ones(1, dtype=torch.double)\\n>>> complex_float_tensor = torch.ones(1, dtype=torch.complex64)\\n>>> complex_double_tensor = torch.ones(1, dtype=torch.complex128)\\n>>> int_tensor = torch.ones(1, dtype=torch.int)\\n>>> long_tensor = torch.ones(1, dtype=torch.long)\\n>>> uint_tensor = torch.ones(1, dtype=torch.uint8)\\n>>> double_tensor = torch.ones(1, dtype=torch.double)\\n>>> bool_tensor = torch.ones(1, dtype=torch.bool)\\n# zero-dim tensors\\n>>> long_zerodim = torch.tensor(1, dtype=torch.long)\\n>>> int_zerodim = torch.tensor(1, dtype=torch.int)\\n\\n>>> torch.add(5, 5).dtype\\ntorch.int64\\n# 5 is an int64, but does not have higher category than int_tensor so is not considered.\\n>>> (int_tensor + 5).dtype\\ntorch.int32\\n>>> (int_tensor + long_zerodim).dtype\\ntorch.int32\\n>>> (long_tensor + int_tensor).dtype\\ntorch.int64\\n>>> (bool_tensor + long_tensor).dtype\\ntorch.int64\\n>>> (bool_tensor + uint_tensor).dtype\\ntorch.uint8\\n>>> (float_tensor + double_tensor).dtype\\ntorch.float64\\n>>> (complex_float_tensor + complex_double_tensor).dtype\\ntorch.complex128\\n>>> (bool_tensor + int_tensor).dtype\\ntorch.int32\\n# Since long is a different kind than float, result dtype only needs to be large enough\\n# to hold the float.\\n>>> torch.add(long_tensor, float_tensor).dtype\\ntorch.float32\\n',\n","   'Id': 156,\n","   'Question': 'How  A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\\nare not yet supported., give an example?',\n","   'context': ' A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\\nare not yet supported.',\n","   'source': 'https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc'},\n","  {'Answer': \"# allowed:\\n>>> float_tensor *= float_tensor\\n>>> float_tensor *= int_tensor\\n>>> float_tensor *= uint_tensor\\n>>> float_tensor *= bool_tensor\\n>>> float_tensor *= double_tensor\\n>>> int_tensor *= long_tensor\\n>>> int_tensor *= uint_tensor\\n>>> uint_tensor *= int_tensor\\n\\n# disallowed (RuntimeError: result type can't be cast to the desired output type):\\n>>> int_tensor *= float_tensor\\n>>> bool_tensor *= int_tensor\\n>>> bool_tensor *= uint_tensor\\n>>> float_tensor *= complex_float_tensor\\n\",\n","   'Id': 157,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc'},\n","  {'Answer': \">>> torch.device('cuda:0')\\ndevice(type='cuda', index=0)\\n\\n>>> torch.device('cpu')\\ndevice(type='cpu')\\n\\n>>> torch.device('cuda')  # current cuda device\\ndevice(type='cuda')\\n\",\n","   'Id': 158,\n","   'Question': 'How  Atorch.devicecan be constructed via a string or via a string and device ordinalVia a string:, give an example?',\n","   'context': ' Atorch.devicecan be constructed via a string or via a string and device ordinalVia a string:',\n","   'source': 'https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc'},\n","  {'Answer': \">>> torch.device('cuda', 0)\\ndevice(type='cuda', index=0)\\n\\n>>> torch.device('cpu', 0)\\ndevice(type='cpu', index=0)\\n\",\n","   'Id': 159,\n","   'Question': 'How  Via a string:Via a string and device ordinal:, give an example?',\n","   'context': ' Via a string:Via a string and device ordinal:',\n","   'source': 'https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc'},\n","  {'Answer': \">>> # Example of a function that takes in a torch.device\\n>>> cuda1 = torch.device('cuda:1')\\n>>> torch.randn((2,3), device=cuda1)\\n\",\n","   'Id': 160,\n","   'Question': 'How  Thetorch.deviceargument in functions can generally be substituted with a string.\\nThis allows for fast prototyping of code., give an example?',\n","   'context': ' Thetorch.deviceargument in functions can generally be substituted with a string.\\nThis allows for fast prototyping of code.',\n","   'source': 'https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc'},\n","  {'Answer': \">>> # You can substitute the torch.device with a string\\n>>> torch.randn((2,3), device='cuda:1')\\n\",\n","   'Id': 161,\n","   'Question': 'How  Thetorch.deviceargument in functions can generally be substituted with a string.\\nThis allows for fast prototyping of code., give an example?',\n","   'context': ' Thetorch.deviceargument in functions can generally be substituted with a string.\\nThis allows for fast prototyping of code.',\n","   'source': 'https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc'},\n","  {'Answer': \">>> torch.device(1)\\ndevice(type='cuda', index=1)\\n\",\n","   'Id': 162,\n","   'Question': 'How  For legacy reasons, a device can be constructed via a single device ordinal, which is treated\\nas a cuda device.  This matchesTensor.get_device(), which returns an ordinal for cuda\\ntensors and is not supported for cpu tensors., give an example?',\n","   'context': ' For legacy reasons, a device can be constructed via a single device ordinal, which is treated\\nas a cuda device.  This matchesTensor.get_device(), which returns an ordinal for cuda\\ntensors and is not supported for cpu tensors.',\n","   'source': 'https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc'},\n","  {'Answer': \">>> torch.randn((2,3), device=torch.device('cuda:1'))\\n>>> torch.randn((2,3), device='cuda:1')\\n>>> torch.randn((2,3), device=1)  # legacy\\n\",\n","   'Id': 163,\n","   'Question': 'How  Methods which take a device will generally accept a (properly formatted) string\\nor (legacy) integer device ordinal, i.e. the following are all equivalent:, give an example?',\n","   'context': ' Methods which take a device will generally accept a (properly formatted) string\\nor (legacy) integer device ordinal, i.e. the following are all equivalent:',\n","   'source': 'https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc'},\n","  {'Answer': '>>> x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\\n>>> x.stride()\\n(5, 1)\\n\\n>>> x.t().stride()\\n(1, 5)\\n',\n","   'Id': 164,\n","   'Question': 'How  torch.stridedrepresents dense Tensors and is the memory layout that\\nis most commonly used. Each strided tensor has an associatedtorch.Storage, which holds its data. These tensors provide\\nmulti-dimensional,stridedview of a storage. Strides are a list of integers: the k-th stride\\nrepresents the jump in the memory necessary to go from one element to the\\nnext one in the k-th dimension of the Tensor. This concept makes it possible\\nto perform many tensor operations efficiently., give an example?',\n","   'context': ' torch.stridedrepresents dense Tensors and is the memory layout that\\nis most commonly used. Each strided tensor has an associatedtorch.Storage, which holds its data. These tensors provide\\nmulti-dimensional,stridedview of a storage. Strides are a list of integers: the k-th stride\\nrepresents the jump in the memory necessary to go from one element to the\\nnext one in the k-th dimension of the Tensor. This concept makes it possible\\nto perform many tensor operations efficiently.',\n","   'source': 'https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc'},\n","  {'Answer': '>>> a = torch.randn(3, 3)\\n>>> a = torch.mm(a, a.t()) # make symmetric positive definite\\n>>> u = torch.cholesky(a)\\n>>> a\\ntensor([[ 0.7747, -1.9549,  1.3086],\\n        [-1.9549,  6.7546, -5.4114],\\n        [ 1.3086, -5.4114,  4.8733]])\\n>>> b = torch.randn(3, 2)\\n>>> b\\ntensor([[-0.6355,  0.9891],\\n        [ 0.1974,  1.4706],\\n        [-0.4115, -0.6225]])\\n>>> torch.cholesky_solve(b, u)\\ntensor([[ -8.1625,  19.6097],\\n        [ -5.8398,  14.2387],\\n        [ -4.3771,  10.4173]])\\n>>> torch.mm(a.inverse(), b)\\ntensor([[ -8.1626,  19.6097],\\n        [ -5.8398,  14.2387],\\n        [ -4.3771,  10.4173]])\\n',\n","   'Id': 165,\n","   'Question': 'How to use torch.cholesky_solve, give an example?',\n","   'context': ' Supports real-valued and complex-valued inputs.\\nFor the complex-valued inputs the transpose operator above is the conjugate transpose.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.cholesky_solve.html#torch.cholesky_solve'},\n","  {'Answer': '>>> a = torch.randn(4, 4)\\n>>> a\\ntensor([[ 0.1139,  0.2254, -0.1381,  0.3687],\\n        [ 1.0100, -1.1975, -0.0102, -0.4732],\\n        [-0.9240,  0.1207, -0.7506, -1.0213],\\n        [ 1.7809, -1.2960,  0.9384,  0.1438]])\\n>>> torch.argmin(a)\\ntensor(13)\\n>>> torch.argmin(a, dim=1)\\ntensor([ 2,  1,  3,  1])\\n>>> torch.argmin(a, dim=1, keepdim=True)\\ntensor([[2],\\n        [1],\\n        [3],\\n        [1]])\\n',\n","   'Id': 166,\n","   'Question': 'How to use torch.argmin, give an example?',\n","   'context': ' This is the second value returned bytorch.min(). See its\\ndocumentation for the exact semantics of this method.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.argmin.html#torch.argmin'},\n","  {'Answer': 'probs = policy_network(state)\\n# Note that this is equivalent to what used to be called multinomial\\nm = Categorical(probs)\\naction = m.sample()\\nnext_state, reward = env.step(action)\\nloss = -m.log_prob(action) * reward\\nloss.backward()\\n',\n","   'Id': 167,\n","   'Question': 'How to use whereθ\\\\thetaθare the parameters,α\\\\alphaαis the learning rate,rrris the reward andp(a∣πθ(s))p(a|\\\\pi^\\\\theta(s))p(a∣πθ(s))is the probability of\\ntaking actionaaain statesssgiven policyπθ\\\\pi^\\\\thetaπθ.In practice we would sample an action from the output of a network, apply this\\naction in an environment, and then uselog_probto construct an equivalent\\nloss function. Note that we use a negative because optimizers use gradient\\ndescent, whilst the rule above assumes gradient ascent. With a categorical\\npolicy, the code for implementing REINFORCE would be as follows:, give an example?',\n","   'context': ' whereθ\\\\thetaθare the parameters,α\\\\alphaαis the learning rate,rrris the reward andp(a∣πθ(s))p(a|\\\\pi^\\\\theta(s))p(a∣πθ(s))is the probability of\\ntaking actionaaain statesssgiven policyπθ\\\\pi^\\\\thetaπθ.In practice we would sample an action from the output of a network, apply this\\naction in an environment, and then uselog_probto construct an equivalent\\nloss function. Note that we use a negative because optimizers use gradient\\ndescent, whilst the rule above assumes gradient ascent. With a categorical\\npolicy, the code for implementing REINFORCE would be as follows:',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': 'params = policy_network(state)\\nm = Normal(*params)\\n# Any distribution with .has_rsample == True could work based on the application\\naction = m.rsample()\\nnext_state, reward = env.step(action)  # Assuming that reward is differentiable\\nloss = -reward\\nloss.backward()\\n',\n","   'Id': 168,\n","   'Question': 'How to use The other way to implement these stochastic/policy gradients would be to use the\\nreparameterization trick from thersample()method, where the\\nparameterized random variable can be constructed via a parameterized\\ndeterministic function of a parameter-free random variable. The reparameterized\\nsample therefore becomes differentiable. The code for implementing the pathwise\\nderivative would be as follows:, give an example?',\n","   'context': ' The other way to implement these stochastic/policy gradients would be to use the\\nreparameterization trick from thersample()method, where the\\nparameterized random variable can be constructed via a parameterized\\ndeterministic function of a parameter-free random variable. The reparameterized\\nsample therefore becomes differentiable. The code for implementing the pathwise\\nderivative would be as follows:',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '>>> m = Bernoulli(torch.tensor([0.3]))\\n>>> m.sample()  # 30% chance 1; 70% chance 0\\ntensor([ 0.])\\n',\n","   'Id': 169,\n","   'Question': 'How to use torch.distributions.bernoulli.Bernoulli, give an example?',\n","   'context': ' Samples are binary (0 or 1). They take the value1with probabilitypand0with probability1 - p.',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '>>> m = Beta(torch.tensor([0.5]), torch.tensor([0.5]))\\n>>> m.sample()  # Beta distributed with concentration concentration1 and concentration0\\ntensor([ 0.1046])\\n',\n","   'Id': 170,\n","   'Question': 'How to use torch.distributions.beta.Beta, give an example?',\n","   'context': ' Beta distribution parameterized byconcentration1andconcentration0.',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '>>> m = Binomial(100, torch.tensor([0 , .2, .8, 1]))\\n>>> x = m.sample()\\ntensor([   0.,   22.,   71.,  100.])\\n\\n>>> m = Binomial(torch.tensor([[5.], [10.]]), torch.tensor([0.5, 0.8]))\\n>>> x = m.sample()\\ntensor([[ 4.,  5.],\\n        [ 7.,  6.]])\\n',\n","   'Id': 171,\n","   'Question': 'How to use torch.distributions.binomial.Binomial, give an example?',\n","   'context': ' Creates a Binomial distribution parameterized bytotal_countand\\neitherprobsorlogits(but not both).total_countmust be\\nbroadcastable withprobs/logits.',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '>>> m = Categorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\\n>>> m.sample()  # equal probability of 0, 1, 2, 3\\ntensor(3)\\n',\n","   'Id': 172,\n","   'Question': 'How to use torch.distributions.categorical.Categorical, give an example?',\n","   'context': ' See also:torch.multinomial()',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '>>> m = Cauchy(torch.tensor([0.0]), torch.tensor([1.0]))\\n>>> m.sample()  # sample from a Cauchy distribution with loc=0 and scale=1\\ntensor([ 2.3214])\\n',\n","   'Id': 173,\n","   'Question': 'How to use torch.distributions.cauchy.Cauchy, give an example?',\n","   'context': ' Samples from a Cauchy (Lorentz) distribution. The distribution of the ratio of\\nindependent normally distributed random variables with means0follows a\\nCauchy distribution.',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '>>> m = Chi2(torch.tensor([1.0]))\\n>>> m.sample()  # Chi2 distributed with shape df=1\\ntensor([ 0.1046])\\n',\n","   'Id': 174,\n","   'Question': 'How to use torch.distributions.chi2.Chi2, give an example?',\n","   'context': ' Creates a Chi2 distribution parameterized by shape parameterdf.\\nThis is exactly equivalent toGamma(alpha=0.5*df,beta=0.5)',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '>>> m = ContinuousBernoulli(torch.tensor([0.3]))\\n>>> m.sample()\\ntensor([ 0.2538])\\n',\n","   'Id': 175,\n","   'Question': 'How to use torch.distributions.continuous_bernoulli.ContinuousBernoulli, give an example?',\n","   'context': ' The distribution is supported in [0, 1] and parameterized by ‘probs’ (in\\n(0,1)) or ‘logits’ (real-valued). Note that, unlike the Bernoulli, ‘probs’\\ndoes not correspond to a probability and ‘logits’ does not correspond to\\nlog-odds, but the same names are used due to the similarity with the\\nBernoulli. See [1] for more details.',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '>>> m = Dirichlet(torch.tensor([0.5, 0.5]))\\n>>> m.sample()  # Dirichlet distributed with concentrarion concentration\\ntensor([ 0.1046,  0.8954])\\n',\n","   'Id': 176,\n","   'Question': 'How to use torch.distributions.dirichlet.Dirichlet, give an example?',\n","   'context': ' Creates a Dirichlet distribution parameterized by concentrationconcentration.',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '>>> m = Exponential(torch.tensor([1.0]))\\n>>> m.sample()  # Exponential distributed with rate=1\\ntensor([ 0.1046])\\n',\n","   'Id': 177,\n","   'Question': 'How to use torch.distributions.exponential.Exponential, give an example?',\n","   'context': ' Creates a Exponential distribution parameterized byrate.',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '>>> m = FisherSnedecor(torch.tensor([1.0]), torch.tensor([2.0]))\\n>>> m.sample()  # Fisher-Snedecor-distributed with df1=1 and df2=2\\ntensor([ 0.2453])\\n',\n","   'Id': 178,\n","   'Question': 'How to use torch.distributions.fishersnedecor.FisherSnedecor, give an example?',\n","   'context': ' Creates a Fisher-Snedecor distribution parameterized bydf1anddf2.',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '>>> m = Gamma(torch.tensor([1.0]), torch.tensor([1.0]))\\n>>> m.sample()  # Gamma distributed with concentration=1 and rate=1\\ntensor([ 0.1046])\\n',\n","   'Id': 179,\n","   'Question': 'How to use torch.distributions.gamma.Gamma, give an example?',\n","   'context': ' Creates a Gamma distribution parameterized by shapeconcentrationandrate.',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '>>> m = Geometric(torch.tensor([0.3]))\\n>>> m.sample()  # underlying Bernoulli has 30% chance 1; 70% chance 0\\ntensor([ 2.])\\n',\n","   'Id': 180,\n","   'Question': 'How to use torch.distributions.geometric.Geometric, give an example?',\n","   'context': ' Samples are non-negative integers [0,inf\\u2061\\\\infinf).',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '>>> m = Gumbel(torch.tensor([1.0]), torch.tensor([2.0]))\\n>>> m.sample()  # sample from Gumbel distribution with loc=1, scale=2\\ntensor([ 1.0124])\\n',\n","   'Id': 181,\n","   'Question': 'How to use torch.distributions.gumbel.Gumbel, give an example?',\n","   'context': ' Samples from a Gumbel Distribution.',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': 'X ~ Cauchy(0, scale)\\nY = |X| ~ HalfCauchy(scale)\\n',\n","   'Id': 182,\n","   'Question': 'How to use Creates a half-Cauchy distribution parameterized byscalewhere:, give an example?',\n","   'context': ' Creates a half-Cauchy distribution parameterized byscalewhere:',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '>>> m = HalfCauchy(torch.tensor([1.0]))\\n>>> m.sample()  # half-cauchy distributed with scale=1\\ntensor([ 2.3214])\\n',\n","   'Id': 183,\n","   'Question': 'How to use torch.distributions.half_cauchy.HalfCauchy, give an example?',\n","   'context': ' Creates a half-Cauchy distribution parameterized byscalewhere:',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': 'X ~ Normal(0, scale)\\nY = |X| ~ HalfNormal(scale)\\n',\n","   'Id': 184,\n","   'Question': 'How to use Creates a half-normal distribution parameterized byscalewhere:, give an example?',\n","   'context': ' Creates a half-normal distribution parameterized byscalewhere:',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '>>> m = HalfNormal(torch.tensor([1.0]))\\n>>> m.sample()  # half-normal distributed with scale=1\\ntensor([ 0.1046])\\n',\n","   'Id': 185,\n","   'Question': 'How to use torch.distributions.half_normal.HalfNormal, give an example?',\n","   'context': ' Creates a half-normal distribution parameterized byscalewhere:',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '>>> loc = torch.zeros(3)\\n>>> scale = torch.ones(3)\\n>>> mvn = MultivariateNormal(loc, scale_tril=torch.diag(scale))\\n>>> [mvn.batch_shape, mvn.event_shape]\\n[torch.Size(()), torch.Size((3,))]\\n>>> normal = Normal(loc, scale)\\n>>> [normal.batch_shape, normal.event_shape]\\n[torch.Size((3,)), torch.Size(())]\\n>>> diagn = Independent(normal, 1)\\n>>> [diagn.batch_shape, diagn.event_shape]\\n[torch.Size(()), torch.Size((3,))]\\n',\n","   'Id': 186,\n","   'Question': 'How to use This is mainly useful for changing the shape of the result oflog_prob(). For example to create a diagonal Normal distribution with\\nthe same shape as a Multivariate Normal distribution (so they are\\ninterchangeable), you can:, give an example?',\n","   'context': ' Reinterprets some of the batch dims of a distribution as event dims.This is mainly useful for changing the shape of the result oflog_prob(). For example to create a diagonal Normal distribution with\\nthe same shape as a Multivariate Normal distribution (so they are\\ninterchangeable), you can:',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '>>> m = Kumaraswamy(torch.tensor([1.0]), torch.tensor([1.0]))\\n>>> m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1\\ntensor([ 0.1729])\\n',\n","   'Id': 187,\n","   'Question': 'How to use torch.distributions.kumaraswamy.Kumaraswamy, give an example?',\n","   'context': ' Samples from a Kumaraswamy distribution.',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '>>> l = LKJCholesky(3, 0.5)\\n>>> l.sample()  # l @ l.T is a sample of a correlation 3x3 matrix\\ntensor([[ 1.0000,  0.0000,  0.0000],\\n        [ 0.3516,  0.9361,  0.0000],\\n        [-0.1899,  0.4748,  0.8593]])\\n',\n","   'Id': 188,\n","   'Question': 'How to use torch.distributions.lkj_cholesky.LKJCholesky, give an example?',\n","   'context': ' LKJ distribution for lower Cholesky factor of correlation matrices.\\nThe distribution is controlled byconcentrationparameterη\\\\etaηto make the probability of the correlation matrixMMMgenerated from\\na Cholesky factor propotional todet\\u2061(M)η−1\\\\det(M)^{\\\\eta - 1}det(M)η−1. Because of that,\\nwhenconcentration==1, we have a uniform distribution over Cholesky\\nfactors of correlation matrices. Note that this distribution samples the\\nCholesky factor of correlation matrices and not the correlation matrices\\nthemselves and thereby differs slightly from the derivations in [1] for\\ntheLKJCorrdistribution. For sampling, this uses the Onion method from\\n[1] Section 3.',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '>>> m = Laplace(torch.tensor([0.0]), torch.tensor([1.0]))\\n>>> m.sample()  # Laplace distributed with loc=0, scale=1\\ntensor([ 0.1046])\\n',\n","   'Id': 189,\n","   'Question': 'How to use torch.distributions.laplace.Laplace, give an example?',\n","   'context': ' Creates a Laplace distribution parameterized bylocandscale.',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': 'X ~ Normal(loc, scale)\\nY = exp(X) ~ LogNormal(loc, scale)\\n',\n","   'Id': 190,\n","   'Question': 'How to use Creates a log-normal distribution parameterized bylocandscalewhere:, give an example?',\n","   'context': ' Creates a log-normal distribution parameterized bylocandscalewhere:',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '>>> m = LogNormal(torch.tensor([0.0]), torch.tensor([1.0]))\\n>>> m.sample()  # log-normal distributed with mean=0 and stddev=1\\ntensor([ 0.1046])\\n',\n","   'Id': 191,\n","   'Question': 'How to use torch.distributions.log_normal.LogNormal, give an example?',\n","   'context': ' Creates a log-normal distribution parameterized bylocandscalewhere:',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': 'covariance_matrix = cov_factor @ cov_factor.T + cov_diag\\n',\n","   'Id': 192,\n","   'Question': 'How to use Creates a multivariate normal distribution with covariance matrix having a low-rank form\\nparameterized bycov_factorandcov_diag:, give an example?',\n","   'context': ' Creates a multivariate normal distribution with covariance matrix having a low-rank form\\nparameterized bycov_factorandcov_diag:',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '>>> m = LowRankMultivariateNormal(torch.zeros(2), torch.tensor([[1.], [0.]]), torch.ones(2))\\n>>> m.sample()  # normally distributed with mean=`[0,0]`, cov_factor=`[[1],[0]]`, cov_diag=`[1,1]`\\ntensor([-0.2102, -0.5429])\\n',\n","   'Id': 193,\n","   'Question': 'How to use torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal, give an example?',\n","   'context': ' Creates a multivariate normal distribution with covariance matrix having a low-rank form\\nparameterized bycov_factorandcov_diag:',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': 'capacitance = I + cov_factor.T @ inv(cov_diag) @ cov_factor\\n',\n","   'Id': 194,\n","   'Question': 'How to use The computation for determinant and inverse of covariance matrix is avoided whencov_factor.shape[1] << cov_factor.shape[0]thanks toWoodbury matrix identityandmatrix determinant lemma.\\nThanks to these formulas, we just need to compute the determinant and inverse of\\nthe small size “capacitance” matrix:, give an example?',\n","   'context': ' The computation for determinant and inverse of covariance matrix is avoided whencov_factor.shape[1] << cov_factor.shape[0]thanks toWoodbury matrix identityandmatrix determinant lemma.\\nThanks to these formulas, we just need to compute the determinant and inverse of\\nthe small size “capacitance” matrix:',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '# Construct Gaussian Mixture Model in 1D consisting of 5 equally\\n# weighted normal distributions\\n>>> mix = D.Categorical(torch.ones(5,))\\n>>> comp = D.Normal(torch.randn(5,), torch.rand(5,))\\n>>> gmm = MixtureSameFamily(mix, comp)\\n\\n# Construct Gaussian Mixture Modle in 2D consisting of 5 equally\\n# weighted bivariate normal distributions\\n>>> mix = D.Categorical(torch.ones(5,))\\n>>> comp = D.Independent(D.Normal(\\n             torch.randn(5,2), torch.rand(5,2)), 1)\\n>>> gmm = MixtureSameFamily(mix, comp)\\n\\n# Construct a batch of 3 Gaussian Mixture Models in 2D each\\n# consisting of 5 random weighted bivariate normal distributions\\n>>> mix = D.Categorical(torch.rand(3,5))\\n>>> comp = D.Independent(D.Normal(\\n            torch.randn(3,5,2), torch.rand(3,5,2)), 1)\\n>>> gmm = MixtureSameFamily(mix, comp)\\n',\n","   'Id': 195,\n","   'Question': 'How to use torch.distributions.mixture_same_family.MixtureSameFamily, give an example?',\n","   'context': ' TheMixtureSameFamilydistribution implements a (batch of) mixture\\ndistribution where all component are from different parameterizations of\\nthe same distribution type. It is parameterized by aCategorical“selecting distribution” (overkcomponent) and a component\\ndistribution, i.e., aDistributionwith a rightmost batch shape\\n(equal to[k]) which indexes each (batch of) component.',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '>>> m = Multinomial(100, torch.tensor([ 1., 1., 1., 1.]))\\n>>> x = m.sample()  # equal probability of 0, 1, 2, 3\\ntensor([ 21.,  24.,  30.,  25.])\\n\\n>>> Multinomial(probs=torch.tensor([1., 1., 1., 1.])).log_prob(x)\\ntensor([-4.1338])\\n',\n","   'Id': 196,\n","   'Question': 'How to use torch.distributions.multinomial.Multinomial, give an example?',\n","   'context': ' Note thattotal_countneed not be specified if onlylog_prob()is\\ncalled (see example below)',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '>>> m = MultivariateNormal(torch.zeros(2), torch.eye(2))\\n>>> m.sample()  # normally distributed with mean=`[0,0]` and covariance_matrix=`I`\\ntensor([-0.2102, -0.5429])\\n',\n","   'Id': 197,\n","   'Question': 'How to use torch.distributions.multivariate_normal.MultivariateNormal, give an example?',\n","   'context': ' The multivariate normal distribution can be parameterized either\\nin terms of a positive definite covariance matrixΣ\\\\mathbf{\\\\Sigma}Σor a positive definite precision matrixΣ−1\\\\mathbf{\\\\Sigma}^{-1}Σ−1or a lower-triangular matrixL\\\\mathbf{L}Lwith positive-valued\\ndiagonal entries, such thatΣ=LL⊤\\\\mathbf{\\\\Sigma} = \\\\mathbf{L}\\\\mathbf{L}^\\\\topΣ=LL⊤. This triangular matrix\\ncan be obtained via e.g. Cholesky decomposition of the covariance.',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '>>> m = Normal(torch.tensor([0.0]), torch.tensor([1.0]))\\n>>> m.sample()  # normally distributed with loc=0 and scale=1\\ntensor([ 0.1046])\\n',\n","   'Id': 198,\n","   'Question': 'How to use torch.distributions.normal.Normal, give an example?',\n","   'context': ' Creates a normal (also called Gaussian) distribution parameterized bylocandscale.',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '>>> m = OneHotCategorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\\n>>> m.sample()  # equal probability of 0, 1, 2, 3\\ntensor([ 0.,  0.,  0.,  1.])\\n',\n","   'Id': 199,\n","   'Question': 'How to use torch.distributions.one_hot_categorical.OneHotCategorical, give an example?',\n","   'context': ' See also:torch.distributions.Categorical()for specifications ofprobsandlogits.',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '>>> m = Pareto(torch.tensor([1.0]), torch.tensor([1.0]))\\n>>> m.sample()  # sample from a Pareto distribution with scale=1 and alpha=1\\ntensor([ 1.5623])\\n',\n","   'Id': 200,\n","   'Question': 'How to use torch.distributions.pareto.Pareto, give an example?',\n","   'context': ' Samples from a Pareto Type 1 distribution.',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '>>> m = Poisson(torch.tensor([4]))\\n>>> m.sample()\\ntensor([ 3.])\\n',\n","   'Id': 201,\n","   'Question': 'How to use torch.distributions.poisson.Poisson, give an example?',\n","   'context': ' Samples are nonnegative integers, with a pmf given by',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '>>> m = RelaxedBernoulli(torch.tensor([2.2]),\\n                         torch.tensor([0.1, 0.2, 0.3, 0.99]))\\n>>> m.sample()\\ntensor([ 0.2951,  0.3442,  0.8918,  0.9021])\\n',\n","   'Id': 202,\n","   'Question': 'How to use torch.distributions.relaxed_bernoulli.RelaxedBernoulli, give an example?',\n","   'context': ' Creates a RelaxedBernoulli distribution, parametrized bytemperature, and eitherprobsorlogits(but not both). This is a relaxed version of theBernoullidistribution,\\nso the values are in (0, 1), and has reparametrizable samples.',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '>>> m = RelaxedOneHotCategorical(torch.tensor([2.2]),\\n                                 torch.tensor([0.1, 0.2, 0.3, 0.4]))\\n>>> m.sample()\\ntensor([ 0.1294,  0.2324,  0.3859,  0.2523])\\n',\n","   'Id': 203,\n","   'Question': 'How to use torch.distributions.relaxed_categorical.RelaxedOneHotCategorical, give an example?',\n","   'context': ' Creates a RelaxedOneHotCategorical distribution parametrized bytemperature, and eitherprobsorlogits.\\nThis is a relaxed version of theOneHotCategoricaldistribution, so\\nits samples are on simplex, and are reparametrizable.',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': \">>> m = StudentT(torch.tensor([2.0]))\\n>>> m.sample()  # Student's t-distributed with degrees of freedom=2\\ntensor([ 0.1046])\\n\",\n","   'Id': 204,\n","   'Question': 'How to use torch.distributions.studentT.StudentT, give an example?',\n","   'context': ' Creates a Student’s t-distribution parameterized by degree of\\nfreedomdf, meanlocand scalescale.',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': 'X ~ BaseDistribution\\nY = f(X) ~ TransformedDistribution(BaseDistribution, f)\\nlog p(Y) = log p(X) + log |det (dX/dY)|\\n',\n","   'Id': 205,\n","   'Question': 'How to use Extension of the Distribution class, which applies a sequence of Transforms\\nto a base distribution.  Let f be the composition of transforms applied:, give an example?',\n","   'context': ' Extension of the Distribution class, which applies a sequence of Transforms\\nto a base distribution.  Let f be the composition of transforms applied:',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '# Building a Logistic Distribution\\n# X ~ Uniform(0, 1)\\n# f = a + b * logit(X)\\n# Y ~ f(X) ~ Logistic(a, b)\\nbase_distribution = Uniform(0, 1)\\ntransforms = [SigmoidTransform().inv, AffineTransform(loc=a, scale=b)]\\nlogistic = TransformedDistribution(base_distribution, transforms)\\n',\n","   'Id': 206,\n","   'Question': 'How to use An example for the usage ofTransformedDistributionwould be:, give an example?',\n","   'context': ' Note that the.event_shapeof aTransformedDistributionis the\\nmaximum shape of its base distribution and its transforms, since transforms\\ncan introduce correlations among events.An example for the usage ofTransformedDistributionwould be:',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '>>> m = Uniform(torch.tensor([0.0]), torch.tensor([5.0]))\\n>>> m.sample()  # uniformly distributed in the range [0.0, 5.0)\\ntensor([ 2.3418])\\n',\n","   'Id': 207,\n","   'Question': 'How to use torch.distributions.uniform.Uniform, give an example?',\n","   'context': ' Generates uniformly distributed random samples from the half-open interval[low,high).',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '>>> m = dist.VonMises(torch.tensor([1.0]), torch.tensor([1.0]))\\n>>> m.sample() # von Mises distributed with loc=1 and concentration=1\\ntensor([1.9777])\\n',\n","   'Id': 208,\n","   'Question': 'How to use VonMises, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '>>> m = Weibull(torch.tensor([1.0]), torch.tensor([1.0]))\\n>>> m.sample()  # sample from a Weibull distribution with scale=1, concentration=1\\ntensor([ 0.4784])\\n',\n","   'Id': 209,\n","   'Question': 'How to use torch.distributions.weibull.Weibull, give an example?',\n","   'context': ' Samples from a two-parameter Weibull distribution.',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '@register_kl(Normal, Normal)\\ndef kl_normal_normal(p, q):\\n    # insert implementation here\\n',\n","   'Id': 210,\n","   'Question': 'How to use torch.distributions.kl.register_kl, give an example?',\n","   'context': ' Decorator to register a pairwise function withkl_divergence().\\nUsage:',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '@register_kl(BaseP, DerivedQ)\\ndef kl_version1(p, q): ...\\n@register_kl(DerivedP, BaseQ)\\ndef kl_version2(p, q): ...\\n',\n","   'Id': 211,\n","   'Question': 'How  Lookup returns the most specific (type,type) match ordered by subclass. If\\nthe match is ambiguous, aRuntimeWarningis raised. For example to\\nresolve the ambiguous situation:, give an example?',\n","   'context': ' Lookup returns the most specific (type,type) match ordered by subclass. If\\nthe match is ambiguous, aRuntimeWarningis raised. For example to\\nresolve the ambiguous situation:',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': 'register_kl(DerivedP, DerivedQ)(kl_version1)  # Break the tie.\\n',\n","   'Id': 212,\n","   'Question': 'How  Lookup returns the most specific (type,type) match ordered by subclass. If\\nthe match is ambiguous, aRuntimeWarningis raised. For example to\\nresolve the ambiguous situation:you should register a third most-specific implementation, e.g.:, give an example?',\n","   'context': ' Lookup returns the most specific (type,type) match ordered by subclass. If\\nthe match is ambiguous, aRuntimeWarningis raised. For example to\\nresolve the ambiguous situation:you should register a third most-specific implementation, e.g.:',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': 'y = t(x)\\nt.log_abs_det_jacobian(x, y).backward()  # x will receive gradients.\\n',\n","   'Id': 213,\n","   'Question': 'How to use Caching is useful for transforms whose inverses are either expensive or\\nnumerically unstable. Note that care must be taken with memoized values\\nsince the autograd graph may be reversed. For example while the following\\nworks with or without caching:, give an example?',\n","   'context': ' Caching is useful for transforms whose inverses are either expensive or\\nnumerically unstable. Note that care must be taken with memoized values\\nsince the autograd graph may be reversed. For example while the following\\nworks with or without caching:',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': 'y = t(x)\\nz = t.inv(y)\\ngrad(z.sum(), [y])  # error because z is x\\n',\n","   'Id': 214,\n","   'Question': 'How to use However the following will error when caching due to dependency reversal:, give an example?',\n","   'context': ' Caching is useful for transforms whose inverses are either expensive or\\nnumerically unstable. Note that care must be taken with memoized values\\nsince the autograd graph may be reversed. For example while the following\\nworks with or without caching:However the following will error when caching due to dependency reversal:',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': \"loc = torch.zeros(100, requires_grad=True)\\nunconstrained = torch.zeros(100, requires_grad=True)\\nscale = transform_to(Normal.arg_constraints['scale'])(unconstrained)\\nloss = -Normal(loc, scale).log_prob(data).sum()\\n\",\n","   'Id': 215,\n","   'Question': 'How to use PyTorch provides two globalConstraintRegistryobjects that linkConstraintobjects toTransformobjects. These objects both\\ninput constraints and return transforms, but they have different guarantees on\\nbijectivity.Thetransform_to()registry is useful for performing unconstrained\\noptimization on constrained parameters of probability distributions, which are\\nindicated by each distribution’s.arg_constraintsdict. These transforms often\\noverparameterize a space in order to avoid rotation; they are thus more\\nsuitable for coordinate-wise optimization algorithms like Adam:, give an example?',\n","   'context': ' Thetransform_to()registry is useful for performing unconstrained\\noptimization on constrained parameters of probability distributions, which are\\nindicated by each distribution’s.arg_constraintsdict. These transforms often\\noverparameterize a space in order to avoid rotation; they are thus more\\nsuitable for coordinate-wise optimization algorithms like Adam:',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': 'dist = Exponential(rate)\\nunconstrained = torch.zeros(100, requires_grad=True)\\nsample = biject_to(dist.support)(unconstrained)\\npotential_energy = -dist.log_prob(sample).sum()\\n',\n","   'Id': 216,\n","   'Question': 'How to use Thetransform_to()registry is useful for performing unconstrained\\noptimization on constrained parameters of probability distributions, which are\\nindicated by each distribution’s.arg_constraintsdict. These transforms often\\noverparameterize a space in order to avoid rotation; they are thus more\\nsuitable for coordinate-wise optimization algorithms like Adam:Thebiject_to()registry is useful for Hamiltonian Monte Carlo, where\\nsamples from a probability distribution with constrained.supportare\\npropagated in an unconstrained space, and algorithms are typically rotation\\ninvariant.:, give an example?',\n","   'context': ' Thetransform_to()registry is useful for performing unconstrained\\noptimization on constrained parameters of probability distributions, which are\\nindicated by each distribution’s.arg_constraintsdict. These transforms often\\noverparameterize a space in order to avoid rotation; they are thus more\\nsuitable for coordinate-wise optimization algorithms like Adam:Thebiject_to()registry is useful for Hamiltonian Monte Carlo, where\\nsamples from a probability distribution with constrained.supportare\\npropagated in an unconstrained space, and algorithms are typically rotation\\ninvariant.:',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': 'transform_to.register(my_constraint, my_transform)\\n',\n","   'Id': 217,\n","   'Question': 'How to use Thebiject_to()registry is useful for Hamiltonian Monte Carlo, where\\nsamples from a probability distribution with constrained.supportare\\npropagated in an unconstrained space, and algorithms are typically rotation\\ninvariant.:Thebiject_toandtransform_toobjects can be extended by user-defined\\nconstraints and transforms using their.register()method either as a\\nfunction on singleton constraints:, give an example?',\n","   'context': ' Thebiject_to()registry is useful for Hamiltonian Monte Carlo, where\\nsamples from a probability distribution with constrained.supportare\\npropagated in an unconstrained space, and algorithms are typically rotation\\ninvariant.:Thebiject_toandtransform_toobjects can be extended by user-defined\\nconstraints and transforms using their.register()method either as a\\nfunction on singleton constraints:',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '@transform_to.register(MyConstraintClass)\\ndef my_factory(constraint):\\n    assert isinstance(constraint, MyConstraintClass)\\n    return MyTransform(constraint.param1, constraint.param2)\\n',\n","   'Id': 218,\n","   'Question': 'How to use Thebiject_toandtransform_toobjects can be extended by user-defined\\nconstraints and transforms using their.register()method either as a\\nfunction on singleton constraints:or as a decorator on parameterized constraints:, give an example?',\n","   'context': ' Thebiject_toandtransform_toobjects can be extended by user-defined\\nconstraints and transforms using their.register()method either as a\\nfunction on singleton constraints:or as a decorator on parameterized constraints:',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '@my_registry.register(MyConstraintClass)\\ndef construct_transform(constraint):\\n    assert isinstance(constraint, MyConstraint)\\n    return MyTransform(constraint.arg_constraints)\\n',\n","   'Id': 219,\n","   'Question': 'How to use torch.distributions.constraint_registry.ConstraintRegistry.register, give an example?',\n","   'context': ' Registers aConstraintsubclass in this registry. Usage:',\n","   'source': 'https://pytorch.org/docs/stable/distributions.html'},\n","  {'Answer': '>>> soboleng = torch.quasirandom.SobolEngine(dimension=5)\\n>>> soboleng.draw(3)\\ntensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\\n        [0.7500, 0.2500, 0.7500, 0.2500, 0.7500],\\n        [0.2500, 0.7500, 0.2500, 0.7500, 0.2500]])\\n',\n","   'Id': 220,\n","   'Question': 'How to use torch.quasirandom.SobolEngine, give an example?',\n","   'context': ' References',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.quasirandom.SobolEngine.html#torch.quasirandom.SobolEngine'},\n","  {'Answer': '>>> x = torch.tensor([1, 1, 2, 2, 3, 1, 1, 2])\\n>>> output = torch.unique_consecutive(x)\\n>>> output\\ntensor([1, 2, 3, 1, 2])\\n\\n>>> output, inverse_indices = torch.unique_consecutive(x, return_inverse=True)\\n>>> output\\ntensor([1, 2, 3, 1, 2])\\n>>> inverse_indices\\ntensor([0, 0, 1, 1, 2, 3, 3, 4])\\n\\n>>> output, counts = torch.unique_consecutive(x, return_counts=True)\\n>>> output\\ntensor([1, 2, 3, 1, 2])\\n>>> counts\\ntensor([2, 2, 1, 2, 1])\\n',\n","   'Id': 221,\n","   'Question': 'How to use torch.unique_consecutive, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.unique_consecutive.html#torch.unique_consecutive'},\n","  {'Answer': '>>> a = torch.tensor([0.7, -1.2, 0., 2.3])\\n>>> torch.signbit(a)\\ntensor([ False, True,  False,  False])\\n',\n","   'Id': 222,\n","   'Question': 'How to use torch.signbit, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.signbit.html#torch.signbit'},\n","  {'Answer': \">>> torch.isinf(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))\\ntensor([False,  True,  False,  True,  False])\\n\",\n","   'Id': 223,\n","   'Question': 'How to use torch.isinf, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.isinf.html#torch.isinf'},\n","  {'Answer': '>>> a = torch.randn(5)\\n>>> a\\ntensor([-1.0090, -0.9923,  1.0249, -0.5372,  0.2492])\\n>>> torch.log1p(a)\\ntensor([    nan, -4.8653,  0.7055, -0.7705,  0.2225])\\n',\n","   'Id': 224,\n","   'Question': 'How to use torch.log1p, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.log1p.html#torch.log1p'},\n","  {'Answer': '>>> torch.randperm(4)\\ntensor([2, 1, 0, 3])\\n',\n","   'Id': 225,\n","   'Question': 'How to use torch.randperm, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.randperm.html#torch.randperm'},\n","  {'Answer': '>>> a = torch.tensor([1, 3, 2])\\n>>> torch.diff(a)\\ntensor([ 2, -1])\\n>>> b = torch.tensor([4, 5])\\n>>> torch.diff(a, append=b)\\ntensor([ 2, -1,  2,  1])\\n>>> c = torch.tensor([[1, 2, 3], [3, 4, 5]])\\n>>> torch.diff(c, dim=0)\\ntensor([[2, 2, 2]])\\n>>> torch.diff(c, dim=1)\\ntensor([[1, 1],\\n        [1, 1]])\\n',\n","   'Id': 226,\n","   'Question': 'How to use torch.diff, give an example?',\n","   'context': ' The first-order differences are given byout[i] = input[i + 1] - input[i]. Higher-order\\ndifferences are calculated by usingtorch.diff()recursively.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.diff.html#torch.diff'},\n","  {'Answer': '>>> a = torch.empty(3, 3).uniform_(0, 1)  # generate a uniform random matrix with range [0, 1]\\n>>> a\\ntensor([[ 0.1737,  0.0950,  0.3609],\\n        [ 0.7148,  0.0289,  0.2676],\\n        [ 0.9456,  0.8937,  0.7202]])\\n>>> torch.bernoulli(a)\\ntensor([[ 1.,  0.,  0.],\\n        [ 0.,  0.,  0.],\\n        [ 1.,  1.,  1.]])\\n\\n>>> a = torch.ones(3, 3) # probability of drawing \"1\" is 1\\n>>> torch.bernoulli(a)\\ntensor([[ 1.,  1.,  1.],\\n        [ 1.,  1.,  1.],\\n        [ 1.,  1.,  1.]])\\n>>> a = torch.zeros(3, 3) # probability of drawing \"1\" is 0\\n>>> torch.bernoulli(a)\\ntensor([[ 0.,  0.,  0.],\\n        [ 0.,  0.,  0.],\\n        [ 0.,  0.,  0.]])\\n',\n","   'Id': 227,\n","   'Question': 'How to use torch.bernoulli, give an example?',\n","   'context': ' outcan have integraldtype, butinputmust have floating\\npointdtype.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.bernoulli.html#torch.bernoulli'},\n","  {'Answer': '>>> a = torch.randn(4, 4)\\n>>> a\\ntensor([[ 0.6451, -0.4866,  0.2987, -1.3312],\\n        [-0.5744,  1.2980,  1.8397, -0.2713],\\n        [ 0.9128,  0.9214, -1.7268, -0.2995],\\n        [ 0.9023,  0.4853,  0.9075, -1.6165]])\\n>>> torch.amin(a, 1)\\ntensor([-1.3312, -0.5744, -1.7268, -1.6165])\\n',\n","   'Id': 228,\n","   'Question': 'How to use torch.amin, give an example?',\n","   'context': ' IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimension(s)dimwhere they are of size 1.\\nOtherwise,dim`saresqueezed(see:func:`torch.squeeze), resulting in\\nthe output tensors having fewer dimensions thaninput.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.amin.html#torch.amin'},\n","  {'Answer': '>>> torch.equal(torch.tensor([1, 2]), torch.tensor([1, 2]))\\nTrue\\n',\n","   'Id': 229,\n","   'Question': 'How to use torch.equal, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.equal.html#torch.equal'},\n","  {'Answer': '>>> torch.logical_not(torch.tensor([True, False]))\\ntensor([False,  True])\\n>>> torch.logical_not(torch.tensor([0, 1, -10], dtype=torch.int8))\\ntensor([ True, False, False])\\n>>> torch.logical_not(torch.tensor([0., 1.5, -10.], dtype=torch.double))\\ntensor([ True, False, False])\\n>>> torch.logical_not(torch.tensor([0., 1., -10.], dtype=torch.double), out=torch.empty(3, dtype=torch.int16))\\ntensor([1, 0, 0], dtype=torch.int16)\\n',\n","   'Id': 230,\n","   'Question': 'How to use torch.logical_not, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.logical_not.html#torch.logical_not'},\n","  {'Answer': '>>> x = torch.zeros(1, requires_grad=True)\\n>>> with torch.no_grad():\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> is_train = False\\n>>> with torch.set_grad_enabled(is_train):\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> torch.set_grad_enabled(True)  # this can also be used as a function\\n>>> y = x * 2\\n>>> y.requires_grad\\nTrue\\n\\n>>> torch.set_grad_enabled(False)\\n>>> y = x * 2\\n>>> y.requires_grad\\nFalse\\n',\n","   'Id': 231,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/torch.html#spectral-ops'},\n","  {'Answer': '>>> a = torch.randn(4)\\n>>> a\\ntensor([ 0.3348, -0.5889,  0.2005, -0.1584])\\n>>> torch.acos(a)\\ntensor([ 1.2294,  2.2004,  1.3690,  1.7298])\\n',\n","   'Id': 232,\n","   'Question': 'How to use torch.acos, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.acos.html#torch.acos'},\n","  {'Answer': '>>> x = torch.zeros(1, requires_grad=True)\\n>>> with torch.no_grad():\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> is_train = False\\n>>> with torch.set_grad_enabled(is_train):\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> torch.set_grad_enabled(True)  # this can also be used as a function\\n>>> y = x * 2\\n>>> y.requires_grad\\nTrue\\n\\n>>> torch.set_grad_enabled(False)\\n>>> y = x * 2\\n>>> y.requires_grad\\nFalse\\n',\n","   'Id': 233,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/torch.html#parallelism'},\n","  {'Answer': '>>> torch.abs(torch.tensor([-1, -2, 3]))\\ntensor([ 1,  2,  3])\\n',\n","   'Id': 234,\n","   'Question': 'How to use torch.abs, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.abs.html#torch.abs'},\n","  {'Answer': '>>> a = numpy.array([1, 2, 3])\\n>>> t = torch.from_numpy(a)\\n>>> t\\ntensor([ 1,  2,  3])\\n>>> t[0] = -1\\n>>> a\\narray([-1,  2,  3])\\n',\n","   'Id': 235,\n","   'Question': 'How to use torch.from_numpy, give an example?',\n","   'context': ' It currently acceptsndarraywith dtypes ofnumpy.float64,numpy.float32,numpy.float16,numpy.complex64,numpy.complex128,numpy.int64,numpy.int32,numpy.int16,numpy.int8,numpy.uint8,\\nandnumpy.bool.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.from_numpy.html#torch.from_numpy'},\n","  {'Answer': '>>> t = torch.tensor([[[1, 2],\\n...                    [3, 4]],\\n...                   [[5, 6],\\n...                    [7, 8]]])\\n>>> torch.ravel(t)\\ntensor([1, 2, 3, 4, 5, 6, 7, 8])\\n',\n","   'Id': 236,\n","   'Question': 'How to use torch.ravel, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.ravel.html#torch.ravel'},\n","  {'Answer': '>>> a = torch.randn(4)\\n>>> a\\ntensor([ 0.2252, -0.2948,  1.0267, -1.1566])\\n>>> torch.sinc(a)\\ntensor([ 0.9186,  0.8631, -0.0259, -0.1300])\\n',\n","   'Id': 237,\n","   'Question': 'How to use torch.sinc, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.sinc.html#torch.sinc'},\n","  {'Answer': '                        | n=1 | n=4 | ...\\n                        ------------- ...\\nReLU(x + 1): (float)    | ... | ... | ...\\nReLU(x + 1): (int)      | ... | ... | ...\\n',\n","   'Id': 238,\n","   'Question': 'How to use String to distinguish measurements with identical label and\\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\\nbased on the input size  to create a table of the form:, give an example?',\n","   'context': ' String to distinguish measurements with identical label and\\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\\nbased on the input size  to create a table of the form:',\n","   'source': 'https://pytorch.org/docs/stable/benchmark_utils.html'},\n","  {'Answer': '`setup`\\n\\ntotal_time = 0\\nwhile total_time < min_run_time\\n    start = timer()\\n    for _ in range(block_size):\\n        `stmt`\\n    total_time += (timer() - start)\\n',\n","   'Id': 239,\n","   'Question': 'How to use torch.utils.benchmark.Timer.blocked_autorange, give an example?',\n","   'context': ' At a high level, blocked_autorange executes the following pseudo-code:',\n","   'source': 'https://pytorch.org/docs/stable/benchmark_utils.html'},\n","  {'Answer': '23234231 /tmp/first_build_dir/thing.c:foo(...)\\n 9823794 /tmp/first_build_dir/thing.c:bar(...)\\n  ...\\n   53453 .../aten/src/Aten/...:function_that_actually_changed(...)\\n  ...\\n -9823794 /tmp/second_build_dir/thing.c:bar(...)\\n-23234231 /tmp/second_build_dir/thing.c:foo(...)\\n',\n","   'Id': 240,\n","   'Question': 'How to use torch.utils.benchmark.CallgrindStats.as_standardized, give an example?',\n","   'context': ' When comparing two different sets of instruction counts, on stumbling\\nblock can be path prefixes. Callgrind includes the full filepath\\nwhen reporting a function (as it should). However, this can cause\\nissues when diffing profiles. If a key component such as Python\\nor PyTorch was built in separate locations in the two profiles, which\\ncan result in something resembling:',\n","   'source': 'https://pytorch.org/docs/stable/benchmark_utils.html'},\n","  {'Answer': \">>> # Save to file\\n>>> x = torch.tensor([0, 1, 2, 3, 4])\\n>>> torch.save(x, 'tensor.pt')\\n>>> # Save to io.BytesIO buffer\\n>>> buffer = io.BytesIO()\\n>>> torch.save(x, buffer)\\n\",\n","   'Id': 241,\n","   'Question': 'How to use torch.save, give an example?',\n","   'context': ' See also:Saving and loading tensors',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.save.html#torch.save'},\n","  {'Answer': '>>> x = torch.tensor([1], requires_grad=True)\\n>>> is_train = False\\n>>> with torch.set_grad_enabled(is_train):\\n...   y = x * 2\\n>>> y.requires_grad\\nFalse\\n>>> torch.set_grad_enabled(True)\\n>>> y = x * 2\\n>>> y.requires_grad\\nTrue\\n>>> torch.set_grad_enabled(False)\\n>>> y = x * 2\\n>>> y.requires_grad\\nFalse\\n',\n","   'Id': 242,\n","   'Question': 'How to use torch.set_grad_enabled, give an example?',\n","   'context': ' This context manager is thread local; it will not affect computation\\nin other threads.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.set_grad_enabled.html#torch.set_grad_enabled'},\n","  {'Answer': '>>> a = [1, 2, 3]\\n>>> list(itertools.combinations(a, r=2))\\n[(1, 2), (1, 3), (2, 3)]\\n>>> list(itertools.combinations(a, r=3))\\n[(1, 2, 3)]\\n>>> list(itertools.combinations_with_replacement(a, r=2))\\n[(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)]\\n>>> tensor_a = torch.tensor(a)\\n>>> torch.combinations(tensor_a)\\ntensor([[1, 2],\\n        [1, 3],\\n        [2, 3]])\\n>>> torch.combinations(tensor_a, r=3)\\ntensor([[1, 2, 3]])\\n>>> torch.combinations(tensor_a, with_replacement=True)\\ntensor([[1, 1],\\n        [1, 2],\\n        [1, 3],\\n        [2, 2],\\n        [2, 3],\\n        [3, 3]])\\n',\n","   'Id': 243,\n","   'Question': 'How to use torch.combinations, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.combinations.html#torch.combinations'},\n","  {'Answer': '>>> mat = torch.randn(2, 3)\\n>>> vec = torch.randn(3)\\n>>> torch.mv(mat, vec)\\ntensor([ 1.0404, -0.6361])\\n',\n","   'Id': 244,\n","   'Question': 'How to use torch.mv, give an example?',\n","   'context': ' Ifinputis a(n×m)(n \\\\times m)(n×m)tensor,vecis a 1-D tensor of\\nsizemmm,outwill be 1-D of sizennn.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.mv.html#torch.mv'},\n","  {'Answer': '>>> a = torch.randn(4, 4)\\n>>> a\\ntensor([[ 0.8177,  1.4878, -0.2491,  0.9130],\\n        [-0.7158,  1.1775,  2.0992,  0.4817],\\n        [-0.0053,  0.0164, -1.3738, -0.0507],\\n        [ 1.9700,  1.1106, -1.0318, -1.0816]])\\n>>> torch.amax(a, 1)\\ntensor([1.4878, 2.0992, 0.0164, 1.9700])\\n',\n","   'Id': 245,\n","   'Question': 'How to use torch.amax, give an example?',\n","   'context': ' Ifkeepdimis``True`, the output tensors are of the same size\\nasinputexcept in the dimension(s)dimwhere they are of size 1.\\nOtherwise,dim`saresqueezed(see:func:`torch.squeeze), resulting\\nin the output tensors having fewer dimension thaninput.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.amax.html#torch.amax'},\n","  {'Answer': '>>> a = torch.randn(1, 2, 3, 4, 5)\\n>>> torch.numel(a)\\n120\\n>>> a = torch.zeros(4,4)\\n>>> torch.numel(a)\\n16\\n',\n","   'Id': 246,\n","   'Question': 'How to use torch.numel, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.numel.html#torch.numel'},\n","  {'Answer': '>>> torch.logspace(start=-10, end=10, steps=5)\\ntensor([ 1.0000e-10,  1.0000e-05,  1.0000e+00,  1.0000e+05,  1.0000e+10])\\n>>> torch.logspace(start=0.1, end=1.0, steps=5)\\ntensor([  1.2589,   2.1135,   3.5481,   5.9566,  10.0000])\\n>>> torch.logspace(start=0.1, end=1.0, steps=1)\\ntensor([1.2589])\\n>>> torch.logspace(start=2, end=2, steps=1, base=2)\\ntensor([4.0])\\n',\n","   'Id': 247,\n","   'Question': 'How to use torch.logspace, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.logspace.html#torch.logspace'},\n","  {'Answer': '>>> a = torch.randn(4).uniform_(1, 2)\\n>>> a\\ntensor([ 1.3192, 1.9915, 1.9674, 1.7151 ])\\n>>> torch.acosh(a)\\ntensor([ 0.7791, 1.3120, 1.2979, 1.1341 ])\\n',\n","   'Id': 248,\n","   'Question': 'How to use torch.acosh, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.acosh.html#torch.acosh'},\n","  {'Answer': '$ unzip my_package.pt && tree my_package\\nmy_package\\n├── .data\\n│   ├── 94304870911616.storage\\n│   ├── 94304900784016.storage\\n│   ├── extern_modules\\n│   └── version\\n├── models\\n│   └── model_1.pkl\\n└── torchvision\\n    └── models\\n        ├── resnet.py\\n        └── utils.py\\n~ cd my_package && cat torchvision/models/resnet.py\\n...\\n',\n","   'Id': 249,\n","   'Question': 'How to use The container format for atorch.packageis ZIP, so any tools that work with standard ZIP files should\\nwork for exploring the contents. Some common ways to interact with ZIP files:, give an example?',\n","   'context': ' The container format for atorch.packageis ZIP, so any tools that work with standard ZIP files should\\nwork for exploring the contents. Some common ways to interact with ZIP files:',\n","   'source': 'https://pytorch.org/docs/stable/package.html'},\n","  {'Answer': 'from zipfile import ZipFile\\nwith ZipFile(\"my_package.pt\") as myzip:\\n    file_bytes = myzip.read(\"torchvision/models/resnet.py\")\\n    # edit file_bytes in some way\\n    myzip.writestr(\"torchvision/models/resnet.py\", new_file_bytes)\\n',\n","   'Id': 250,\n","   'Question': 'How  The container format for atorch.packageis ZIP, so any tools that work with standard ZIP files should\\nwork for exploring the contents. Some common ways to interact with ZIP files:, give an example?',\n","   'context': ' The container format for atorch.packageis ZIP, so any tools that work with standard ZIP files should\\nwork for exploring the contents. Some common ways to interact with ZIP files:',\n","   'source': 'https://pytorch.org/docs/stable/package.html'},\n","  {'Answer': '# add this to your .vimrc to treat `*.pt` files as zip files\\nau BufReadCmd *.pt call zip#Browse(expand(\"<amatch>\"))\\n\\n~ vi my_package.pt\\n',\n","   'Id': 251,\n","   'Question': 'How  The container format for atorch.packageis ZIP, so any tools that work with standard ZIP files should\\nwork for exploring the contents. Some common ways to interact with ZIP files:, give an example?',\n","   'context': ' The container format for atorch.packageis ZIP, so any tools that work with standard ZIP files should\\nwork for exploring the contents. Some common ways to interact with ZIP files:',\n","   'source': 'https://pytorch.org/docs/stable/package.html'},\n","  {'Answer': 'with PackageExporter(\\'my_package.pt\\', verbose=False) as pe:\\n    pe.save_pickle(\\'models\\', \\'model_1.pkl\\', mod)\\n    # can limit printed items with include/exclude args\\n    print(pe.file_structure(include=[\"**/utils.py\", \"**/*.pkl\"], exclude=\"**/*.storages\"))\\n\\nimporter = PackageImporter(\\'my_package.pt\\')\\nprint(importer.file_structure()) # will print out all files\\n',\n","   'Id': 252,\n","   'Question': 'How to use PackageImporterandPackageExporterprovide afile_structure()method, which will return a printable\\nand queryableFolderobject. TheFolderobject is a simple directory structure that you can use to explore the\\ncurrent contents of atorch.package.TheFolderobject itself is directly printable and will print out a file tree representation. To filter what is returned,\\nuse the glob-styleincludeandexcludefiltering arguments., give an example?',\n","   'context': ' TheFolderobject itself is directly printable and will print out a file tree representation. To filter what is returned,\\nuse the glob-styleincludeandexcludefiltering arguments.',\n","   'source': 'https://pytorch.org/docs/stable/package.html'},\n","  {'Answer': '# filtered with glob pattern:\\n#    include=[\"**/utils.py\", \"**/*.pkl\"], exclude=\"**/*.storages\"\\n─── my_package.pt\\n    ├── models\\n    │   └── model_1.pkl\\n    └── torchvision\\n        └── models\\n            └── utils.py\\n\\n# all files\\n─── my_package.pt\\n    ├── .data\\n    │   ├── 94304870911616.storage\\n    │   ├── 94304900784016.storage\\n    │   ├── extern_modules\\n    │   └── version\\n    ├── models\\n    │   └── model_1.pkl\\n    └── torchvision\\n        └── models\\n            ├── resnet.py\\n            └── utils.py\\n',\n","   'Id': 253,\n","   'Question': 'How to use TheFolderobject itself is directly printable and will print out a file tree representation. To filter what is returned,\\nuse the glob-styleincludeandexcludefiltering arguments.Output:, give an example?',\n","   'context': ' TheFolderobject itself is directly printable and will print out a file tree representation. To filter what is returned,\\nuse the glob-styleincludeandexcludefiltering arguments.Output:',\n","   'source': 'https://pytorch.org/docs/stable/package.html'},\n","  {'Answer': 'exporter_file_structure = exporter.file_structure()\\nfound: bool = exporter_file_structure.has_file(\"package_a/subpackage.py\")\\n',\n","   'Id': 254,\n","   'Question': 'How to use Output:You can also queryFolderobjects with thehas_file()method., give an example?',\n","   'context': ' Output:You can also queryFolderobjects with thehas_file()method.',\n","   'source': 'https://pytorch.org/docs/stable/package.html'},\n","  {'Answer': 'with torch.PackageExporter(\"package.pt\") as exporter:\\n    # Pickles the object and saves to `my_resources/tens.pkl` in the archive.\\n    exporter.save_pickle(\"my_resources\", \"tensor.pkl\", torch.randn(4))\\n    exporter.save_text(\"config_stuff\", \"words.txt\", \"a sample string\")\\n    exporter.save_binary(\"raw_data\", \"binary\", my_bytes)\\n',\n","   'Id': 255,\n","   'Question': 'How to use PackageExporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\\nPython objects, text, and binary data to a package., give an example?',\n","   'context': ' PackageExporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\\nPython objects, text, and binary data to a package.',\n","   'source': 'https://pytorch.org/docs/stable/package.html'},\n","  {'Answer': 'importer = torch.PackageImporter(\"package.pt\")\\nmy_tensor = importer.load_pickle(\"my_resources\", \"tensor.pkl\")\\ntext = importer.load_text(\"config_stuff\", \"words.txt\")\\nbinary = importer.load_binary(\"raw_data\", \"binary\")\\n',\n","   'Id': 256,\n","   'Question': 'How to use PackageExporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\\nPython objects, text, and binary data to a package.PackageImporterexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\\nPython objects, text and binary data from a package., give an example?',\n","   'context': ' PackageImporterexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\\nPython objects, text and binary data from a package.',\n","   'source': 'https://pytorch.org/docs/stable/package.html'},\n","  {'Answer': '# foo.py [Example of customizing how class Foo is packaged]\\nfrom torch.package import PackageExporter, PackageImporter\\nimport time\\n\\n\\nclass Foo:\\n    def __init__(self, my_string: str):\\n        super().__init__()\\n        self.my_string = my_string\\n        self.time_imported = 0\\n        self.time_exported = 0\\n\\n    def __reduce_package__(self, exporter: PackageExporter):\\n        \"\"\"\\n        Called by ``torch.package.PackageExporter``\\'s Pickler\\'s ``persistent_id`` when\\n        saving an instance of this object. This method should do the work to save this\\n        object inside of the ``torch.package`` archive.\\n\\n        Returns function w/ arguments to load the object from a\\n        ``torch.package.PackageImporter``\\'s Pickler\\'s ``persistent_load`` function.\\n        \"\"\"\\n\\n        # use this pattern to ensure no naming conflicts with normal dependencies,\\n        # anything saved under this module name shouldn\\'t conflict with other\\n        # items in the package\\n        generated_module_name = f\"foo-generated._{exporter.get_unique_id()}\"\\n        exporter.save_text(\\n            generated_module_name,\\n            \"foo.txt\",\\n            self.my_string + \", with exporter modification!\",\\n        )\\n        time_exported = time.clock_gettime(1)\\n\\n        # returns de-packaging function w/ arguments to invoke with\\n        return (unpackage_foo, (generated_module_name, time_exported,))\\n\\n\\ndef unpackage_foo(\\n    importer: PackageImporter, generated_module_name: str, time_exported: float\\n) -> Foo:\\n    \"\"\"\\n    Called by ``torch.package.PackageImporter``\\'s Pickler\\'s ``persistent_load`` function\\n    when depickling a Foo object.\\n    Performs work of loading and returning a Foo instance from a ``torch.package`` archive.\\n    \"\"\"\\n    time_imported = time.clock_gettime(1)\\n    foo = Foo(importer.load_text(generated_module_name, \"foo.txt\"))\\n    foo.time_imported = time_imported\\n    foo.time_exported = time_exported\\n    return foo\\n',\n","   'Id': 257,\n","   'Question': 'How to use torch.packageallows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\\nPython’s normal pickling process.Steps:, give an example?',\n","   'context': ' Steps:',\n","   'source': 'https://pytorch.org/docs/stable/package.html'},\n","  {'Answer': '# example of saving instances of class Foo\\n\\nimport torch\\nfrom torch.package import PackageImporter, PackageExporter\\nimport foo\\n\\nfoo_1 = foo.Foo(\"foo_1 initial string\")\\nfoo_2 = foo.Foo(\"foo_2 initial string\")\\nwith PackageExporter(\\'foo_package.pt\\', verbose=False) as pe:\\n    # save as normal, no extra work necessary\\n    pe.save_pickle(\\'foo_collection\\', \\'foo1.pkl\\', foo_1)\\n    pe.save_pickle(\\'foo_collection\\', \\'foo2.pkl\\', foo_2)\\n    print(pe.file_structure())\\n\\npi = PackageImporter(\\'foo_package.pt\\')\\nimported_foo = pi.load_pickle(\\'foo_collection\\', \\'foo1.pkl\\')\\nprint(f\"foo_1 string: \\'{imported_foo.my_string}\\'\")\\nprint(f\"foo_1 export time: {imported_foo.time_exported}\")\\nprint(f\"foo_1 import time: {imported_foo.time_imported}\")\\n',\n","   'Id': 258,\n","   'Question': 'How  Steps:, give an example?',\n","   'context': ' Steps:',\n","   'source': 'https://pytorch.org/docs/stable/package.html'},\n","  {'Answer': \"# output of running above script\\n─── foo_package\\n    ├── foo-generated\\n    │   ├── _0\\n    │   │   └── foo.txt\\n    │   └── _1\\n    │       └── foo.txt\\n    ├── foo_collection\\n    │   ├── foo1.pkl\\n    │   └── foo2.pkl\\n    └── foo.py\\n\\nfoo_1 string: 'foo_1 initial string, with reduction modification!'\\nfoo_1 export time: 9857706.650140837\\nfoo_1 import time: 9857706.652698385\\n\",\n","   'Id': 259,\n","   'Question': 'How  Steps:, give an example?',\n","   'context': ' Steps:',\n","   'source': 'https://pytorch.org/docs/stable/package.html'},\n","  {'Answer': '# In foo/bar.py:\\n\\nif \"__torch_package__\" in dir():  # true if the code is being loaded from a package\\n    def is_in_package():\\n        return True\\n\\n    UserException = Exception\\nelse:\\n    def is_in_package():\\n        return False\\n\\n    UserException = UnpackageableException\\n',\n","   'Id': 260,\n","   'Question': 'How to use APackageImporterwill add the attribute__torch_package__to every module that it initializes. Your code can check for the\\npresence of this attribute to determine whether it is executing in a packaged context or not., give an example?',\n","   'context': ' APackageImporterwill add the attribute__torch_package__to every module that it initializes. Your code can check for the\\npresence of this attribute to determine whether it is executing in a packaged context or not.',\n","   'source': 'https://pytorch.org/docs/stable/package.html'},\n","  {'Answer': 'from foo.bar import is_in_package\\n\\nprint(is_in_package())  # False\\n\\nloaded_module = PackageImporter(my_pacakge).import_module(\"foo.bar\")\\nloaded_module.is_in_package()  # True\\n',\n","   'Id': 261,\n","   'Question': 'How to use APackageImporterwill add the attribute__torch_package__to every module that it initializes. Your code can check for the\\npresence of this attribute to determine whether it is executing in a packaged context or not.Now, the code will behave differently depending on whether it’s imported normally through your Python environment or imported from atorch.package., give an example?',\n","   'context': ' Now, the code will behave differently depending on whether it’s imported normally through your Python environment or imported from atorch.package.',\n","   'source': 'https://pytorch.org/docs/stable/package.html'},\n","  {'Answer': 'with PackageExporter(f) as exporter:\\n    # Save the my_module.foo available in your current Python environment.\\n    exporter.save_module(\"my_module.foo\")\\n\\n    # This saves the provided string to my_module/foo.py in the package archive.\\n    # It will override the my_module.foo that was previously saved.\\n    exporter.save_source_string(\"my_module.foo\", textwrap.dedent(\\n        \"\"\"\\\\\\n        def my_function():\\n            print(\\'hello world\\')\\n        \"\"\"\\n    ))\\n\\n    # If you want to treat my_module.bar as a package\\n    # (e.g. save to `my_module/bar/__init__.py` instead of `my_module/bar.py)\\n    # pass is_package=True,\\n    exporter.save_source_string(\"my_module.bar\",\\n                                \"def foo(): print(\\'hello\\')\\\\n\",\\n                                is_package=True)\\n\\nimporter = PackageImporter(f)\\nimporter.import_module(\"my_module.foo\").my_function()  # prints \\'hello world\\'\\n',\n","   'Id': 262,\n","   'Question': 'How to use PackageExporteroffers asave_source_string()method that allows one to save arbitrary Python source code to a module of your choosing., give an example?',\n","   'context': ' PackageExporteroffers asave_source_string()method that allows one to save arbitrary Python source code to a module of your choosing.',\n","   'source': 'https://pytorch.org/docs/stable/package.html'},\n","  {'Answer': 'with PackageExporter(f) as exporter:\\n    # saves text to one/a.txt in the archive\\n    exporter.save_text(\"my_resource\", \"a.txt\", \"hello world!\")\\n    # saves the tensor to my_pickle/obj.pkl\\n    exporter.save_pickle(\"my_pickle\", \"obj.pkl\", torch.ones(2, 2))\\n\\n    # see below for module contents\\n    exporter.save_module(\"foo\")\\n    exporter.save_module(\"bar\")\\n',\n","   'Id': 263,\n","   'Question': 'How to use PackageImporterimplements theimportlib.resourcesAPI for accessing resources from inside a package., give an example?',\n","   'context': ' PackageImporterimplements theimportlib.resourcesAPI for accessing resources from inside a package.',\n","   'source': 'https://pytorch.org/docs/stable/package.html'},\n","  {'Answer': '# foo.py:\\nimport importlib.resources\\nimport my_resource\\n\\n# returns \"hello world!\"\\ndef get_my_resource():\\n    return importlib.resources.read_text(my_resource, \"a.txt\")\\n',\n","   'Id': 264,\n","   'Question': 'How to use PackageImporterimplements theimportlib.resourcesAPI for accessing resources from inside a package.Theimportlib.resourcesAPI allows access to resources from within packaged code., give an example?',\n","   'context': ' Theimportlib.resourcesAPI allows access to resources from within packaged code.',\n","   'source': 'https://pytorch.org/docs/stable/package.html'},\n","  {'Answer': '# bar.py:\\nimport torch_package_importer # this is the PackageImporter that imported this module.\\n\\n# Prints \"hello world!\", equivalient to importlib.resources.read_text\\ndef get_my_resource():\\n    return torch_package_importer.load_text(\"my_resource\", \"a.txt\")\\n\\n# You also do things that the importlib.resources API does not support, like loading\\n# a pickled object from the package.\\ndef get_my_pickle():\\n    return torch_package_importer.load_pickle(\"my_pickle\", \"obj.pkl\")\\n',\n","   'Id': 265,\n","   'Question': 'How to use Theimportlib.resourcesAPI allows access to resources from within packaged code.Usingimportlib.resourcesis the recommended way to access package contents from within packaged code, since it complies\\nwith the Python standard. However, it is also possible to access the parentPackageImporterinstance itself from within\\npackaged code., give an example?',\n","   'context': ' Theimportlib.resourcesAPI allows access to resources from within packaged code.Usingimportlib.resourcesis the recommended way to access package contents from within packaged code, since it complies\\nwith the Python standard. However, it is also possible to access the parentPackageImporterinstance itself from within\\npackaged code.',\n","   'source': 'https://pytorch.org/docs/stable/package.html'},\n","  {'Answer': \"importer = PackageImporter(f)\\nmod = importer.import_module('foo')\\nobj = importer.load_pickle('model', 'model.pkl')\\ntxt = importer.load_text('text', 'my_test.txt')\\n\\nassert is_from_package(mod)\\nassert is_from_package(obj)\\nassert not is_from_package(txt) # str is from stdlib, so this will return False\\n\",\n","   'Id': 266,\n","   'Question': 'How to use To tell if an object’s code is from atorch.package, use thetorch.package.is_from_package()function.\\nNote: if an object is from a package but its definition is from a module markedexternor fromstdlib,\\nthis check will returnFalse., give an example?',\n","   'context': ' To tell if an object’s code is from atorch.package, use thetorch.package.is_from_package()function.\\nNote: if an object is from a package but its definition is from a module markedexternor fromstdlib,\\nthis check will returnFalse.',\n","   'source': 'https://pytorch.org/docs/stable/package.html'},\n","  {'Answer': 'importer = PackageImporter(f)\\nobj = importer.load_pickle(\"model\", \"model.pkl\")\\n\\n# re-export obj in a new package\\nwith PackageExporter(f2, importer=(importer, sys_importer)) as exporter:\\n    exporter.save_pickle(\"model\", \"model.pkl\", obj)\\n',\n","   'Id': 267,\n","   'Question': 'How to use To re-export an object that was previously imported by aPackageImporter, you must make the newPackageExporteraware of the originalPackageImporterso that it can find source code for your object’s dependencies., give an example?',\n","   'context': ' To re-export an object that was previously imported by aPackageImporter, you must make the newPackageExporteraware of the originalPackageImporterso that it can find source code for your object’s dependencies.',\n","   'source': 'https://pytorch.org/docs/stable/package.html'},\n","  {'Answer': '# save TorchScript just like any other object\\nwith PackageExporter(file_name, verbose=True) as e:\\n    e.save_pickle(\"res\", \"script_model.pkl\", scripted_model)\\n    e.save_pickle(\"res\", \"mixed_model.pkl\", python_model_with_scripted_submodule)\\n# load as normal\\nimporter = PackageImporter(file_name)\\nloaded_script = importer.load_pickle(\"res\", \"script_model.pkl\")\\nloaded_mixed = importer.load_pickle(\"res\", \"mixed_model.pkl\"\\n',\n","   'Id': 268,\n","   'Question': 'How to use To package a TorchScript model, use the samesave_pickleandload_pickleAPIs as you would with any other object.\\nSaving TorchScript objects that are attributes or submodules is supported as well with no extra work., give an example?',\n","   'context': ' To package a TorchScript model, use the samesave_pickleandload_pickleAPIs as you would with any other object.\\nSaving TorchScript objects that are attributes or submodules is supported as well with no extra work.',\n","   'source': 'https://pytorch.org/docs/stable/package.html'},\n","  {'Answer': \"resnet\\n├── .data  # All framework-specific data is stored here.\\n│   │      # It's named to avoid conflicts with user-serialized code.\\n│   ├── 94286146172688.storage  # tensor data\\n│   ├── 94286146172784.storage\\n│   ├── extern_modules  # text file with names of extern modules (e.g. 'torch')\\n│   ├── version         # version metadata\\n│   ├── ...\\n├── model  # the pickled model\\n│   └── model.pkl\\n└── torchvision  # all code dependencies are captured as source files\\n    └── models\\n        ├── resnet.py\\n        └── utils.py\\n\",\n","   'Id': 269,\n","   'Question': 'How to use Atorch.packagefile is a ZIP archive which conventionally uses the.ptextension. Inside the ZIP archive, there are two kinds of files:As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like:, give an example?',\n","   'context': ' As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like:',\n","   'source': 'https://pytorch.org/docs/stable/package.html'},\n","  {'Answer': '.data\\n├── 94286146172688.storage\\n├── 94286146172784.storage\\n├── extern_modules\\n├── version\\n├── ...\\n',\n","   'Id': 270,\n","   'Question': 'How to use The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\\n(that is, newer version of PyTorch will always be able to load oldertorch.packages).Currently, the.data/directory contains the following items:, give an example?',\n","   'context': ' Currently, the.data/directory contains the following items:',\n","   'source': 'https://pytorch.org/docs/stable/package.html'},\n","  {'Answer': '<package root>\\n├── model  # the pickled model\\n│   └── model.pkl\\n├── another_package\\n│   ├── __init__.py\\n│   ├── foo.txt         # a resource file , see importlib.resources\\n│   └── ...\\n└── torchvision\\n    └── models\\n        ├── resnet.py   # torchvision.models.resnet\\n        └── utils.py    # torchvision.models.utils\\n',\n","   'Id': 271,\n","   'Question': 'How to use All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\\nplease consultthis essay(it’s slightly out of date, so double-check implementation details\\nwith thePython reference documentation)., give an example?',\n","   'context': ' All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\\nplease consultthis essay(it’s slightly out of date, so double-check implementation details\\nwith thePython reference documentation).',\n","   'source': 'https://pytorch.org/docs/stable/package.html'},\n","  {'Answer': \"GLOBAL 'torchvision.models.resnet Resnet`\\n\",\n","   'Id': 272,\n","   'Question': 'How to use When you issue asave_pickle(obj,...)call,PackageExporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode.In a pickle, an object is saved along with aGLOBALopcode that describes where to find the implementation of the object’s type, like:, give an example?',\n","   'context': ' In a pickle, an object is saved along with aGLOBALopcode that describes where to find the implementation of the object’s type, like:',\n","   'source': 'https://pytorch.org/docs/stable/package.html'},\n","  {'Answer': 'my_exporter.intern(\"torchvision.**\")\\nmy_exporter.extern(\"numpy\")\\n',\n","   'Id': 273,\n","   'Question': 'How to use Note that actions are only defined on entire Python modules. There is no way to package “just” a function or class from module and leave the rest out.\\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\\nmodule, so that’s whattorch.packageuses.Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\\nwith an action using methods onPackageImporter, e.g., give an example?',\n","   'context': ' Note that actions are only defined on entire Python modules. There is no way to package “just” a function or class from module and leave the rest out.\\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\\nmodule, so that’s whattorch.packageuses.Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\\nwith an action using methods onPackageImporter, e.g.',\n","   'source': 'https://pytorch.org/docs/stable/package.html'},\n","  {'Answer': 'exporter.intern([\"torchvision.models.**\", \"torchvision.utils.**\"])\\n',\n","   'Id': 274,\n","   'Question': 'How to use When specifying actions, you can pass multiple patterns, e.g., give an example?',\n","   'context': ' When specifying actions, you can pass multiple patterns, e.g.',\n","   'source': 'https://pytorch.org/docs/stable/package.html'},\n","  {'Answer': 'exporter.mock(\"**\", exclude=[\"torchvision.**\"])\\n',\n","   'Id': 275,\n","   'Question': 'How to use A module will match against this action if it matches any of the patterns.You can also specify patterns to exlcude, e.g., give an example?',\n","   'context': ' A module will match against this action if it matches any of the patterns.You can also specify patterns to exlcude, e.g.',\n","   'source': 'https://pytorch.org/docs/stable/package.html'},\n","  {'Answer': 'from foo import MyClass\\n\\nmy_class_instance = MyClass()\\n\\nwith PackageExporter(f) as exporter:\\n    exporter.save_module(\"foo\")\\n\\nimporter = PackageImporter(f)\\nimported_MyClass = importer.import_module(\"foo\").MyClass\\n\\nassert isinstance(my_class_instance, MyClass)  # works\\nassert isinstance(my_class_instance, imported_MyClass)  # ERROR!\\n',\n","   'Id': 276,\n","   'Question': 'How to use Any class that you import from aPackageImporterwill be a version of the class specific to that importer. For example:, give an example?',\n","   'context': ' Any class that you import from aPackageImporterwill be a version of the class specific to that importer. For example:',\n","   'source': 'https://pytorch.org/docs/stable/package.html'},\n","  {'Answer': 'print(MyClass.__name__)  # prints \"foo.MyClass\"\\nprint(imported_MyClass.__name__)  # prints <torch_package_0>.foo.MyClass\\n',\n","   'Id': 277,\n","   'Question': 'How to use In this example,MyClassandimport_MyClassarenot the same type. In this specific example,MyClassandimport_MyClasshave exactly the\\nsame implementation, so you might thing it’s okay to consider them the same class. But consider the situation whereimport_MyClassis coming from an\\nolder package with an entirely different implementation ofMyClass— in that case, it’s unsafe to consider them the same class.Under the hood, each importer has a prefix that allows it to uniquely identify classes:, give an example?',\n","   'context': ' In this example,MyClassandimport_MyClassarenot the same type. In this specific example,MyClassandimport_MyClasshave exactly the\\nsame implementation, so you might thing it’s okay to consider them the same class. But consider the situation whereimport_MyClassis coming from an\\nolder package with an entirely different implementation ofMyClass— in that case, it’s unsafe to consider them the same class.Under the hood, each importer has a prefix that allows it to uniquely identify classes:',\n","   'source': 'https://pytorch.org/docs/stable/package.html'},\n","  {'Answer': 'with PackageExporter(\"file.zip\") as e:\\n    ...\\n',\n","   'Id': 278,\n","   'Question': 'How to use torch.package.PackageExporter.close, give an example?',\n","   'context': ' Write the package to the filesystem. Any calls afterclose()are now invalid.\\nIt is preferable to use resource guard syntax instead:',\n","   'source': 'https://pytorch.org/docs/stable/package.html'},\n","  {'Answer': 'hook(exporter: PackageExporter, module_name: str) -> None\\n',\n","   'Id': 279,\n","   'Question': 'How to use torch.package.PackageExporter.register_extern_hook, give an example?',\n","   'context': ' The hook will be called each time a module matches against anextern()pattern.\\nIt should have the following signature:',\n","   'source': 'https://pytorch.org/docs/stable/package.html'},\n","  {'Answer': 'hook(exporter: PackageExporter, module_name: str) -> None\\n',\n","   'Id': 280,\n","   'Question': 'How to use torch.package.PackageExporter.register_intern_hook, give an example?',\n","   'context': ' The hook will be called each time a module matches against anintern()pattern.\\nIt should have the following signature:',\n","   'source': 'https://pytorch.org/docs/stable/package.html'},\n","  {'Answer': 'hook(exporter: PackageExporter, module_name: str) -> None\\n',\n","   'Id': 281,\n","   'Question': 'How to use torch.package.PackageExporter.register_mock_hook, give an example?',\n","   'context': ' The hook will be called each time a module matches against amock()pattern.\\nIt should have the following signature:',\n","   'source': 'https://pytorch.org/docs/stable/package.html'},\n","  {'Answer': '<torch_package_0>\\n',\n","   'Id': 282,\n","   'Question': 'How to use torch.package.PackageImporter.id, give an example?',\n","   'context': ' Returns internal identifier that torch.package uses to distinguishPackageImporterinstances.\\nLooks like:',\n","   'source': 'https://pytorch.org/docs/stable/package.html'},\n","  {'Answer': '>>> a = torch.randn(10)\\n>>> a\\ntensor([-0.8286, -0.4890,  0.5155,  0.8443,  0.1865, -0.1752, -2.0595,\\n         0.1850, -1.1571, -0.4243])\\n>>> torch.cumsum(a, dim=0)\\ntensor([-0.8286, -1.3175, -0.8020,  0.0423,  0.2289,  0.0537, -2.0058,\\n        -1.8209, -2.9780, -3.4022])\\n',\n","   'Id': 283,\n","   'Question': 'How to use torch.cumsum, give an example?',\n","   'context': ' For example, ifinputis a vector of size N, the result will also be\\na vector of size N, with elements.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.cumsum.html#torch.cumsum'},\n","  {'Answer': 'matrix_power(torch.linalg.solve(A, B), n) == matrix_power(A, -n)  @ B\\n',\n","   'Id': 284,\n","   'Question': 'How to use torch.linalg.matrix_power, give an example?',\n","   'context': ' Consider usingtorch.linalg.solve()if possible for multiplying a matrix on the left by\\na negative power as, ifn> 0:',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.linalg.matrix_power.html#torch.linalg.matrix_power'},\n","  {'Answer': '>>> a = torch.randn(3, 3)\\n>>> a\\ntensor([[-0.2270,  0.6663, -1.3515],\\n        [-0.9838, -0.4002, -1.9313],\\n        [-0.7886, -0.0450,  0.0528]])\\n>>> torch.linalg.matrix_power(a, 0)\\ntensor([[1., 0., 0.],\\n        [0., 1., 0.],\\n        [0., 0., 1.]])\\n>>> torch.linalg.matrix_power(a, 3)\\ntensor([[ 1.0756,  0.4980,  0.0100],\\n        [-1.6617,  1.4994, -1.9980],\\n        [-0.4509,  0.2731,  0.8001]])\\n>>> torch.linalg.matrix_power(a.expand(2, -1, -1), -2)\\ntensor([[[ 0.2640,  0.4571, -0.5511],\\n        [-1.0163,  0.3491, -1.5292],\\n        [-0.4899,  0.0822,  0.2773]],\\n        [[ 0.2640,  0.4571, -0.5511],\\n        [-1.0163,  0.3491, -1.5292],\\n        [-0.4899,  0.0822,  0.2773]]])\\n',\n","   'Id': 285,\n","   'Question': 'How  Ifn= 0, it returns the identity matrix (or batch) of the same shape\\nasA. Ifnis negative, it returns the inverse of each matrix\\n(if invertible) raised to the power ofabs(n)., give an example?',\n","   'context': ' Ifn= 0, it returns the identity matrix (or batch) of the same shape\\nasA. Ifnis negative, it returns the inverse of each matrix\\n(if invertible) raised to the power ofabs(n).',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.linalg.matrix_power.html#torch.linalg.matrix_power'},\n","  {'Answer': '>>> x = torch.zeros(1, requires_grad=True)\\n>>> with torch.no_grad():\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> is_train = False\\n>>> with torch.set_grad_enabled(is_train):\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> torch.set_grad_enabled(True)  # this can also be used as a function\\n>>> y = x * 2\\n>>> y.requires_grad\\nTrue\\n\\n>>> torch.set_grad_enabled(False)\\n>>> y = x * 2\\n>>> y.requires_grad\\nFalse\\n',\n","   'Id': 286,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/torch.html#quasi-random-sampling'},\n","  {'Answer': 'with torch.profiler.profile(\\n    activities=[\\n        torch.profiler.ProfilerActivity.CPU,\\n        torch.profiler.ProfilerActivity.CUDA,\\n    ]\\n) as p:\\n    code_to_profile()\\nprint(p.key_averages().table(\\n    sort_by=\"self_cuda_time_total\", row_limit=-1))\\n',\n","   'Id': 287,\n","   'Question': 'How to use torch.profiler.profile, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/profiler.html'},\n","  {'Answer': '# Non-default profiler schedule allows user to turn profiler on and off\\n# on different iterations of the training loop;\\n# trace_handler is called every time a new trace becomes available\\ndef trace_handler(prof):\\n    print(prof.key_averages().table(\\n        sort_by=\"self_cuda_time_total\", row_limit=-1))\\n    # prof.export_chrome_trace(\"/tmp/test_trace_\" + str(prof.step_num) + \".json\")\\n\\nwith torch.profiler.profile(\\n    activities=[\\n        torch.profiler.ProfilerActivity.CPU,\\n        torch.profiler.ProfilerActivity.CUDA,\\n    ],\\n\\n    # In this example with wait=1, warmup=1, active=2,\\n    # profiler will skip the first step/iteration,\\n    # start warming up on the second, record\\n    # the third and the forth iterations,\\n    # after which the trace will become available\\n    # and on_trace_ready (when set) is called;\\n    # the cycle repeats starting with the next step\\n\\n    schedule=torch.profiler.schedule(\\n        wait=1,\\n        warmup=1,\\n        active=2),\\n    on_trace_ready=trace_handler\\n    # on_trace_ready=torch.profiler.tensorboard_trace_handler(\\'./log\\')\\n    # used when outputting for tensorboard\\n    ) as p:\\n        for iter in range(N):\\n            code_iteration_to_profile(iter)\\n            # send a signal to the profiler that the next iteration has started\\n            p.step()\\n',\n","   'Id': 288,\n","   'Question': 'How to use Using the profiler’sschedule,on_trace_readyandstepfunctions:, give an example?',\n","   'context': ' Using the profiler’sschedule,on_trace_readyandstepfunctions:',\n","   'source': 'https://pytorch.org/docs/stable/profiler.html'},\n","  {'Answer': '>>> output = torch.unique(torch.tensor([1, 3, 2, 3], dtype=torch.long))\\n>>> output\\ntensor([ 2,  3,  1])\\n\\n>>> output, inverse_indices = torch.unique(\\n...     torch.tensor([1, 3, 2, 3], dtype=torch.long), sorted=True, return_inverse=True)\\n>>> output\\ntensor([ 1,  2,  3])\\n>>> inverse_indices\\ntensor([ 0,  2,  1,  2])\\n\\n>>> output, inverse_indices = torch.unique(\\n...     torch.tensor([[1, 3], [2, 3]], dtype=torch.long), sorted=True, return_inverse=True)\\n>>> output\\ntensor([ 1,  2,  3])\\n>>> inverse_indices\\ntensor([[ 0,  2],\\n        [ 1,  2]])\\n',\n","   'Id': 289,\n","   'Question': 'How to use torch.unique, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique'},\n","  {'Answer': '>>> a = torch.randn(4)\\n>>> a\\ntensor([-0.5962,  1.4985, -0.4396,  1.4525])\\n>>> torch.asin(a)\\ntensor([-0.6387,     nan, -0.4552,     nan])\\n',\n","   'Id': 290,\n","   'Question': 'How to use torch.asin, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.asin.html#torch.asin'},\n","  {'Answer': 'optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\\noptimizer = optim.Adam([var1, var2], lr=0.0001)\\n',\n","   'Id': 291,\n","   'Question': 'How to use To construct anOptimizeryou have to give it an iterable containing the\\nparameters (all should beVariables) to optimize. Then,\\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc., give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/optim.html'},\n","  {'Answer': \"optim.SGD([\\n                {'params': model.base.parameters()},\\n                {'params': model.classifier.parameters(), 'lr': 1e-3}\\n            ], lr=1e-2, momentum=0.9)\\n\",\n","   'Id': 292,\n","   'Question': 'How to use Optimizers also support specifying per-parameter options. To do this, instead\\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\\naparamskey, containing a list of parameters belonging to it. Other keys\\nshould match the keyword arguments accepted by the optimizers, and will be used\\nas optimization options for this group.For example, this is very useful when one wants to specify per-layer learning rates:, give an example?',\n","   'context': ' For example, this is very useful when one wants to specify per-layer learning rates:',\n","   'source': 'https://pytorch.org/docs/stable/optim.html'},\n","  {'Answer': 'for input, target in dataset:\\n    optimizer.zero_grad()\\n    output = model(input)\\n    loss = loss_fn(output, target)\\n    loss.backward()\\n    optimizer.step()\\n',\n","   'Id': 293,\n","   'Question': 'How to use This is a simplified version supported by most optimizers. The function can be\\ncalled once the gradients are computed using e.g.backward()., give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/optim.html'},\n","  {'Answer': 'for input, target in dataset:\\n    def closure():\\n        optimizer.zero_grad()\\n        output = model(input)\\n        loss = loss_fn(output, target)\\n        loss.backward()\\n        return loss\\n    optimizer.step(closure)\\n',\n","   'Id': 294,\n","   'Question': 'How to use Some optimization algorithms such as Conjugate Gradient and LBFGS need to\\nreevaluate the function multiple times, so you have to pass in a closure that\\nallows them to recompute your model. The closure should clear the gradients,\\ncompute the loss, and return it., give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/optim.html'},\n","  {'Answer': 'model = [Parameter(torch.randn(2, 2, requires_grad=True))]\\noptimizer = SGD(model, 0.1)\\nscheduler = ExponentialLR(optimizer, gamma=0.9)\\n\\nfor epoch in range(20):\\n    for input, target in dataset:\\n        optimizer.zero_grad()\\n        output = model(input)\\n        loss = loss_fn(output, target)\\n        loss.backward()\\n        optimizer.step()\\n    scheduler.step()\\n',\n","   'Id': 295,\n","   'Question': 'How to use Learning rate scheduling should be applied after optimizer’s update; e.g., you\\nshould write your code this way:, give an example?',\n","   'context': ' Learning rate scheduling should be applied after optimizer’s update; e.g., you\\nshould write your code this way:',\n","   'source': 'https://pytorch.org/docs/stable/optim.html'},\n","  {'Answer': 'model = [Parameter(torch.randn(2, 2, requires_grad=True))]\\noptimizer = SGD(model, 0.1)\\nscheduler1 = ExponentialLR(optimizer, gamma=0.9)\\nscheduler2 = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\\n\\nfor epoch in range(20):\\n    for input, target in dataset:\\n        optimizer.zero_grad()\\n        output = model(input)\\n        loss = loss_fn(output, target)\\n        loss.backward()\\n        optimizer.step()\\n    scheduler1.step()\\n    scheduler2.step()\\n',\n","   'Id': 296,\n","   'Question': 'How to use Most learning rate schedulers can be called back-to-back (also referred to as\\nchaining schedulers). The result is that each scheduler is applied one after the\\nother on the learning rate obtained by the one preceding it., give an example?',\n","   'context': ' Most learning rate schedulers can be called back-to-back (also referred to as\\nchaining schedulers). The result is that each scheduler is applied one after the\\nother on the learning rate obtained by the one preceding it.',\n","   'source': 'https://pytorch.org/docs/stable/optim.html'},\n","  {'Answer': '>>> scheduler = ...\\n>>> for epoch in range(100):\\n>>>     train(...)\\n>>>     validate(...)\\n>>>     scheduler.step()\\n',\n","   'Id': 297,\n","   'Question': 'How to use In many places in the documentation, we will use the following template to refer to schedulers\\nalgorithms., give an example?',\n","   'context': ' In many places in the documentation, we will use the following template to refer to schedulers\\nalgorithms.',\n","   'source': 'https://pytorch.org/docs/stable/optim.html'},\n","  {'Answer': '>>> swa_model = AveragedModel(model)\\n',\n","   'Id': 298,\n","   'Question': 'How to use AveragedModelclass serves to compute the weights of the SWA model. You can create an\\naveraged model by running:, give an example?',\n","   'context': ' AveragedModelclass serves to compute the weights of the SWA model. You can create an\\naveraged model by running:',\n","   'source': 'https://pytorch.org/docs/stable/optim.html'},\n","  {'Answer': '>>> swa_model.update_parameters(model)\\n',\n","   'Id': 299,\n","   'Question': 'How to use AveragedModelclass serves to compute the weights of the SWA model. You can create an\\naveraged model by running:Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\\naverages, you can use theupdate_parameters()function:, give an example?',\n","   'context': ' Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\\naverages, you can use theupdate_parameters()function:',\n","   'source': 'https://pytorch.org/docs/stable/optim.html'},\n","  {'Answer': '>>> swa_scheduler = torch.optim.swa_utils.SWALR(optimizer, \\\\\\n>>>         anneal_strategy=\"linear\", anneal_epochs=5, swa_lr=0.05)\\n',\n","   'Id': 300,\n","   'Question': 'How to use Typically, in SWA the learning rate is set to a high constant value.SWALRis a\\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\\nconstant. For example, the following code creates a scheduler that linearly anneals the\\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group:, give an example?',\n","   'context': ' Typically, in SWA the learning rate is set to a high constant value.SWALRis a\\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\\nconstant. For example, the following code creates a scheduler that linearly anneals the\\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group:',\n","   'source': 'https://pytorch.org/docs/stable/optim.html'},\n","  {'Answer': '>>> torch.optim.swa_utils.update_bn(loader, swa_model)\\n',\n","   'Id': 301,\n","   'Question': 'How to use update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\\non a given dataloaderloaderat the end of training:, give an example?',\n","   'context': ' update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\\non a given dataloaderloaderat the end of training:',\n","   'source': 'https://pytorch.org/docs/stable/optim.html'},\n","  {'Answer': '>>> ema_avg = lambda averaged_model_parameter, model_parameter, num_averaged:\\\\\\n>>>         0.1 * averaged_model_parameter + 0.9 * model_parameter\\n>>> ema_model = torch.optim.swa_utils.AveragedModel(model, avg_fn=ema_avg)\\n',\n","   'Id': 302,\n","   'Question': 'How to use By default,torch.optim.swa_utils.AveragedModelcomputes a running equal average of\\nthe parameters that you provide, but you can also use custom averaging functions with theavg_fnparameter. In the following exampleema_modelcomputes an exponential moving average., give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/optim.html'},\n","  {'Answer': '>>> loader, optimizer, model, loss_fn = ...\\n>>> swa_model = torch.optim.swa_utils.AveragedModel(model)\\n>>> scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\\n>>> swa_start = 160\\n>>> swa_scheduler = SWALR(optimizer, swa_lr=0.05)\\n>>>\\n>>> for epoch in range(300):\\n>>>       for input, target in loader:\\n>>>           optimizer.zero_grad()\\n>>>           loss_fn(model(input), target).backward()\\n>>>           optimizer.step()\\n>>>       if epoch > swa_start:\\n>>>           swa_model.update_parameters(model)\\n>>>           swa_scheduler.step()\\n>>>       else:\\n>>>           scheduler.step()\\n>>>\\n>>> # Update bn statistics for the swa_model at the end\\n>>> torch.optim.swa_utils.update_bn(loader, swa_model)\\n>>> # Use swa_model to make predictions on test data\\n>>> preds = swa_model(test_input)\\n',\n","   'Id': 303,\n","   'Question': 'How to use In the example below,swa_modelis the SWA model that accumulates the averages of the weights.\\nWe train the model for a total of 300 epochs and we switch to the SWA learning rate schedule\\nand start to collect SWA averages of the parameters at epoch 160:, give an example?',\n","   'context': ' In the example below,swa_modelis the SWA model that accumulates the averages of the weights.\\nWe train the model for a total of 300 epochs and we switch to the SWA learning rate schedule\\nand start to collect SWA averages of the parameters at epoch 160:',\n","   'source': 'https://pytorch.org/docs/stable/optim.html'},\n","  {'Answer': \">>> t = torch.tensor([float('nan'), 1, 2])\\n>>> t.quantile(0.5)\\ntensor(nan)\\n>>> t.nanquantile(0.5)\\ntensor(1.5000)\\n>>> t = torch.tensor([[float('nan'), float('nan')], [1, 2]])\\n>>> t\\ntensor([[nan, nan],\\n        [1., 2.]])\\n>>> t.nanquantile(0.5, dim=0)\\ntensor([1., 2.])\\n>>> t.nanquantile(0.5, dim=1)\\ntensor([   nan, 1.5000])\\n\",\n","   'Id': 304,\n","   'Question': 'How to use torch.nanquantile, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.nanquantile.html#torch.nanquantile'},\n","  {'Answer': 'UPLO = \"U\" if upper else \"L\"\\nL = torch.linalg.eigvalsh(A, UPLO=UPLO)\\n',\n","   'Id': 305,\n","   'Question': 'How to use torch.symeig, give an example?',\n","   'context': ' torch.symeig()is deprecated in favor oftorch.linalg.eigh()and will be removed in a future PyTorch release. The default behavior has changed\\nfrom using the upper triangular portion of the matrix by default to using the\\nlower triangular portion.L,_=torch.symeig(A,upper=upper)should be replaced with',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig'},\n","  {'Answer': 'UPLO = \"U\" if upper else \"L\"\\nL, V = torch.linalg.eigh(A, UPLO=UPLO)\\n',\n","   'Id': 306,\n","   'Question': 'How  L,_=torch.symeig(A,upper=upper)should be replaced withL,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with, give an example?',\n","   'context': ' L,_=torch.symeig(A,upper=upper)should be replaced withL,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig'},\n","  {'Answer': '>>> a = torch.randn(5, 5)\\n>>> a = a + a.t()  # To make a symmetric\\n>>> a\\ntensor([[-5.7827,  4.4559, -0.2344, -1.7123, -1.8330],\\n        [ 4.4559,  1.4250, -2.8636, -3.2100, -0.1798],\\n        [-0.2344, -2.8636,  1.7112, -5.5785,  7.1988],\\n        [-1.7123, -3.2100, -5.5785, -2.6227,  3.1036],\\n        [-1.8330, -0.1798,  7.1988,  3.1036, -5.1453]])\\n>>> e, v = torch.symeig(a, eigenvectors=True)\\n>>> e\\ntensor([-13.7012,  -7.7497,  -2.3163,   5.2477,   8.1050])\\n>>> v\\ntensor([[ 0.1643,  0.9034, -0.0291,  0.3508,  0.1817],\\n        [-0.2417, -0.3071, -0.5081,  0.6534,  0.4026],\\n        [-0.5176,  0.1223, -0.0220,  0.3295, -0.7798],\\n        [-0.4850,  0.2695, -0.5773, -0.5840,  0.1337],\\n        [ 0.6415, -0.0447, -0.6381, -0.0193, -0.4230]])\\n>>> a_big = torch.randn(5, 2, 2)\\n>>> a_big = a_big + a_big.transpose(-2, -1)  # To make a_big symmetric\\n>>> e, v = a_big.symeig(eigenvectors=True)\\n>>> torch.allclose(torch.matmul(v, torch.matmul(e.diag_embed(), v.transpose(-2, -1))), a_big)\\nTrue\\n',\n","   'Id': 307,\n","   'Question': 'How  IfupperisFalse, then lower triangular portion is used., give an example?',\n","   'context': ' IfupperisFalse, then lower triangular portion is used.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig'},\n","  {'Answer': '>>> a = torch.tensor([5, 10, 15])\\n>>> b = torch.tensor([3, 4, 5])\\n>>> torch.gcd(a, b)\\ntensor([1, 2, 5])\\n>>> c = torch.tensor([3])\\n>>> torch.gcd(a, c)\\ntensor([1, 1, 3])\\n',\n","   'Id': 308,\n","   'Question': 'How to use torch.gcd, give an example?',\n","   'context': ' Bothinputandothermust have integer types.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.gcd.html#torch.gcd'},\n","  {'Answer': '>>> a = torch.randn(2, 2, 2)\\n>>> a[0, :, :] = torch.eye(2, 2)\\n>>> a[1, :, :] = 2 * torch.eye(2, 2)\\n>>> a\\ntensor([[[1., 0.],\\n         [0., 1.]],\\n\\n        [[2., 0.],\\n         [0., 2.]]])\\n>>> torch.matrix_exp(a)\\ntensor([[[2.7183, 0.0000],\\n         [0.0000, 2.7183]],\\n\\n         [[7.3891, 0.0000],\\n          [0.0000, 7.3891]]])\\n\\n>>> import math\\n>>> x = torch.tensor([[0, math.pi/3], [-math.pi/3, 0]])\\n>>> x.matrix_exp() # should be [[cos(pi/3), sin(pi/3)], [-sin(pi/3), cos(pi/3)]]\\ntensor([[ 0.5000,  0.8660],\\n        [-0.8660,  0.5000]])\\n',\n","   'Id': 309,\n","   'Question': 'How to use torch.matrix_exp, give an example?',\n","   'context': ' Bader, P.; Blanes, S.; Casas, F.\\nComputing the Matrix Exponential with an Optimized Taylor Polynomial Approximation.\\nMathematics 2019, 7, 1174.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.matrix_exp.html#torch.matrix_exp'},\n","  {'Answer': 'def entrypoint_name(*args, **kwargs):\\n    # args & kwargs are optional, for models which take positional/keyword arguments.\\n    ...\\n',\n","   'Id': 310,\n","   'Question': 'How to use Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights)\\nto a github repository by adding a simplehubconf.pyfile;hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\\n(example: a pre-trained model you want to publish)., give an example?',\n","   'context': ' hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\\n(example: a pre-trained model you want to publish).',\n","   'source': 'https://pytorch.org/docs/stable/hub.html'},\n","  {'Answer': 'dependencies = [\\'torch\\']\\nfrom torchvision.models.resnet import resnet18 as _resnet18\\n\\n# resnet18 is the name of entrypoint\\ndef resnet18(pretrained=False, **kwargs):\\n    \"\"\" # This docstring shows up in hub.help()\\n    Resnet18 model\\n    pretrained (bool): kwargs, load pretrained weights into the model\\n    \"\"\"\\n    # Call the model, load pretrained weights\\n    model = _resnet18(pretrained=pretrained, **kwargs)\\n    return model\\n',\n","   'Id': 311,\n","   'Question': 'How to use Here is a code snippet specifies an entrypoint forresnet18model if we expand\\nthe implementation inpytorch/vision/hubconf.py.\\nIn most case importing the right function inhubconf.pyis sufficient. Here we\\njust want to use the expanded version as an example to show how it works.\\nYou can see the full script inpytorch/vision repo, give an example?',\n","   'context': ' Here is a code snippet specifies an entrypoint forresnet18model if we expand\\nthe implementation inpytorch/vision/hubconf.py.\\nIn most case importing the right function inhubconf.pyis sufficient. Here we\\njust want to use the expanded version as an example to show how it works.\\nYou can see the full script inpytorch/vision repo',\n","   'source': 'https://pytorch.org/docs/stable/hub.html'},\n","  {'Answer': \"if pretrained:\\n    # For checkpoint saved in local github repo, e.g. <RELATIVE_PATH_TO_CHECKPOINT>=weights/save.pth\\n    dirname = os.path.dirname(__file__)\\n    checkpoint = os.path.join(dirname, <RELATIVE_PATH_TO_CHECKPOINT>)\\n    state_dict = torch.load(checkpoint)\\n    model.load_state_dict(state_dict)\\n\\n    # For checkpoint saved elsewhere\\n    checkpoint = 'https://download.pytorch.org/models/resnet18-5c106cde.pth'\\n    model.load_state_dict(torch.hub.load_state_dict_from_url(checkpoint, progress=False))\\n\",\n","   'Id': 312,\n","   'Question': 'How  Here is a code snippet specifies an entrypoint forresnet18model if we expand\\nthe implementation inpytorch/vision/hubconf.py.\\nIn most case importing the right function inhubconf.pyis sufficient. Here we\\njust want to use the expanded version as an example to show how it works.\\nYou can see the full script inpytorch/vision repo, give an example?',\n","   'context': ' Here is a code snippet specifies an entrypoint forresnet18model if we expand\\nthe implementation inpytorch/vision/hubconf.py.\\nIn most case importing the right function inhubconf.pyis sufficient. Here we\\njust want to use the expanded version as an example to show how it works.\\nYou can see the full script inpytorch/vision repo',\n","   'source': 'https://pytorch.org/docs/stable/hub.html'},\n","  {'Answer': \">>> entrypoints = torch.hub.list('pytorch/vision', force_reload=True)\\n\",\n","   'Id': 313,\n","   'Question': 'How to use torch.hub.list, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/hub.html'},\n","  {'Answer': \">>> print(torch.hub.help('pytorch/vision', 'resnet18', force_reload=True))\\n\",\n","   'Id': 314,\n","   'Question': 'How to use torch.hub.help, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/hub.html'},\n","  {'Answer': \">>> # from a github repo\\n>>> repo = 'pytorch/vision'\\n>>> model = torch.hub.load(repo, 'resnet50', pretrained=True)\\n>>> # from a local directory\\n>>> path = '/some/local/path/pytorch/vision'\\n>>> model = torch.hub.load(path, 'resnet50', pretrained=True)\\n\",\n","   'Id': 315,\n","   'Question': 'How to use torch.hub.load, give an example?',\n","   'context': \" Ifsourceis'local',repo_or_diris expected to be a\\npath to a local directory.\",\n","   'source': 'https://pytorch.org/docs/stable/hub.html'},\n","  {'Answer': \">>> torch.hub.download_url_to_file('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth', '/tmp/temporary_file')\\n\",\n","   'Id': 316,\n","   'Question': 'How to use torch.hub.download_url_to_file, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/hub.html'},\n","  {'Answer': \">>> state_dict = torch.hub.load_state_dict_from_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')\\n\",\n","   'Id': 317,\n","   'Question': 'How to use torch.hub.load_state_dict_from_url, give an example?',\n","   'context': ' If the object is already present inmodel_dir, it’s deserialized and\\nreturned.\\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir().',\n","   'source': 'https://pytorch.org/docs/stable/hub.html'},\n","  {'Answer': '>>> a = torch.rand(5)\\n>>> a\\ntensor([ 0.5224,  0.9354,  0.7257,  0.1301,  0.2251])\\n\\n\\n>>> torch.log10(a)\\ntensor([-0.2820, -0.0290, -0.1392, -0.8857, -0.6476])\\n',\n","   'Id': 318,\n","   'Question': 'How to use torch.log10, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.log10.html#torch.log10'},\n","  {'Answer': \">>> a = torch.tensor([2.2, float('nan'), 2.1, float('nan')])\\n>>> b = torch.tensor([-9.3, 0.1, float('nan'), float('nan')])\\n>>> torch.fmin(a, b)\\ntensor([-9.3000, 0.1000, 2.1000,    nan])\\n\",\n","   'Id': 319,\n","   'Question': 'How to use torch.fmin, give an example?',\n","   'context': ' Supportsbroadcasting to a common shape,type promotion, and integer and floating-point inputs.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.fmin.html#torch.fmin'},\n","  {'Answer': '>>> v1 = torch.arange(1., 5.)\\n>>> v2 = torch.arange(1., 4.)\\n>>> torch.outer(v1, v2)\\ntensor([[  1.,   2.,   3.],\\n        [  2.,   4.,   6.],\\n        [  3.,   6.,   9.],\\n        [  4.,   8.,  12.]])\\n',\n","   'Id': 320,\n","   'Question': 'How to use torch.outer, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.outer.html#torch.outer'},\n","  {'Answer': '>>> a = torch.randn(4)\\n>>> a\\ntensor([-1.2027, -1.7687,  0.4412, -1.3856])\\n>>> torch.tan(a)\\ntensor([-2.5930,  4.9859,  0.4722, -5.3366])\\n',\n","   'Id': 321,\n","   'Question': 'How to use torch.tan, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.tan.html#torch.tan'},\n","  {'Answer': '>>> vec1 = torch.arange(1., 4.)\\n>>> vec2 = torch.arange(1., 3.)\\n>>> M = torch.zeros(3, 2)\\n>>> torch.addr(M, vec1, vec2)\\ntensor([[ 1.,  2.],\\n        [ 2.,  4.],\\n        [ 3.,  6.]])\\n',\n","   'Id': 322,\n","   'Question': 'How to use torch.addr, give an example?',\n","   'context': ' Ifvec1is a vector of sizenandvec2is a vector\\nof sizem, theninputmust bebroadcastablewith a matrix of size(n×m)(n \\\\times m)(n×m)andoutwill be a matrix of size(n×m)(n \\\\times m)(n×m).',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.addr.html#torch.addr'},\n","  {'Answer': '>>> a = torch.randn(10)\\n>>> a\\ntensor([ 0.6001,  0.2069, -0.1919,  0.9792,  0.6727,  1.0062,  0.4126,\\n        -0.2129, -0.4206,  0.1968])\\n>>> torch.cumprod(a, dim=0)\\ntensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0158, -0.0065,\\n         0.0014, -0.0006, -0.0001])\\n\\n>>> a[5] = 0.0\\n>>> torch.cumprod(a, dim=0)\\ntensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0000, -0.0000,\\n         0.0000, -0.0000, -0.0000])\\n',\n","   'Id': 323,\n","   'Question': 'How to use torch.cumprod, give an example?',\n","   'context': ' For example, ifinputis a vector of size N, the result will also be\\na vector of size N, with elements.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.cumprod.html#torch.cumprod'},\n","  {'Answer': '# Dot product\\n>>> torch.inner(torch.tensor([1, 2, 3]), torch.tensor([0, 2, 1]))\\ntensor(7)\\n\\n# Multidimensional input tensors\\n>>> a = torch.randn(2, 3)\\n>>> a\\ntensor([[0.8173, 1.0874, 1.1784],\\n        [0.3279, 0.1234, 2.7894]])\\n>>> b = torch.randn(2, 4, 3)\\n>>> b\\ntensor([[[-0.4682, -0.7159,  0.1506],\\n        [ 0.4034, -0.3657,  1.0387],\\n        [ 0.9892, -0.6684,  0.1774],\\n        [ 0.9482,  1.3261,  0.3917]],\\n\\n        [[ 0.4537,  0.7493,  1.1724],\\n        [ 0.2291,  0.5749, -0.2267],\\n        [-0.7920,  0.3607, -0.3701],\\n        [ 1.3666, -0.5850, -1.7242]]])\\n>>> torch.inner(a, b)\\ntensor([[[-0.9837,  1.1560,  0.2907,  2.6785],\\n        [ 2.5671,  0.5452, -0.6912, -1.5509]],\\n\\n        [[ 0.1782,  2.9843,  0.7366,  1.5672],\\n        [ 3.5115, -0.4864, -1.2476, -4.4337]]])\\n\\n# Scalar input\\n>>> torch.inner(a, torch.tensor(2))\\ntensor([[1.6347, 2.1748, 2.3567],\\n        [0.6558, 0.2469, 5.5787]])\\n',\n","   'Id': 324,\n","   'Question': 'How to use torch.inner, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.inner.html#torch.inner'},\n","  {'Answer': \">>> torch.isclose(torch.tensor((1., 2, 3)), torch.tensor((1 + 1e-10, 3, 4)))\\ntensor([ True, False, False])\\n>>> torch.isclose(torch.tensor((float('inf'), 4)), torch.tensor((float('inf'), 6)), rtol=.5)\\ntensor([True, True])\\n\",\n","   'Id': 325,\n","   'Question': 'How to use torch.isclose, give an example?',\n","   'context': ' whereinputandotherare finite. Whereinputand/orotherare nonfinite they are close if and only if\\nthey are equal, with NaNs being considered equal to each other whenequal_nanis True.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.isclose.html#torch.isclose'},\n","  {'Answer': '>>> a = torch.randn(4)\\n>>> a\\ntensor([-0.0370,  0.2970,  1.5420, -0.9105])\\n>>> torch.rsqrt(a)\\ntensor([    nan,  1.8351,  0.8053,     nan])\\n',\n","   'Id': 326,\n","   'Question': 'How to use torch.rsqrt, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.rsqrt.html#torch.rsqrt'},\n","  {'Answer': '>>> sorted_sequence = torch.tensor([[1, 3, 5, 7, 9], [2, 4, 6, 8, 10]])\\n>>> sorted_sequence\\ntensor([[ 1,  3,  5,  7,  9],\\n        [ 2,  4,  6,  8, 10]])\\n>>> values = torch.tensor([[3, 6, 9], [3, 6, 9]])\\n>>> values\\ntensor([[3, 6, 9],\\n        [3, 6, 9]])\\n>>> torch.searchsorted(sorted_sequence, values)\\ntensor([[1, 3, 4],\\n        [1, 2, 4]])\\n>>> torch.searchsorted(sorted_sequence, values, right=True)\\ntensor([[2, 3, 5],\\n        [1, 3, 4]])\\n\\n>>> sorted_sequence_1d = torch.tensor([1, 3, 5, 7, 9])\\n>>> sorted_sequence_1d\\ntensor([1, 3, 5, 7, 9])\\n>>> torch.searchsorted(sorted_sequence_1d, values)\\ntensor([[1, 3, 4],\\n        [1, 3, 4]])\\n',\n","   'Id': 327,\n","   'Question': 'How to use torch.searchsorted, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted'},\n","  {'Answer': '>>> import torch\\n>>> A = torch.tensor([[0, 1], [1, 0]])\\n>>> B = torch.tensor([[3, 4, 5], [6, 7, 8]])\\n>>> C = torch.tensor(7)\\n>>> D = torch.tensor([1, 2, 3])\\n>>> E = torch.tensor([[4], [5], [6]])\\n>>> torch.block_diag(A, B, C, D, E)\\ntensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\\n        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\\n        [0, 0, 3, 4, 5, 0, 0, 0, 0, 0],\\n        [0, 0, 6, 7, 8, 0, 0, 0, 0, 0],\\n        [0, 0, 0, 0, 0, 7, 0, 0, 0, 0],\\n        [0, 0, 0, 0, 0, 0, 1, 2, 3, 0],\\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 4],\\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 5],\\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 6]])\\n',\n","   'Id': 328,\n","   'Question': 'How to use torch.block_diag, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.block_diag.html#torch.block_diag'},\n","  {'Answer': 'torch.linalg.solve(A, B) == A.inv() @ B\\n',\n","   'Id': 329,\n","   'Question': 'How to use torch.linalg.inv, give an example?',\n","   'context': ' Consider usingtorch.linalg.solve()if possible for multiplying a matrix on the left by\\nthe inverse, as:',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv'},\n","  {'Answer': '>>> x = torch.rand(4, 4)\\n>>> y = torch.linalg.inv(x)\\n>>> z = x @ y\\n>>> z\\ntensor([[ 1.0000, -0.0000, -0.0000,  0.0000],\\n        [ 0.0000,  1.0000,  0.0000,  0.0000],\\n        [ 0.0000,  0.0000,  1.0000,  0.0000],\\n        [ 0.0000, -0.0000, -0.0000,  1.0000]])\\n>>> torch.dist(z, torch.eye(4))\\ntensor(1.1921e-07)\\n\\n>>> # Batched inverse example\\n>>> x = torch.randn(2, 3, 4, 4)\\n>>> y = torch.linalg.inv(x)\\n>>> z = x @ y\\n>>> torch.dist(z, torch.eye(4).expand_as(x))\\ntensor(1.9073e-06)\\n\\n>>> x = torch.rand(4, 4, dtype=torch.cdouble)\\n>>> y = torch.linalg.inv(x)\\n>>> z = x @ y\\n>>> torch.dist(z, torch.eye(4, dtype=torch.cdouble))\\ntensor(7.5107e-16, dtype=torch.float64)\\n',\n","   'Id': 330,\n","   'Question': 'How  Supports input of float, double, cfloat and cdouble dtypes.\\nAlso supports batches of matrices, and ifAis a batch of matrices\\nthen the output has the same batch dimensions., give an example?',\n","   'context': ' Supports input of float, double, cfloat and cdouble dtypes.\\nAlso supports batches of matrices, and ifAis a batch of matrices\\nthen the output has the same batch dimensions.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv'},\n","  {'Answer': '>>> torch.bitwise_or(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))\\ntensor([-1, -2,  3], dtype=torch.int8)\\n>>> torch.bitwise_or(torch.tensor([True, True, False]), torch.tensor([False, True, False]))\\ntensor([ True, True, False])\\n',\n","   'Id': 331,\n","   'Question': 'How to use torch.bitwise_or, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.bitwise_or.html#torch.bitwise_or'},\n","  {'Answer': '>>> a1 = torch.tensor([4.0])\\n>>> a2 = torch.tensor([3.0, 4.0, 5.0])\\n>>> a = torch.igammac(a1, a2)\\ntensor([0.6472, 0.4335, 0.2650])\\n>>> b = torch.igamma(a1, a2) + torch.igammac(a1, a2)\\ntensor([1., 1., 1.])\\n',\n","   'Id': 332,\n","   'Question': 'How to use torch.igammac, give an example?',\n","   'context': ' Supportsbroadcasting to a common shapeand float inputs.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.igammac.html#torch.igammac'},\n","  {'Answer': '>>> torch.randint(3, 5, (3,))\\ntensor([4, 3, 4])\\n\\n\\n>>> torch.randint(10, (2, 2))\\ntensor([[0, 2],\\n        [5, 5]])\\n\\n\\n>>> torch.randint(3, 10, (2, 2))\\ntensor([[4, 5],\\n        [6, 7]])\\n',\n","   'Id': 333,\n","   'Question': 'How to use torch.randint, give an example?',\n","   'context': ' The shape of the tensor is defined by the variable argumentsize.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.randint.html#torch.randint'},\n","  {'Answer': '>>> x = torch.randn(3, 4)\\n>>> x\\ntensor([[ 0.3552, -2.3825, -0.8297,  0.3477],\\n        [-1.2035,  1.2252,  0.5002,  0.6248],\\n        [ 0.1307, -2.0608,  0.1244,  2.0139]])\\n>>> mask = x.ge(0.5)\\n>>> mask\\ntensor([[False, False, False, False],\\n        [False, True, True, True],\\n        [False, False, False, True]])\\n>>> torch.masked_select(x, mask)\\ntensor([ 1.2252,  0.5002,  0.6248,  2.0139])\\n',\n","   'Id': 334,\n","   'Question': 'How to use torch.masked_select, give an example?',\n","   'context': ' The shapes of themasktensor and theinputtensor don’t need\\nto match, but they must bebroadcastable.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.masked_select.html#torch.masked_select'},\n","  {'Answer': '>>> x = torch.zeros(1, requires_grad=True)\\n>>> with torch.no_grad():\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> is_train = False\\n>>> with torch.set_grad_enabled(is_train):\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> torch.set_grad_enabled(True)  # this can also be used as a function\\n>>> y = x * 2\\n>>> y.requires_grad\\nTrue\\n\\n>>> torch.set_grad_enabled(False)\\n>>> y = x * 2\\n>>> y.requires_grad\\nFalse\\n',\n","   'Id': 335,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/torch.html#tensors'},\n","  {'Answer': '>>> mat1 = torch.randn(2, 3)\\n>>> mat2 = torch.randn(3, 3)\\n>>> torch.mm(mat1, mat2)\\ntensor([[ 0.4851,  0.5037, -0.3633],\\n        [-0.0760, -3.6705,  2.4784]])\\n',\n","   'Id': 336,\n","   'Question': 'How to use torch.mm, give an example?',\n","   'context': ' This operator supportsTensorFloat32.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.mm.html#torch.mm'},\n","  {'Answer': '>>> t = torch.randn(3,2,1)\\n>>> t\\ntensor([[[-0.3362],\\n        [-0.8437]],\\n\\n        [[-0.9627],\\n        [ 0.1727]],\\n\\n        [[ 0.5173],\\n        [-0.1398]]])\\n>>> torch.movedim(t, 1, 0).shape\\ntorch.Size([2, 3, 1])\\n>>> torch.movedim(t, 1, 0)\\ntensor([[[-0.3362],\\n        [-0.9627],\\n        [ 0.5173]],\\n\\n        [[-0.8437],\\n        [ 0.1727],\\n        [-0.1398]]])\\n>>> torch.movedim(t, (1, 2), (0, 1)).shape\\ntorch.Size([2, 1, 3])\\n>>> torch.movedim(t, (1, 2), (0, 1))\\ntensor([[[-0.3362, -0.9627,  0.5173]],\\n\\n        [[-0.8437,  0.1727, -0.1398]]])\\n',\n","   'Id': 337,\n","   'Question': 'How to use torch.movedim, give an example?',\n","   'context': ' Other dimensions ofinputthat are not explicitly moved remain in\\ntheir original order and appear at the positions not specified indestination.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.movedim.html#torch.movedim'},\n","  {'Answer': '>>> M = torch.randn(2, 3)\\n>>> mat1 = torch.randn(2, 3)\\n>>> mat2 = torch.randn(3, 3)\\n>>> torch.addmm(M, mat1, mat2)\\ntensor([[-4.8716,  1.4671, -1.3746],\\n        [ 0.7573, -3.9555, -2.8681]])\\n',\n","   'Id': 338,\n","   'Question': 'How to use torch.addmm, give an example?',\n","   'context': ' This operator supportsTensorFloat32.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.addmm.html#torch.addmm'},\n","  {'Answer': '>>> x = torch.zeros(1, requires_grad=True)\\n>>> with torch.no_grad():\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> is_train = False\\n>>> with torch.set_grad_enabled(is_train):\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> torch.set_grad_enabled(True)  # this can also be used as a function\\n>>> y = x * 2\\n>>> y.requires_grad\\nTrue\\n\\n>>> torch.set_grad_enabled(False)\\n>>> y = x * 2\\n>>> y.requires_grad\\nFalse\\n',\n","   'Id': 339,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/torch.html#serialization'},\n","  {'Answer': '>>> a = torch.randn(1, 3)\\n>>> a\\ntensor([[-0.8020,  0.5428, -1.5854]])\\n>>> torch.prod(a)\\ntensor(0.6902)\\n',\n","   'Id': 340,\n","   'Question': 'How to use torch.prod, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.prod.html#torch.prod'},\n","  {'Answer': '>>> a = torch.randn(4, 2)\\n>>> a\\ntensor([[ 0.5261, -0.3837],\\n        [ 1.1857, -0.2498],\\n        [-1.1646,  0.0705],\\n        [ 1.1131, -1.0629]])\\n>>> torch.prod(a, 1)\\ntensor([-0.2018, -0.2962, -0.0821, -1.1831])\\n',\n","   'Id': 341,\n","   'Question': 'How  IfkeepdimisTrue, the output tensor is of the same size\\nasinputexcept in the dimensiondimwhere it is of size 1.\\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\\nthe output tensor having 1 fewer dimension thaninput., give an example?',\n","   'context': ' IfkeepdimisTrue, the output tensor is of the same size\\nasinputexcept in the dimensiondimwhere it is of size 1.\\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\\nthe output tensor having 1 fewer dimension thaninput.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.prod.html#torch.prod'},\n","  {'Answer': '>>> x = torch.randn(3, 2)\\n>>> y = torch.ones(3, 2)\\n>>> x\\ntensor([[-0.4620,  0.3139],\\n        [ 0.3898, -0.7197],\\n        [ 0.0478, -0.1657]])\\n>>> torch.where(x > 0, x, y)\\ntensor([[ 1.0000,  0.3139],\\n        [ 0.3898,  1.0000],\\n        [ 0.0478,  1.0000]])\\n>>> x = torch.randn(2, 2, dtype=torch.double)\\n>>> x\\ntensor([[ 1.0779,  0.0383],\\n        [-0.8785, -1.1089]], dtype=torch.float64)\\n>>> torch.where(x > 0, x, 0.)\\ntensor([[1.0779, 0.0383],\\n        [0.0000, 0.0000]], dtype=torch.float64)\\n',\n","   'Id': 342,\n","   'Question': 'How to use torch.where, give an example?',\n","   'context': ' The operation is defined as:',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.where.html#torch.where'},\n","  {'Answer': '>>> torch.empty((2,3), dtype=torch.int64)\\ntensor([[ 9.4064e+13,  2.8000e+01,  9.3493e+13],\\n        [ 7.5751e+18,  7.1428e+18,  7.5955e+18]])\\n',\n","   'Id': 343,\n","   'Question': 'How to use torch.empty_like, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.empty_like.html#torch.empty_like'},\n","  {'Answer': '>>> input = torch.randn(10, 3, 4)\\n>>> mat2 = torch.randn(10, 4, 5)\\n>>> res = torch.bmm(input, mat2)\\n>>> res.size()\\ntorch.Size([10, 3, 5])\\n',\n","   'Id': 344,\n","   'Question': 'How to use torch.bmm, give an example?',\n","   'context': ' This operator supportsTensorFloat32.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.bmm.html#torch.bmm'},\n","  {'Answer': '>>> torch.randn(4)\\ntensor([-2.1436,  0.9966,  2.3426, -0.6366])\\n>>> torch.randn(2, 3)\\ntensor([[ 1.5954,  2.8929, -1.0923],\\n        [ 1.1719, -0.4709, -0.1996]])\\n',\n","   'Id': 345,\n","   'Question': 'How to use torch.randn, give an example?',\n","   'context': ' The shape of the tensor is defined by the variable argumentsize.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn'},\n","  {'Answer': '>>> rates = torch.rand(4, 4) * 5  # rate parameter between 0 and 5\\n>>> torch.poisson(rates)\\ntensor([[9., 1., 3., 5.],\\n        [8., 6., 6., 0.],\\n        [0., 4., 5., 3.],\\n        [2., 1., 4., 2.]])\\n',\n","   'Id': 346,\n","   'Question': 'How to use torch.poisson, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.poisson.html#torch.poisson'},\n","  {'Answer': '>>> a = torch.tensor([1, 2, 3])\\n>>> b = torch.tensor([4, 5, 6])\\n>>> torch.vstack((a,b))\\ntensor([[1, 2, 3],\\n        [4, 5, 6]])\\n>>> a = torch.tensor([[1],[2],[3]])\\n>>> b = torch.tensor([[4],[5],[6]])\\n>>> torch.vstack((a,b))\\ntensor([[1],\\n        [2],\\n        [3],\\n        [4],\\n        [5],\\n        [6]])\\n',\n","   'Id': 347,\n","   'Question': 'How to use torch.vstack, give an example?',\n","   'context': ' This is equivalent to concatenation along the first axis after all 1-D tensors have been reshaped bytorch.atleast_2d().',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.vstack.html#torch.vstack'},\n","  {'Answer': '>>> a = torch.arange(4.)\\n>>> torch.reshape(a, (2, 2))\\ntensor([[ 0.,  1.],\\n        [ 2.,  3.]])\\n>>> b = torch.tensor([[0, 1], [2, 3]])\\n>>> torch.reshape(b, (-1,))\\ntensor([ 0,  1,  2,  3])\\n',\n","   'Id': 348,\n","   'Question': 'How to use torch.reshape, give an example?',\n","   'context': ' A single dimension may be -1, in which case it’s inferred from the remaining\\ndimensions and the number of elements ininput.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape'},\n","  {'Answer': '>>> boundaries = torch.tensor([1, 3, 5, 7, 9])\\n>>> boundaries\\ntensor([1, 3, 5, 7, 9])\\n>>> v = torch.tensor([[3, 6, 9], [3, 6, 9]])\\n>>> v\\ntensor([[3, 6, 9],\\n        [3, 6, 9]])\\n>>> torch.bucketize(v, boundaries)\\ntensor([[1, 3, 4],\\n        [1, 3, 4]])\\n>>> torch.bucketize(v, boundaries, right=True)\\ntensor([[2, 3, 5],\\n        [2, 3, 5]])\\n',\n","   'Id': 349,\n","   'Question': 'How to use torch.bucketize, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.bucketize.html#torch.bucketize'},\n","  {'Answer': '>>> a = torch.randn(5)\\n>>> a\\ntensor([-1.2557, -0.0026, -0.5387,  0.4740, -0.9244])\\n>>> torch.copysign(a, 1)\\ntensor([1.2557, 0.0026, 0.5387, 0.4740, 0.9244])\\n>>> a = torch.randn(4, 4)\\n>>> a\\ntensor([[ 0.7079,  0.2778, -1.0249,  0.5719],\\n        [-0.0059, -0.2600, -0.4475, -1.3948],\\n        [ 0.3667, -0.9567, -2.5757, -0.1751],\\n        [ 0.2046, -0.0742,  0.2998, -0.1054]])\\n>>> b = torch.randn(4)\\ntensor([ 0.2373,  0.3120,  0.3190, -1.1128])\\n>>> torch.copysign(a, b)\\ntensor([[ 0.7079,  0.2778,  1.0249, -0.5719],\\n        [ 0.0059,  0.2600,  0.4475, -1.3948],\\n        [ 0.3667,  0.9567,  2.5757, -0.1751],\\n        [ 0.2046,  0.0742,  0.2998, -0.1054]])\\n',\n","   'Id': 350,\n","   'Question': 'How to use torch.copysign, give an example?',\n","   'context': ' Supportsbroadcasting to a common shape,\\nand integer and float inputs.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.copysign.html#torch.copysign'},\n","  {'Answer': '>>> x = torch.arange(1., 6.)\\n>>> x\\ntensor([ 1.,  2.,  3.,  4.,  5.])\\n>>> torch.kthvalue(x, 4)\\ntorch.return_types.kthvalue(values=tensor(4.), indices=tensor(3))\\n\\n>>> x=torch.arange(1.,7.).resize_(2,3)\\n>>> x\\ntensor([[ 1.,  2.,  3.],\\n        [ 4.,  5.,  6.]])\\n>>> torch.kthvalue(x, 2, 0, True)\\ntorch.return_types.kthvalue(values=tensor([[4., 5., 6.]]), indices=tensor([[1, 1, 1]]))\\n',\n","   'Id': 351,\n","   'Question': 'How to use torch.kthvalue, give an example?',\n","   'context': ' IfkeepdimisTrue, both thevaluesandindicestensors\\nare the same size asinput, except in the dimensiondimwhere\\nthey are of size 1. Otherwise,dimis squeezed\\n(seetorch.squeeze()), resulting in both thevaluesandindicestensors having 1 fewer dimension than theinputtensor.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.kthvalue.html#torch.kthvalue'},\n","  {'Answer': 'L = torch.linalg.cholesky(A)\\n',\n","   'Id': 352,\n","   'Question': 'How to use torch.cholesky, give an example?',\n","   'context': ' torch.cholesky()is deprecated in favor oftorch.linalg.cholesky()and will be removed in a future PyTorch release.L=torch.cholesky(A)should be replaced with',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky'},\n","  {'Answer': 'U = torch.linalg.cholesky(A.transpose(-2, -1).conj()).transpose(-2, -1).conj()\\n',\n","   'Id': 353,\n","   'Question': 'How  L=torch.cholesky(A)should be replaced withU=torch.cholesky(A,upper=True)should be replaced with, give an example?',\n","   'context': ' L=torch.cholesky(A)should be replaced withU=torch.cholesky(A,upper=True)should be replaced with',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky'},\n","  {'Answer': '>>> a = torch.randn(3, 3)\\n>>> a = torch.mm(a, a.t()) # make symmetric positive-definite\\n>>> l = torch.cholesky(a)\\n>>> a\\ntensor([[ 2.4112, -0.7486,  1.4551],\\n        [-0.7486,  1.3544,  0.1294],\\n        [ 1.4551,  0.1294,  1.6724]])\\n>>> l\\ntensor([[ 1.5528,  0.0000,  0.0000],\\n        [-0.4821,  1.0592,  0.0000],\\n        [ 0.9371,  0.5487,  0.7023]])\\n>>> torch.mm(l, l.t())\\ntensor([[ 2.4112, -0.7486,  1.4551],\\n        [-0.7486,  1.3544,  0.1294],\\n        [ 1.4551,  0.1294,  1.6724]])\\n>>> a = torch.randn(3, 2, 2)\\n>>> a = torch.matmul(a, a.transpose(-1, -2)) + 1e-03 # make symmetric positive-definite\\n>>> l = torch.cholesky(a)\\n>>> z = torch.matmul(l, l.transpose(-1, -2))\\n>>> torch.max(torch.abs(z - a)) # Max non-zero\\ntensor(2.3842e-07)\\n',\n","   'Id': 354,\n","   'Question': 'How  IfupperisTrue, andAAAis a batch of symmetric positive-definite\\nmatrices, then the returned tensor will be composed of upper-triangular Cholesky factors\\nof each of the individual matrices. Similarly, whenupperisFalse, the returned\\ntensor will be composed of lower-triangular Cholesky factors of each of the individual\\nmatrices., give an example?',\n","   'context': ' IfupperisTrue, andAAAis a batch of symmetric positive-definite\\nmatrices, then the returned tensor will be composed of upper-triangular Cholesky factors\\nof each of the individual matrices. Similarly, whenupperisFalse, the returned\\ntensor will be composed of lower-triangular Cholesky factors of each of the individual\\nmatrices.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky'},\n","  {'Answer': '>>> torch.histc(torch.tensor([1., 2, 1]), bins=4, min=0, max=3)\\ntensor([ 0.,  2.,  1.,  0.])\\n',\n","   'Id': 355,\n","   'Question': 'How to use torch.histc, give an example?',\n","   'context': ' Elements lower than min and higher than max are ignored.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.histc.html#torch.histc'},\n","  {'Answer': '>>> x = torch.zeros(1, requires_grad=True)\\n>>> with torch.no_grad():\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> is_train = False\\n>>> with torch.set_grad_enabled(is_train):\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> torch.set_grad_enabled(True)  # this can also be used as a function\\n>>> y = x * 2\\n>>> y.requires_grad\\nTrue\\n\\n>>> torch.set_grad_enabled(False)\\n>>> y = x * 2\\n>>> y.requires_grad\\nFalse\\n',\n","   'Id': 356,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/torch.html#creation-ops'},\n","  {'Answer': '>>> x = torch.zeros(1, requires_grad=True)\\n>>> with torch.no_grad():\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> is_train = False\\n>>> with torch.set_grad_enabled(is_train):\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> torch.set_grad_enabled(True)  # this can also be used as a function\\n>>> y = x * 2\\n>>> y.requires_grad\\nTrue\\n\\n>>> torch.set_grad_enabled(False)\\n>>> y = x * 2\\n>>> y.requires_grad\\nFalse\\n',\n","   'Id': 357,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/torch.html#torch'},\n","  {'Answer': '>>> torch.eq(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\\ntensor([[ True, False],\\n        [False, True]])\\n',\n","   'Id': 358,\n","   'Question': 'How to use torch.eq, give an example?',\n","   'context': ' The second argument can be a number or a tensor whose shape isbroadcastablewith the first argument.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.eq.html#torch.eq'},\n","  {'Answer': '>>> x = torch.zeros(1, requires_grad=True)\\n>>> with torch.no_grad():\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> is_train = False\\n>>> with torch.set_grad_enabled(is_train):\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> torch.set_grad_enabled(True)  # this can also be used as a function\\n>>> y = x * 2\\n>>> y.requires_grad\\nTrue\\n\\n>>> torch.set_grad_enabled(False)\\n>>> y = x * 2\\n>>> y.requires_grad\\nFalse\\n',\n","   'Id': 359,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/torch.html#utilities'},\n","  {'Answer': '>>> x = torch.tensor([1, 2, 3, 4])\\n>>> torch.unsqueeze(x, 0)\\ntensor([[ 1,  2,  3,  4]])\\n>>> torch.unsqueeze(x, 1)\\ntensor([[ 1],\\n        [ 2],\\n        [ 3],\\n        [ 4]])\\n',\n","   'Id': 360,\n","   'Question': 'How to use torch.unsqueeze, give an example?',\n","   'context': ' Adimvalue within the range[-input.dim()-1,input.dim()+1)can be used. Negativedimwill correspond tounsqueeze()applied atdim=dim+input.dim()+1.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.unsqueeze.html#torch.unsqueeze'},\n","  {'Answer': '>>> x = torch.zeros(1, requires_grad=True)\\n>>> with torch.no_grad():\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> is_train = False\\n>>> with torch.set_grad_enabled(is_train):\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> torch.set_grad_enabled(True)  # this can also be used as a function\\n>>> y = x * 2\\n>>> y.requires_grad\\nTrue\\n\\n>>> torch.set_grad_enabled(False)\\n>>> y = x * 2\\n>>> y.requires_grad\\nFalse\\n',\n","   'Id': 361,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/torch.html#random-sampling'},\n","  {'Answer': '>>> a = torch.eye(10)\\n>>> torch.matrix_rank(a)\\ntensor(10)\\n>>> b = torch.eye(10)\\n>>> b[0, 0] = 0\\n>>> torch.matrix_rank(b)\\ntensor(9)\\n',\n","   'Id': 362,\n","   'Question': 'How to use torch.matrix_rank, give an example?',\n","   'context': ' tolis the threshold below which the singular values (or the eigenvalues\\nwhensymmetricisTrue) are considered to be 0. Iftolis not\\nspecified,tolis set toS.max()*max(S.size())*epswhereSis the\\nsingular values (or the eigenvalues whensymmetricisTrue), andepsis the epsilon value for the datatype ofinput.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.matrix_rank.html#torch.matrix_rank'},\n","  {'Answer': '>>> a = torch.randn(4, 4)\\n>>> a\\ntensor([[ 1.3398,  0.2663, -0.2686,  0.2450],\\n        [-0.7401, -0.8805, -0.3402, -1.1936],\\n        [ 0.4907, -1.3948, -1.0691, -0.3132],\\n        [-1.6092,  0.5419, -0.2993,  0.3195]])\\n>>> torch.argmax(a)\\ntensor(0)\\n',\n","   'Id': 363,\n","   'Question': 'How to use torch.argmax, give an example?',\n","   'context': ' This is the second value returned bytorch.max(). See its\\ndocumentation for the exact semantics of this method.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax'},\n","  {'Answer': '>>> a = torch.randn(4, 4)\\n>>> a\\ntensor([[ 1.3398,  0.2663, -0.2686,  0.2450],\\n        [-0.7401, -0.8805, -0.3402, -1.1936],\\n        [ 0.4907, -1.3948, -1.0691, -0.3132],\\n        [-1.6092,  0.5419, -0.2993,  0.3195]])\\n>>> torch.argmax(a, dim=1)\\ntensor([ 0,  2,  0,  1])\\n',\n","   'Id': 364,\n","   'Question': 'How  This is the second value returned bytorch.max(). See its\\ndocumentation for the exact semantics of this method., give an example?',\n","   'context': ' This is the second value returned bytorch.max(). See its\\ndocumentation for the exact semantics of this method.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax'},\n","  {'Answer': '>>> x=torch.randn(4, dtype=torch.cfloat)\\n>>> x\\ntensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])\\n>>> x.imag\\ntensor([ 0.3553, -0.7896, -0.0633, -0.8119])\\n',\n","   'Id': 365,\n","   'Question': 'How to use torch.imag, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.imag.html#torch.imag'},\n","  {'Answer': '>>> torch.exp(torch.tensor([0, math.log(2.)]))\\ntensor([ 1.,  2.])\\n',\n","   'Id': 366,\n","   'Question': 'How to use torch.exp, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.exp.html#torch.exp'},\n","  {'Answer': '>>> torch.angle(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))*180/3.14159\\ntensor([ 135.,  135,  -45])\\n',\n","   'Id': 367,\n","   'Question': 'How to use torch.angle, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.angle.html#torch.angle'},\n","  {'Answer': '>>> a = torch.arange(-0.5, 1, 0.5)\\n>>> a\\ntensor([-0.5000,  0.0000,  0.5000])\\n>>> torch.special.entr(a)\\ntensor([  -inf, 0.0000, 0.3466])\\n',\n","   'Id': 368,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.exp2'},\n","  {'Answer': '>>> torch.special.erf(torch.tensor([0, -1., 10.]))\\ntensor([ 0.0000, -0.8427,  1.0000])\\n',\n","   'Id': 369,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.exp2'},\n","  {'Answer': '>>> torch.special.erfc(torch.tensor([0, -1., 10.]))\\ntensor([ 1.0000, 1.8427,  0.0000])\\n',\n","   'Id': 370,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.exp2'},\n","  {'Answer': '>>> torch.special.erfinv(torch.tensor([0, 0.5, -1.]))\\ntensor([ 0.0000,  0.4769,    -inf])\\n',\n","   'Id': 371,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.exp2'},\n","  {'Answer': '>>> t = torch.randn(4)\\n>>> t\\ntensor([ 0.9213,  1.0887, -0.8858, -1.7683])\\n>>> torch.special.expit(t)\\ntensor([ 0.7153,  0.7481,  0.2920,  0.1458])\\n',\n","   'Id': 372,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.exp2'},\n","  {'Answer': '>>> torch.special.expm1(torch.tensor([0, math.log(2.)]))\\ntensor([ 0.,  1.])\\n',\n","   'Id': 373,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.exp2'},\n","  {'Answer': '>>> torch.special.exp2(torch.tensor([0, math.log2(2.), 3, 4]))\\ntensor([ 1.,  2.,  8., 16.])\\n',\n","   'Id': 374,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.exp2'},\n","  {'Answer': '>>> a = torch.arange(0.5, 2, 0.5)\\n>>> torch.special.gammaln(a)\\ntensor([ 0.5724,  0.0000, -0.1208])\\n',\n","   'Id': 375,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.exp2'},\n","  {'Answer': '>>> torch.special.i0e(torch.arange(5, dtype=torch.float32))\\ntensor([1.0000, 0.4658, 0.3085, 0.2430, 0.2070])\\n',\n","   'Id': 376,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.exp2'},\n","  {'Answer': '>>> a = torch.rand(5)\\n>>> a\\ntensor([0.2796, 0.9331, 0.6486, 0.1523, 0.6516])\\n>>> torch.special.logit(a, eps=1e-6)\\ntensor([-0.9466,  2.6352,  0.6131, -1.7169,  0.6261])\\n',\n","   'Id': 377,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.exp2'},\n","  {'Answer': \">>> x = torch.zeros(5,)\\n>>> y = torch.tensor([-1, 0, 1, float('inf'), float('nan')])\\n>>> torch.special.xlog1py(x, y)\\ntensor([0., 0., 0., 0., nan])\\n>>> x = torch.tensor([1, 2, 3])\\n>>> y = torch.tensor([3, 2, 1])\\n>>> torch.special.xlog1py(x, y)\\ntensor([1.3863, 2.1972, 2.0794])\\n>>> torch.special.xlog1py(x, 4)\\ntensor([1.6094, 3.2189, 4.8283])\\n>>> torch.special.xlog1py(2, y)\\ntensor([2.7726, 2.1972, 1.3863])\\n\",\n","   'Id': 378,\n","   'Question': 'How  Similar to SciPy’sscipy.special.xlog1py., give an example?',\n","   'context': ' Similar to SciPy’sscipy.special.xlog1py.',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.exp2'},\n","  {'Answer': 'self[index[i][j][k]][j][k] = src[i][j][k]  # if dim == 0\\nself[i][index[i][j][k]][k] = src[i][j][k]  # if dim == 1\\nself[i][j][index[i][j][k]] = src[i][j][k]  # if dim == 2\\n',\n","   'Id': 379,\n","   'Question': 'How to use torch.Tensor.scatter_, give an example?',\n","   'context': ' For a 3-D tensor,selfis updated as:',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_'},\n","  {'Answer': 'self[index[i][j][k]][j][k] *= src[i][j][k]  # if dim == 0\\nself[i][index[i][j][k]][k] *= src[i][j][k]  # if dim == 1\\nself[i][j][index[i][j][k]] *= src[i][j][k]  # if dim == 2\\n',\n","   'Id': 380,\n","   'Question': 'How  Additionally accepts an optionalreduceargument that allows\\nspecification of an optional reduction operation, which is applied to all\\nvalues in the tensorsrcintoselfat the indicies\\nspecified in theindex. For each value insrc, the reduction\\noperation is applied to an index inselfwhich is specified by\\nits index insrcfordimension!=dimand by the corresponding\\nvalue inindexfordimension=dim.Given a 3-D tensor and reduction using the multiplication operation,selfis updated as:, give an example?',\n","   'context': ' Additionally accepts an optionalreduceargument that allows\\nspecification of an optional reduction operation, which is applied to all\\nvalues in the tensorsrcintoselfat the indicies\\nspecified in theindex. For each value insrc, the reduction\\noperation is applied to an index inselfwhich is specified by\\nits index insrcfordimension!=dimand by the corresponding\\nvalue inindexfordimension=dim.Given a 3-D tensor and reduction using the multiplication operation,selfis updated as:',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_'},\n","  {'Answer': \">>> src = torch.arange(1, 11).reshape((2, 5))\\n>>> src\\ntensor([[ 1,  2,  3,  4,  5],\\n        [ 6,  7,  8,  9, 10]])\\n>>> index = torch.tensor([[0, 1, 2, 0]])\\n>>> torch.zeros(3, 5, dtype=src.dtype).scatter_(0, index, src)\\ntensor([[1, 0, 0, 4, 0],\\n        [0, 2, 0, 0, 0],\\n        [0, 0, 3, 0, 0]])\\n>>> index = torch.tensor([[0, 1, 2], [0, 1, 4]])\\n>>> torch.zeros(3, 5, dtype=src.dtype).scatter_(1, index, src)\\ntensor([[1, 2, 3, 0, 0],\\n        [6, 7, 0, 0, 8],\\n        [0, 0, 0, 0, 0]])\\n\\n>>> torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),\\n...            1.23, reduce='multiply')\\ntensor([[2.0000, 2.0000, 2.4600, 2.0000],\\n        [2.0000, 2.0000, 2.0000, 2.4600]])\\n>>> torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),\\n...            1.23, reduce='add')\\ntensor([[2.0000, 2.0000, 3.2300, 2.0000],\\n        [2.0000, 2.0000, 2.0000, 3.2300]])\\n\",\n","   'Id': 381,\n","   'Question': 'How  Reducing with the addition operation is the same as usingscatter_add_()., give an example?',\n","   'context': ' Reducing with the addition operation is the same as usingscatter_add_().',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_'},\n","  {'Answer': '>>> torch.range(1, 4)\\ntensor([ 1.,  2.,  3.,  4.])\\n>>> torch.range(1, 4, 0.5)\\ntensor([ 1.0000,  1.5000,  2.0000,  2.5000,  3.0000,  3.5000,  4.0000])\\n',\n","   'Id': 382,\n","   'Question': 'How to use torch.range, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.range.html#torch.range'},\n","  {'Answer': '>>> x = torch.tensor([1, 2, 3])\\n>>> torch.broadcast_to(x, (3, 3))\\ntensor([[1, 2, 3],\\n        [1, 2, 3],\\n        [1, 2, 3]])\\n',\n","   'Id': 383,\n","   'Question': 'How to use torch.broadcast_to, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.broadcast_to.html#torch.broadcast_to'},\n","  {'Answer': '>>> x = torch.zeros(2, 1, 2, 1, 2)\\n>>> x.size()\\ntorch.Size([2, 1, 2, 1, 2])\\n>>> y = torch.squeeze(x)\\n>>> y.size()\\ntorch.Size([2, 2, 2])\\n>>> y = torch.squeeze(x, 0)\\n>>> y.size()\\ntorch.Size([2, 1, 2, 1, 2])\\n>>> y = torch.squeeze(x, 1)\\n>>> y.size()\\ntorch.Size([2, 2, 1, 2])\\n',\n","   'Id': 384,\n","   'Question': 'How to use torch.squeeze, give an example?',\n","   'context': ' Whendimis given, a squeeze operation is done only in the given\\ndimension. Ifinputis of shape:(A×1×B)(A \\\\times 1 \\\\times B)(A×1×B),squeeze(input,0)leaves the tensor unchanged, butsqueeze(input,1)will squeeze the tensor to the shape(A×B)(A \\\\times B)(A×B).',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.squeeze.html#torch.squeeze'},\n","  {'Answer': 'import torch\\ntorch.manual_seed(0)\\n',\n","   'Id': 385,\n","   'Question': 'How to use You can usetorch.manual_seed()to seed the RNG for all devices (both\\nCPU and CUDA):, give an example?',\n","   'context': ' You can usetorch.manual_seed()to seed the RNG for all devices (both\\nCPU and CUDA):',\n","   'source': 'https://pytorch.org/docs/stable/notes/randomness.html'},\n","  {'Answer': 'import random\\nrandom.seed(0)\\n',\n","   'Id': 386,\n","   'Question': 'How to use For custom operators, you might need to set python seed as well:, give an example?',\n","   'context': ' For custom operators, you might need to set python seed as well:',\n","   'source': 'https://pytorch.org/docs/stable/notes/randomness.html'},\n","  {'Answer': 'import numpy as np\\nnp.random.seed(0)\\n',\n","   'Id': 387,\n","   'Question': 'How to use If you or any of the libraries you are using rely on NumPy, you can seed the global\\nNumPy RNG with:, give an example?',\n","   'context': ' If you or any of the libraries you are using rely on NumPy, you can seed the global\\nNumPy RNG with:',\n","   'source': 'https://pytorch.org/docs/stable/notes/randomness.html'},\n","  {'Answer': '>>> import torch\\n>>> torch.use_deterministic_algorithms(True)\\n>>> torch.randn(2, 2).cuda().index_add_(0, torch.tensor([0, 1]), torch.randn(2, 2))\\nTraceback (most recent call last):\\nFile \"<stdin>\", line 1, in <module>\\nRuntimeError: index_add_cuda_ does not have a deterministic implementation, but you set\\n\\'torch.use_deterministic_algorithms(True)\\'. ...\\n',\n","   'Id': 388,\n","   'Question': 'How to use Please check the documentation fortorch.use_deterministic_algorithms()for a full list of affected operations. If an operation does not act correctly\\naccording to the documentation, or if you need a deterministic implementation\\nof an operation that does not have one, please submit an issue:https://github.com/pytorch/pytorch/issues?q=label:%22topic:%20determinism%22For example, running the nondeterministic CUDA implementation oftorch.Tensor.index_add_()will throw an error:, give an example?',\n","   'context': ' Please check the documentation fortorch.use_deterministic_algorithms()for a full list of affected operations. If an operation does not act correctly\\naccording to the documentation, or if you need a deterministic implementation\\nof an operation that does not have one, please submit an issue:https://github.com/pytorch/pytorch/issues?q=label:%22topic:%20determinism%22For example, running the nondeterministic CUDA implementation oftorch.Tensor.index_add_()will throw an error:',\n","   'source': 'https://pytorch.org/docs/stable/notes/randomness.html'},\n","  {'Answer': \">>> import torch\\n>>> torch.use_deterministic_algorithms(True)\\n>>> torch.bmm(torch.randn(2, 2, 2).to_sparse().cuda(), torch.randn(2, 2, 2).cuda())\\ntensor([[[ 1.1900, -2.3409],\\n         [ 0.4796,  0.8003]],\\n        [[ 0.1509,  1.8027],\\n         [ 0.0333, -1.1444]]], device='cuda:0')\\n\",\n","   'Id': 389,\n","   'Question': 'How to use For example, running the nondeterministic CUDA implementation oftorch.Tensor.index_add_()will throw an error:Whentorch.bmm()is called with sparse-dense CUDA tensors it typically uses a\\nnondeterministic algorithm, but when the deterministic flag is turned on, its alternate\\ndeterministic implementation will be used:, give an example?',\n","   'context': ' For example, running the nondeterministic CUDA implementation oftorch.Tensor.index_add_()will throw an error:Whentorch.bmm()is called with sparse-dense CUDA tensors it typically uses a\\nnondeterministic algorithm, but when the deterministic flag is turned on, its alternate\\ndeterministic implementation will be used:',\n","   'source': 'https://pytorch.org/docs/stable/notes/randomness.html'},\n","  {'Answer': 'def seed_worker(worker_id):\\n    worker_seed = torch.initial_seed() % 2**32\\n    numpy.random.seed(worker_seed)\\n    random.seed(worker_seed)\\n\\ng = torch.Generator()\\ng.manual_seed(0)\\n\\nDataLoader(\\n    train_dataset,\\n    batch_size=batch_size,\\n    num_workers=num_workers,\\n    worker_init_fn=seed_worker\\n    generator=g,\\n)\\n',\n","   'Id': 390,\n","   'Question': 'How to use DataLoader will reseed workers followingRandomness in multi-process data loadingalgorithm.\\nUseworker_init_fn()andgeneratorto preserve reproducibility:, give an example?',\n","   'context': ' DataLoader will reseed workers followingRandomness in multi-process data loadingalgorithm.\\nUseworker_init_fn()andgeneratorto preserve reproducibility:',\n","   'source': 'https://pytorch.org/docs/stable/notes/randomness.html'},\n","  {'Answer': '>>> t = torch.arange(16.0).reshape(2, 2, 4)\\n>>> t\\ntensor([[[ 0.,  1.,  2.,  3.],\\n         [ 4.,  5.,  6.,  7.]],\\n        [[ 8.,  9., 10., 11.],\\n         [12., 13., 14., 15.]]])\\n>>> torch.dsplit(t, 2)\\n(tensor([[[ 0.,  1.],\\n        [ 4.,  5.]],\\n       [[ 8.,  9.],\\n        [12., 13.]]]),\\n tensor([[[ 2.,  3.],\\n          [ 6.,  7.]],\\n         [[10., 11.],\\n          [14., 15.]]]))\\n',\n","   'Id': 391,\n","   'Question': 'How to use torch.dsplit, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.dsplit.html#torch.dsplit'},\n","  {'Answer': '>>> torch.dsplit(t, [3, 6])\\n(tensor([[[ 0.,  1.,  2.],\\n          [ 4.,  5.,  6.]],\\n         [[ 8.,  9., 10.],\\n          [12., 13., 14.]]]),\\n tensor([[[ 3.],\\n          [ 7.]],\\n         [[11.],\\n          [15.]]]),\\n tensor([], size=(2, 2, 0)))\\n',\n","   'Id': 392,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.dsplit.html#torch.dsplit'},\n","  {'Answer': '>>> torch.can_cast(torch.double, torch.float)\\nTrue\\n>>> torch.can_cast(torch.float, torch.int)\\nFalse\\n',\n","   'Id': 393,\n","   'Question': 'How to use torch.can_cast, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.can_cast.html#torch.can_cast'},\n","  {'Answer': '>>> A = torch.randn(3, 3)\\n>>> A\\ntensor([[ 0.0032, -0.2239, -1.1219],\\n        [-0.6690,  0.1161,  0.4053],\\n        [-1.6218, -0.9273, -0.0082]])\\n>>> torch.linalg.det(A)\\ntensor(-0.7576)\\n>>> torch.linalg.logdet(A)\\ntensor(nan)\\n>>> torch.linalg.slogdet(A)\\ntorch.return_types.linalg_slogdet(sign=tensor(-1.), logabsdet=tensor(-0.2776))\\n',\n","   'Id': 394,\n","   'Question': 'How to use torch.linalg.slogdet, give an example?',\n","   'context': ' Supports input of float, double, cfloat and cdouble dtypes.\\nAlso supports batches of matrices, and ifAis a batch of matrices then\\nthe output has the same batch dimensions.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.linalg.slogdet.html#torch.linalg.slogdet'},\n","  {'Answer': 'Q, R = torch.linalg.qr(A)\\n',\n","   'Id': 395,\n","   'Question': 'How to use torch.qr, give an example?',\n","   'context': ' torch.qr()is deprecated in favor oftorch.linalg.qr()and will be removed in a future PyTorch release. The boolean parametersomehas been\\nreplaced with a string parametermode.Q,R=torch.qr(A)should be replaced with',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.qr.html#torch.qr'},\n","  {'Answer': 'Q, R = torch.linalg.qr(A, mode=\"complete\")\\n',\n","   'Id': 396,\n","   'Question': 'How  Q,R=torch.qr(A)should be replaced withQ,R=torch.qr(A,some=False)should be replaced with, give an example?',\n","   'context': ' Q,R=torch.qr(A)should be replaced withQ,R=torch.qr(A,some=False)should be replaced with',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.qr.html#torch.qr'},\n","  {'Answer': '>>> a = torch.tensor([[12., -51, 4], [6, 167, -68], [-4, 24, -41]])\\n>>> q, r = torch.qr(a)\\n>>> q\\ntensor([[-0.8571,  0.3943,  0.3314],\\n        [-0.4286, -0.9029, -0.0343],\\n        [ 0.2857, -0.1714,  0.9429]])\\n>>> r\\ntensor([[ -14.0000,  -21.0000,   14.0000],\\n        [   0.0000, -175.0000,   70.0000],\\n        [   0.0000,    0.0000,  -35.0000]])\\n>>> torch.mm(q, r).round()\\ntensor([[  12.,  -51.,    4.],\\n        [   6.,  167.,  -68.],\\n        [  -4.,   24.,  -41.]])\\n>>> torch.mm(q.t(), q).round()\\ntensor([[ 1.,  0.,  0.],\\n        [ 0.,  1., -0.],\\n        [ 0., -0.,  1.]])\\n>>> a = torch.randn(3, 4, 5)\\n>>> q, r = torch.qr(a, some=False)\\n>>> torch.allclose(torch.matmul(q, r), a)\\nTrue\\n>>> torch.allclose(torch.matmul(q.transpose(-2, -1), q), torch.eye(5))\\nTrue\\n',\n","   'Id': 397,\n","   'Question': 'How  IfsomeisTrue, then this function returns the thin (reduced) QR factorization.\\nOtherwise, ifsomeisFalse, this function returns the complete QR factorization., give an example?',\n","   'context': ' IfsomeisTrue, then this function returns the thin (reduced) QR factorization.\\nOtherwise, ifsomeisFalse, this function returns the complete QR factorization.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.qr.html#torch.qr'},\n","  {'Answer': \">>> x = torch.tensor([float('nan'), float('inf'), -float('inf'), 3.14])\\n>>> torch.nan_to_num(x)\\ntensor([ 0.0000e+00,  3.4028e+38, -3.4028e+38,  3.1400e+00])\\n>>> torch.nan_to_num(x, nan=2.0)\\ntensor([ 2.0000e+00,  3.4028e+38, -3.4028e+38,  3.1400e+00])\\n>>> torch.nan_to_num(x, nan=2.0, posinf=1.0)\\ntensor([ 2.0000e+00,  1.0000e+00, -3.4028e+38,  3.1400e+00])\\n\",\n","   'Id': 398,\n","   'Question': 'How to use torch.nan_to_num, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.nan_to_num.html#torch.nan_to_num'},\n","  {'Answer': \">>> x = torch.tensor([ 0.3810,  1.2774, -0.2972, -0.3719,  0.4637])\\n>>> torch.div(x, 0.5)\\ntensor([ 0.7620,  2.5548, -0.5944, -0.7438,  0.9274])\\n\\n>>> a = torch.tensor([[-0.3711, -1.9353, -0.4605, -0.2917],\\n...                   [ 0.1815, -1.0111,  0.9805, -1.5923],\\n...                   [ 0.1062,  1.4581,  0.7759, -1.2344],\\n...                   [-0.1830, -0.0313,  1.1908, -1.4757]])\\n>>> b = torch.tensor([ 0.8032,  0.2930, -0.8113, -0.2308])\\n>>> torch.div(a, b)\\ntensor([[-0.4620, -6.6051,  0.5676,  1.2639],\\n        [ 0.2260, -3.4509, -1.2086,  6.8990],\\n        [ 0.1322,  4.9764, -0.9564,  5.3484],\\n        [-0.2278, -0.1068, -1.4678,  6.3938]])\\n\\n>>> torch.div(a, b, rounding_mode='trunc')\\ntensor([[-0., -6.,  0.,  1.],\\n        [ 0., -3., -1.,  6.],\\n        [ 0.,  4., -0.,  5.],\\n        [-0., -0., -1.,  6.]])\\n\\n>>> torch.div(a, b, rounding_mode='floor')\\ntensor([[-1., -7.,  0.,  1.],\\n        [ 0., -4., -2.,  6.],\\n        [ 0.,  4., -1.,  5.],\\n        [-1., -1., -2.,  6.]])\\n\",\n","   'Id': 399,\n","   'Question': 'How to use torch.div, give an example?',\n","   'context': ' Supportsbroadcasting to a common shape,type promotion, and integer, float, and complex inputs.\\nAlways promotes integer types to the default scalar type.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.div.html#torch.div'},\n","  {'Answer': 'import torch\\n\\ndef foo(x, y):\\n    return 2 * x + y\\n\\ntraced_foo = torch.jit.trace(foo, (torch.rand(3), torch.rand(3)))\\n\\n@torch.jit.script\\ndef bar(x):\\n    return traced_foo(x, x)\\n',\n","   'Id': 400,\n","   'Question': 'How to use Mixing Tracing and Scripting, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/jit.html'},\n","  {'Answer': 'import torch\\n\\n@torch.jit.script\\ndef foo(x, y):\\n    if x.max() > y.max():\\n        r = x\\n    else:\\n        r = y\\n    return r\\n\\n\\ndef bar(x, y, z):\\n    return foo(x, y) + z\\n\\ntraced_bar = torch.jit.trace(bar, (torch.rand(3), torch.rand(3), torch.rand(3)))\\n',\n","   'Id': 401,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/jit.html'},\n","  {'Answer': 'import torch\\nimport torchvision\\n\\nclass MyScriptModule(torch.nn.Module):\\n    def __init__(self):\\n        super(MyScriptModule, self).__init__()\\n        self.means = torch.nn.Parameter(torch.tensor([103.939, 116.779, 123.68])\\n                                        .resize_(1, 3, 1, 1))\\n        self.resnet = torch.jit.trace(torchvision.models.resnet18(),\\n                                      torch.rand(1, 3, 224, 224))\\n\\n    def forward(self, input):\\n        return self.resnet(input - self.means)\\n\\nmy_script_module = torch.jit.script(MyScriptModule())\\n',\n","   'Id': 402,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/jit.html'},\n","  {'Answer': '@torch.jit.script\\ndef scripted_fn(x : torch.Tensor):\\n    for i in range(12):\\n        x = x + x\\n    return x\\n\\ndef fn(x):\\n    x = torch.neg(x)\\n    import pdb; pdb.set_trace()\\n    return scripted_fn(x)\\n\\ntraced_fn = torch.jit.trace(fn, (torch.rand(4, 5),))\\ntraced_fn(torch.rand(3, 4))\\n',\n","   'Id': 403,\n","   'Question': 'How to use Setting the environment variablePYTORCH_JIT=0will disable all script\\nand tracing annotations. If there is hard-to-debug error in one of your\\nTorchScript models, you can use this flag to force everything to run using native\\nPython. Since TorchScript (scripting and tracing) is disabled with this flag,\\nyou can use tools likepdbto debug the model code.  For example:, give an example?',\n","   'context': ' Setting the environment variablePYTORCH_JIT=0will disable all script\\nand tracing annotations. If there is hard-to-debug error in one of your\\nTorchScript models, you can use this flag to force everything to run using native\\nPython. Since TorchScript (scripting and tracing) is disabled with this flag,\\nyou can use tools likepdbto debug the model code.  For example:',\n","   'source': 'https://pytorch.org/docs/stable/jit.html'},\n","  {'Answer': '$ PYTORCH_JIT=0 python disable_jit_example.py\\n',\n","   'Id': 404,\n","   'Question': 'How to use Setting the environment variablePYTORCH_JIT=0will disable all script\\nand tracing annotations. If there is hard-to-debug error in one of your\\nTorchScript models, you can use this flag to force everything to run using native\\nPython. Since TorchScript (scripting and tracing) is disabled with this flag,\\nyou can use tools likepdbto debug the model code.  For example:Debugging this script withpdbworks except for when we invoke the@torch.jit.scriptfunction. We can globally disable\\nJIT, so that we can call the@torch.jit.scriptfunction as a normal Python function and not compile it. If the above script\\nis calleddisable_jit_example.py, we can invoke it like so:, give an example?',\n","   'context': ' Debugging this script withpdbworks except for when we invoke the@torch.jit.scriptfunction. We can globally disable\\nJIT, so that we can call the@torch.jit.scriptfunction as a normal Python function and not compile it. If the above script\\nis calleddisable_jit_example.py, we can invoke it like so:',\n","   'source': 'https://pytorch.org/docs/stable/jit.html'},\n","  {'Answer': '@torch.jit.script\\ndef foo(len):\\n    # type: (int) -> torch.Tensor\\n    rv = torch.zeros(3, 4)\\n    for i in range(len):\\n        if i < 10:\\n            rv = rv - 1.0\\n        else:\\n            rv = rv + 1.0\\n    return rv\\n\\nprint(foo.code)\\n',\n","   'Id': 405,\n","   'Question': 'How to use Inspecting Code, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/jit.html'},\n","  {'Answer': 'def foo(len: int) -> Tensor:\\n    rv = torch.zeros([3, 4], dtype=None, layout=None, device=None, pin_memory=None)\\n    rv0 = rv\\n    for i in range(len):\\n        if torch.lt(i, 10):\\n            rv1 = torch.sub(rv0, 1., 1)\\n        else:\\n            rv1 = torch.add(rv0, 1., 1)\\n        rv0 = rv1\\n    return rv0\\n',\n","   'Id': 406,\n","   'Question': 'How to use TorchScript provides a code pretty-printer for allScriptModuleinstances. This\\npretty-printer gives an interpretation of the script method’s code as valid\\nPython syntax. For example:AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule’s code.\\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\\nThe example above produces this output:, give an example?',\n","   'context': ' AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule’s code.\\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\\nThe example above produces this output:',\n","   'source': 'https://pytorch.org/docs/stable/jit.html'},\n","  {'Answer': '@torch.jit.script\\ndef foo(len):\\n    # type: (int) -> torch.Tensor\\n    rv = torch.zeros(3, 4)\\n    for i in range(len):\\n        if i < 10:\\n            rv = rv - 1.0\\n        else:\\n            rv = rv + 1.0\\n    return rv\\n\\nprint(foo.graph)\\n',\n","   'Id': 407,\n","   'Question': 'How to use Interpreting Graphs, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/jit.html'},\n","  {'Answer': 'graph(%len.1 : int):\\n  %24 : int = prim::Constant[value=1]()\\n  %17 : bool = prim::Constant[value=1]() # test.py:10:5\\n  %12 : bool? = prim::Constant()\\n  %10 : Device? = prim::Constant()\\n  %6 : int? = prim::Constant()\\n  %1 : int = prim::Constant[value=3]() # test.py:9:22\\n  %2 : int = prim::Constant[value=4]() # test.py:9:25\\n  %20 : int = prim::Constant[value=10]() # test.py:11:16\\n  %23 : float = prim::Constant[value=1]() # test.py:12:23\\n  %4 : int[] = prim::ListConstruct(%1, %2)\\n  %rv.1 : Tensor = aten::zeros(%4, %6, %6, %10, %12) # test.py:9:10\\n  %rv : Tensor = prim::Loop(%len.1, %17, %rv.1) # test.py:10:5\\n    block0(%i.1 : int, %rv.14 : Tensor):\\n      %21 : bool = aten::lt(%i.1, %20) # test.py:11:12\\n      %rv.13 : Tensor = prim::If(%21) # test.py:11:9\\n        block0():\\n          %rv.3 : Tensor = aten::sub(%rv.14, %23, %24) # test.py:12:18\\n          -> (%rv.3)\\n        block1():\\n          %rv.6 : Tensor = aten::add(%rv.14, %23, %24) # test.py:14:18\\n          -> (%rv.6)\\n      -> (%17, %rv.13)\\n  return (%rv)\\n',\n","   'Id': 408,\n","   'Question': 'How to use graphfollows the same rules described in theInspecting Codesection\\nwith regard toforwardmethod lookup.The example script above produces the graph:, give an example?',\n","   'context': ' graphfollows the same rules described in theInspecting Codesection\\nwith regard toforwardmethod lookup.The example script above produces the graph:',\n","   'source': 'https://pytorch.org/docs/stable/jit.html'},\n","  {'Answer': 'def loop_in_traced_fn(x):\\n    result = x[0]\\n    for i in range(x.size(0)):\\n        result = result * x[i]\\n    return result\\n\\ninputs = (torch.rand(3, 4, 5),)\\ncheck_inputs = [(torch.rand(4, 5, 6),), (torch.rand(2, 3, 4),)]\\n\\ntraced = torch.jit.trace(loop_in_traced_fn, inputs, check_inputs=check_inputs)\\n',\n","   'Id': 409,\n","   'Question': 'How to use One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\\nof inputs that will be used to re-trace the computation and verify the\\nresults. For example:, give an example?',\n","   'context': ' One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\\nof inputs that will be used to re-trace the computation and verify the\\nresults. For example:',\n","   'source': 'https://pytorch.org/docs/stable/jit.html'},\n","  {'Answer': 'ERROR: Graphs differed across invocations!\\nGraph diff:\\n\\n            graph(%x : Tensor) {\\n            %1 : int = prim::Constant[value=0]()\\n            %2 : int = prim::Constant[value=0]()\\n            %result.1 : Tensor = aten::select(%x, %1, %2)\\n            %4 : int = prim::Constant[value=0]()\\n            %5 : int = prim::Constant[value=0]()\\n            %6 : Tensor = aten::select(%x, %4, %5)\\n            %result.2 : Tensor = aten::mul(%result.1, %6)\\n            %8 : int = prim::Constant[value=0]()\\n            %9 : int = prim::Constant[value=1]()\\n            %10 : Tensor = aten::select(%x, %8, %9)\\n        -   %result : Tensor = aten::mul(%result.2, %10)\\n        +   %result.3 : Tensor = aten::mul(%result.2, %10)\\n        ?          ++\\n            %12 : int = prim::Constant[value=0]()\\n            %13 : int = prim::Constant[value=2]()\\n            %14 : Tensor = aten::select(%x, %12, %13)\\n        +   %result : Tensor = aten::mul(%result.3, %14)\\n        +   %16 : int = prim::Constant[value=0]()\\n        +   %17 : int = prim::Constant[value=3]()\\n        +   %18 : Tensor = aten::select(%x, %16, %17)\\n        -   %15 : Tensor = aten::mul(%result, %14)\\n        ?     ^                                 ^\\n        +   %19 : Tensor = aten::mul(%result, %18)\\n        ?     ^                                 ^\\n        -   return (%15);\\n        ?             ^\\n        +   return (%19);\\n        ?             ^\\n            }\\n',\n","   'Id': 410,\n","   'Question': 'How to use One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\\nof inputs that will be used to re-trace the computation and verify the\\nresults. For example:Gives us the following diagnostic information:, give an example?',\n","   'context': ' Gives us the following diagnostic information:',\n","   'source': 'https://pytorch.org/docs/stable/jit.html'},\n","  {'Answer': 'def fn(x):\\n    result = x[0]\\n    for i in range(x.size(0)):\\n        result = result * x[i]\\n    return result\\n\\ninputs = (torch.rand(3, 4, 5),)\\ncheck_inputs = [(torch.rand(4, 5, 6),), (torch.rand(2, 3, 4),)]\\n\\nscripted_fn = torch.jit.script(fn)\\nprint(scripted_fn.graph)\\n#print(str(scripted_fn.graph).strip())\\n\\nfor input_tuple in [inputs] + check_inputs:\\n    torch.testing.assert_allclose(fn(*input_tuple), scripted_fn(*input_tuple))\\n',\n","   'Id': 411,\n","   'Question': 'How to use  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/jit.html'},\n","  {'Answer': 'graph(%x : Tensor) {\\n    %5 : bool = prim::Constant[value=1]()\\n    %1 : int = prim::Constant[value=0]()\\n    %result.1 : Tensor = aten::select(%x, %1, %1)\\n    %4 : int = aten::size(%x, %1)\\n    %result : Tensor = prim::Loop(%4, %5, %result.1)\\n    block0(%i : int, %7 : Tensor) {\\n        %10 : Tensor = aten::select(%x, %1, %i)\\n        %result.2 : Tensor = aten::mul(%7, %10)\\n        -> (%5, %result.2)\\n    }\\n    return (%result);\\n}\\n',\n","   'Id': 412,\n","   'Question': 'How to use In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead:Which produces:, give an example?',\n","   'context': ' In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead:Which produces:',\n","   'source': 'https://pytorch.org/docs/stable/jit.html'},\n","  {'Answer': 'def fill_row_zero(x):\\n    x[0] = torch.rand(*x.shape[1:2])\\n    return x\\n\\ntraced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),))\\nprint(traced.graph)\\n',\n","   'Id': 413,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/jit.html'},\n","  {'Answer': 'fill_row_zero.py:4: TracerWarning: There are 2 live references to the data region being modified when tracing in-place operator copy_ (possibly due to an assignment). This might cause the trace to be incorrect, because all other views that also reference this data will not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe.\\n    x[0] = torch.rand(*x.shape[1:2])\\nfill_row_zero.py:6: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\\nNot within tolerance rtol=1e-05 atol=1e-05 at input[0, 1] (0.09115803241729736 vs. 0.6782537698745728) and 3 other locations (33.00%)\\n    traced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),))\\ngraph(%0 : Float(3, 4)) {\\n    return (%0);\\n}\\n',\n","   'Id': 414,\n","   'Question': 'How to use The tracer produces warnings for several problematic patterns in traced\\ncomputation. As an example, take a trace of a function that contains an\\nin-place assignment on a slice (a view) of a Tensor:Produces several warnings and a graph which simply returns the input:, give an example?',\n","   'context': ' Produces several warnings and a graph which simply returns the input:',\n","   'source': 'https://pytorch.org/docs/stable/jit.html'},\n","  {'Answer': 'def fill_row_zero(x):\\n    x = torch.cat((torch.rand(1, *x.shape[1:2]), x[1:2]), dim=0)\\n    return x\\n\\ntraced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),))\\nprint(traced.graph)\\n',\n","   'Id': 415,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/jit.html'},\n","  {'Answer': 'cpu_model = gpu_model.cpu()\\nsample_input_cpu = sample_input_gpu.cpu()\\ntraced_cpu = torch.jit.trace(cpu_model, sample_input_cpu)\\ntorch.jit.save(traced_cpu, \"cpu.pt\")\\n\\ntraced_gpu = torch.jit.trace(gpu_model, sample_input_gpu)\\ntorch.jit.save(traced_gpu, \"gpu.pt\")\\n\\n# ... later, when using the model:\\n\\nif use_gpu:\\n  model = torch.jit.load(\"gpu.pt\")\\nelse:\\n  model = torch.jit.load(\"cpu.pt\")\\n\\nmodel(input)\\n',\n","   'Id': 416,\n","   'Question': 'How to use First convert your model from GPU to CPU and then save it, like so:, give an example?',\n","   'context': ' First convert your model from GPU to CPU and then save it, like so:',\n","   'source': 'https://pytorch.org/docs/stable/jit.html'},\n","  {'Answer': 'import torch\\n\\nclass Model(torch.nn.Module):\\n    def __init__(self):\\n        super(Model, self).__init__()\\n        self.x = 2\\n\\n    def forward(self):\\n        return self.x\\n\\nm = torch.jit.script(Model())\\n',\n","   'Id': 417,\n","   'Question': 'How to use Frequently Asked Questions, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/jit.html'},\n","  {'Answer': 'import torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\nclass Model(nn.Module):\\n    def __init__(self):\\n        super(Model, self).__init__()\\n        self.conv1 = nn.Conv2d(1, 20, 5)\\n        self.conv2 = nn.Conv2d(20, 20, 5)\\n\\n    def forward(self, x):\\n        x = F.relu(self.conv1(x))\\n        return F.relu(self.conv2(x))\\n\\nmy_model = Model()\\nmy_scripted_model = torch.jit.script(my_model)\\n',\n","   'Id': 418,\n","   'Question': 'How to use Migrating to PyTorch 1 2 Recursive Scripting API, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/jit.html'},\n","  {'Answer': \"import torch\\nimport torch.nn as nn\\n\\nclass MyModule(nn.Module):\\n    def implicitly_compiled_method(self, x):\\n        return x + 99\\n\\n    # `forward` is implicitly decorated with `@torch.jit.export`,\\n    # so adding it here would have no effect\\n    def forward(self, x):\\n        return x + 10\\n\\n    @torch.jit.export\\n    def another_forward(self, x):\\n        # When the compiler sees this call, it will compile\\n        # `implicitly_compiled_method`\\n        return self.implicitly_compiled_method(x)\\n\\n    def unused_method(self, x):\\n        return x - 20\\n\\n# `m` will contain compiled methods:\\n#     `forward`\\n#     `another_forward`\\n#     `implicitly_compiled_method`\\n# `unused_method` will not be compiled since it was not called from\\n# any compiled methods and wasn't decorated with `@torch.jit.export`\\nm = torch.jit.script(MyModule())\\n\",\n","   'Id': 419,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/jit.html'},\n","  {'Answer': \"# Same behavior as pre-PyTorch 1.2\\n@torch.jit.script\\ndef some_fn():\\n    return 2\\n\\n# Marks a function as ignored, if nothing\\n# ever calls it then this has no effect\\n@torch.jit.ignore\\ndef some_fn2():\\n    return 2\\n\\n# As with ignore, if nothing calls it then it has no effect.\\n# If it is called in script it is replaced with an exception.\\n@torch.jit.unused\\ndef some_fn3():\\n  import pdb; pdb.set_trace()\\n  return 4\\n\\n# Doesn't do anything, this function is already\\n# the main entry point\\n@torch.jit.export\\ndef some_fn4():\\n    return 2\\n\",\n","   'Id': 420,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/jit.html'},\n","  {'Answer': 'from typing import Dict\\nimport torch\\n\\nclass MyModule(torch.jit.ScriptModule):\\n    def __init__(self):\\n        super(MyModule, self).__init__()\\n        self.my_dict = torch.jit.Attribute({}, Dict[str, int])\\n        self.my_int = torch.jit.Attribute(20, int)\\n\\nm = MyModule()\\n',\n","   'Id': 421,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/jit.html'},\n","  {'Answer': 'from typing import Dict\\n\\nclass MyModule(torch.nn.Module):\\n    my_dict: Dict[str, int]\\n\\n    def __init__(self):\\n        super(MyModule, self).__init__()\\n        # This type cannot be inferred and must be specified\\n        self.my_dict = {}\\n\\n        # The attribute type here is inferred to be `int`\\n        self.my_int = 20\\n\\n    def forward(self):\\n        pass\\n\\nm = torch.jit.script(MyModule())\\n',\n","   'Id': 422,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/jit.html'},\n","  {'Answer': \"class MyModule(torch.jit.ScriptModule):\\n    __constants__ = ['my_constant']\\n\\n    def __init__(self):\\n        super(MyModule, self).__init__()\\n        self.my_constant = 2\\n\\n    def forward(self):\\n        pass\\nm = MyModule()\\n\",\n","   'Id': 423,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/jit.html'},\n","  {'Answer': \"try:\\n    from typing_extensions import Final\\nexcept:\\n    # If you don't have `typing_extensions` installed, you can use a\\n    # polyfill from `torch.jit`.\\n    from torch.jit import Final\\n\\nclass MyModule(torch.nn.Module):\\n\\n    my_constant: Final[int]\\n\\n    def __init__(self):\\n        super(MyModule, self).__init__()\\n        self.my_constant = 2\\n\\n    def forward(self):\\n        pass\\n\\nm = torch.jit.script(MyModule())\\n\",\n","   'Id': 424,\n","   'Question': 'How to use Old API:New API:, give an example?',\n","   'context': ' Old API:New API:',\n","   'source': 'https://pytorch.org/docs/stable/jit.html'},\n","  {'Answer': \"import torch\\nfrom typing import Dict, Optional\\n\\n@torch.jit.script\\ndef make_dict(flag: bool):\\n    x: Dict[str, int] = {}\\n    x['hi'] = 2\\n    b: Optional[int] = None\\n    if flag:\\n        b = 2\\n    return x, b\\n\",\n","   'Id': 425,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/jit.html'},\n","  {'Answer': '>>> a = torch.tensor([1, 2, 3])\\n>>> b = torch.tensor([4, 5, 6])\\n>>> torch.column_stack((a, b))\\ntensor([[1, 4],\\n    [2, 5],\\n    [3, 6]])\\n>>> a = torch.arange(5)\\n>>> b = torch.arange(10).reshape(5, 2)\\n>>> torch.column_stack((a, b, b))\\ntensor([[0, 0, 1, 0, 1],\\n        [1, 2, 3, 2, 3],\\n        [2, 4, 5, 4, 5],\\n        [3, 6, 7, 6, 7],\\n        [4, 8, 9, 8, 9]])\\n',\n","   'Id': 426,\n","   'Question': 'How to use torch.column_stack, give an example?',\n","   'context': ' Equivalent totorch.hstack(tensors), except each zero or one dimensional tensortintensorsis first reshaped into a(t.numel(),1)column before being stacked horizontally.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.column_stack.html#torch.column_stack'},\n","  {'Answer': '>>> x=torch.tensor([1,2,3])\\n>>> torch.is_tensor(x)\\nTrue\\n',\n","   'Id': 427,\n","   'Question': 'How to use torch.is_tensor, give an example?',\n","   'context': ' Note that this function is simply doingisinstance(obj,Tensor).\\nUsing thatisinstancecheck is better for typechecking with mypy,\\nand more explicit - so it’s recommended to use that instead ofis_tensor.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.is_tensor.html'},\n","  {'Answer': '>>> torch.set_flush_denormal(True)\\nTrue\\n>>> torch.tensor([1e-323], dtype=torch.float64)\\ntensor([ 0.], dtype=torch.float64)\\n>>> torch.set_flush_denormal(False)\\nTrue\\n>>> torch.tensor([1e-323], dtype=torch.float64)\\ntensor(9.88131e-324 *\\n       [ 1.0000], dtype=torch.float64)\\n',\n","   'Id': 428,\n","   'Question': 'How to use torch.set_flush_denormal, give an example?',\n","   'context': ' ReturnsTrueif your system supports flushing denormal numbers and it\\nsuccessfully configures flush denormal mode.set_flush_denormal()is only supported on x86 architectures supporting SSE3.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.set_flush_denormal.html#torch.set_flush_denormal'},\n","  {'Answer': \"# trace\\n>>> torch.einsum('ii', torch.randn(4, 4))\\ntensor(-1.2104)\\n\\n# diagonal\\n>>> torch.einsum('ii->i', torch.randn(4, 4))\\ntensor([-0.1034,  0.7952, -0.2433,  0.4545])\\n\\n# outer product\\n>>> x = torch.randn(5)\\n>>> y = torch.randn(4)\\n>>> torch.einsum('i,j->ij', x, y)\\ntensor([[ 0.1156, -0.2897, -0.3918,  0.4963],\\n        [-0.3744,  0.9381,  1.2685, -1.6070],\\n        [ 0.7208, -1.8058, -2.4419,  3.0936],\\n        [ 0.1713, -0.4291, -0.5802,  0.7350],\\n        [ 0.5704, -1.4290, -1.9323,  2.4480]])\\n\\n# batch matrix multiplication\\n>>> As = torch.randn(3,2,5)\\n>>> Bs = torch.randn(3,5,4)\\n>>> torch.einsum('bij,bjk->bik', As, Bs)\\ntensor([[[-1.0564, -1.5904,  3.2023,  3.1271],\\n        [-1.6706, -0.8097, -0.8025, -2.1183]],\\n\\n        [[ 4.2239,  0.3107, -0.5756, -0.2354],\\n        [-1.4558, -0.3460,  1.5087, -0.8530]],\\n\\n        [[ 2.8153,  1.8787, -4.3839, -1.2112],\\n        [ 0.3728, -2.1131,  0.0921,  0.8305]]])\\n\\n# batch permute\\n>>> A = torch.randn(2, 3, 4, 5)\\n>>> torch.einsum('...ij->...ji', A).shape\\ntorch.Size([2, 3, 5, 4])\\n\\n# equivalent to torch.nn.functional.bilinear\\n>>> A = torch.randn(3,5,4)\\n>>> l = torch.randn(2,5)\\n>>> r = torch.randn(2,4)\\n>>> torch.einsum('bn,anm,bm->ba', l, A, r)\\ntensor([[-0.3430, -5.2405,  0.4494],\\n        [ 0.3311,  5.5201, -3.0356]])\\n\",\n","   'Id': 429,\n","   'Question': 'How to use torch.einsum, give an example?',\n","   'context': ' Equation:',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum'},\n","  {'Answer': '>>> start = torch.arange(1., 5.)\\n>>> end = torch.empty(4).fill_(10)\\n>>> start\\ntensor([ 1.,  2.,  3.,  4.])\\n>>> end\\ntensor([ 10.,  10.,  10.,  10.])\\n>>> torch.lerp(start, end, 0.5)\\ntensor([ 5.5000,  6.0000,  6.5000,  7.0000])\\n>>> torch.lerp(start, end, torch.full_like(start, 0.5))\\ntensor([ 5.5000,  6.0000,  6.5000,  7.0000])\\n',\n","   'Id': 430,\n","   'Question': 'How to use torch.lerp, give an example?',\n","   'context': ' The shapes ofstartandendmust bebroadcastable. Ifweightis a tensor, then\\nthe shapes ofweight,start, andendmust bebroadcastable.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.lerp.html#torch.lerp'},\n","  {'Answer': '>>> a = torch.tensor((1, 2, -1))\\n>>> b = torch.tensor((3, 0, 4))\\n>>> torch.maximum(a, b)\\ntensor([3, 2, 4])\\n',\n","   'Id': 431,\n","   'Question': 'How to use torch.maximum, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.maximum.html#torch.maximum'},\n","  {'Answer': \">>> import torch\\n>>> a = torch.arange(9, dtype= torch.float) - 4\\n>>> b = a.reshape((3, 3))\\n>>> torch.norm(a)\\ntensor(7.7460)\\n>>> torch.norm(b)\\ntensor(7.7460)\\n>>> torch.norm(a, float('inf'))\\ntensor(4.)\\n>>> torch.norm(b, float('inf'))\\ntensor(4.)\\n>>> c = torch.tensor([[ 1, 2, 3],[-1, 1, 4]] , dtype= torch.float)\\n>>> torch.norm(c, dim=0)\\ntensor([1.4142, 2.2361, 5.0000])\\n>>> torch.norm(c, dim=1)\\ntensor([3.7417, 4.2426])\\n>>> torch.norm(c, p=1, dim=1)\\ntensor([6., 6.])\\n>>> d = torch.arange(8, dtype= torch.float).reshape(2,2,2)\\n>>> torch.norm(d, dim=(1,2))\\ntensor([ 3.7417, 11.2250])\\n>>> torch.norm(d[0, :, :]), torch.norm(d[1, :, :])\\n(tensor(3.7417), tensor(11.2250))\\n\",\n","   'Id': 432,\n","   'Question': 'How to use torch.norm, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm'},\n","  {'Answer': '>>> torch.zeros(2, 3)\\ntensor([[ 0.,  0.,  0.],\\n        [ 0.,  0.,  0.]])\\n\\n>>> torch.zeros(5)\\ntensor([ 0.,  0.,  0.,  0.,  0.])\\n',\n","   'Id': 433,\n","   'Question': 'How to use torch.zeros, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros'},\n","  {'Answer': '>>> a = torch.rand(1, 2).bool()\\n>>> a\\ntensor([[False, True]], dtype=torch.bool)\\n>>> torch.any(a)\\ntensor(True, dtype=torch.bool)\\n>>> a = torch.arange(0, 3)\\n>>> a\\ntensor([0, 1, 2])\\n>>> torch.any(a)\\ntensor(True)\\n',\n","   'Id': 434,\n","   'Question': 'How to use torch.any, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.any.html#torch.any'},\n","  {'Answer': '>>> a = torch.randn(4, 2) < 0\\n>>> a\\ntensor([[ True,  True],\\n        [False,  True],\\n        [ True,  True],\\n        [False, False]])\\n>>> torch.any(a, 1)\\ntensor([ True,  True,  True, False])\\n>>> torch.any(a, 0)\\ntensor([True, True])\\n',\n","   'Id': 435,\n","   'Question': 'How  IfkeepdimisTrue, the output tensor is of the same size\\nasinputexcept in the dimensiondimwhere it is of size 1.\\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\\nthe output tensor having 1 fewer dimension thaninput., give an example?',\n","   'context': ' IfkeepdimisTrue, the output tensor is of the same size\\nasinputexcept in the dimensiondimwhere it is of size 1.\\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\\nthe output tensor having 1 fewer dimension thaninput.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.any.html#torch.any'},\n","  {'Answer': '>>> A = torch.randn(2, 3, 3)\\n>>> b = torch.randn(2, 3, 1)\\n>>> A_LU = torch.lu(A)\\n>>> x = torch.lu_solve(b, *A_LU)\\n>>> torch.norm(torch.bmm(A, x) - b)\\ntensor(1.00000e-07 *\\n       2.8312)\\n',\n","   'Id': 436,\n","   'Question': 'How to use torch.lu_solve, give an example?',\n","   'context': ' This function supportsfloat,double,cfloatandcdoubledtypes forinput.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.lu_solve.html#torch.lu_solve'},\n","  {'Answer': '>>> a = torch.randn(4, 4)\\n>>> a\\ntensor([[ 0.0785,  1.5267, -0.8521,  0.4065],\\n        [ 0.1598,  0.0788, -0.0745, -1.2700],\\n        [ 1.2208,  1.0722, -0.7064,  1.2564],\\n        [ 0.0669, -0.2318, -0.8229, -0.9280]])\\n\\n\\n>>> torch.argsort(a, dim=1)\\ntensor([[2, 0, 3, 1],\\n        [3, 2, 1, 0],\\n        [2, 1, 0, 3],\\n        [3, 2, 1, 0]])\\n',\n","   'Id': 437,\n","   'Question': 'How to use torch.argsort, give an example?',\n","   'context': ' This is the second value returned bytorch.sort().  See its documentation\\nfor the exact semantics of this method.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.argsort.html#torch.argsort'},\n","  {'Answer': '>>> a = torch.randn(10)\\n>>> a\\ntensor([-0.3449, -1.5447,  0.0685, -1.5104, -1.1706,  0.2259,  1.4696, -1.3284,\\n     1.9946, -0.8209])\\n>>> torch.cummax(a, dim=0)\\ntorch.return_types.cummax(\\n    values=tensor([-0.3449, -0.3449,  0.0685,  0.0685,  0.0685,  0.2259,  1.4696,  1.4696,\\n     1.9946,  1.9946]),\\n    indices=tensor([0, 0, 2, 2, 2, 5, 6, 6, 8, 8]))\\n',\n","   'Id': 438,\n","   'Question': 'How to use torch.cummax, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.cummax.html#torch.cummax'},\n","  {'Answer': 'out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0\\nout[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1\\nout[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2\\n',\n","   'Id': 439,\n","   'Question': 'How to use torch.gather, give an example?',\n","   'context': ' For a 3-D tensor the output is specified by:',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.gather.html#torch.gather'},\n","  {'Answer': '>>> t = torch.tensor([[1, 2], [3, 4]])\\n>>> torch.gather(t, 1, torch.tensor([[0, 0], [1, 0]]))\\ntensor([[ 1,  1],\\n        [ 4,  3]])\\n',\n","   'Id': 440,\n","   'Question': 'How  inputandindexmust have the same number of dimensions.\\nIt is also required thatindex.size(d)<=input.size(d)for all\\ndimensionsd!=dim.outwill have the same shape asindex.\\nNote thatinputandindexdo not broadcast against each other., give an example?',\n","   'context': ' inputandindexmust have the same number of dimensions.\\nIt is also required thatindex.size(d)<=input.size(d)for all\\ndimensionsd!=dim.outwill have the same shape asindex.\\nNote thatinputandindexdo not broadcast against each other.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.gather.html#torch.gather'},\n","  {'Answer': '>>> torch.isreal(torch.tensor([1, 1+1j, 2+0j]))\\ntensor([True, False, True])\\n',\n","   'Id': 441,\n","   'Question': 'How to use torch.isreal, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.isreal.html#torch.isreal'},\n","  {'Answer': \">>> gain = nn.init.calculate_gain('leaky_relu', 0.2)  # leaky_relu with negative_slope=0.2\\n\",\n","   'Id': 442,\n","   'Question': 'How to use torch.nn.init.calculate_gain, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/nn.init.html'},\n","  {'Answer': '>>> w = torch.empty(3, 5)\\n>>> nn.init.uniform_(w)\\n',\n","   'Id': 443,\n","   'Question': 'How to use torch.nn.init.uniform_, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/nn.init.html'},\n","  {'Answer': '>>> w = torch.empty(3, 5)\\n>>> nn.init.normal_(w)\\n',\n","   'Id': 444,\n","   'Question': 'How to use torch.nn.init.normal_, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/nn.init.html'},\n","  {'Answer': '>>> w = torch.empty(3, 5)\\n>>> nn.init.constant_(w, 0.3)\\n',\n","   'Id': 445,\n","   'Question': 'How to use torch.nn.init.constant_, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/nn.init.html'},\n","  {'Answer': '>>> w = torch.empty(3, 5)\\n>>> nn.init.ones_(w)\\n',\n","   'Id': 446,\n","   'Question': 'How to use torch.nn.init.ones_, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/nn.init.html'},\n","  {'Answer': '>>> w = torch.empty(3, 5)\\n>>> nn.init.zeros_(w)\\n',\n","   'Id': 447,\n","   'Question': 'How to use torch.nn.init.zeros_, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/nn.init.html'},\n","  {'Answer': '>>> w = torch.empty(3, 5)\\n>>> nn.init.eye_(w)\\n',\n","   'Id': 448,\n","   'Question': 'How to use torch.nn.init.eye_, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/nn.init.html'},\n","  {'Answer': '>>> w = torch.empty(3, 16, 5, 5)\\n>>> nn.init.dirac_(w)\\n>>> w = torch.empty(3, 24, 5, 5)\\n>>> nn.init.dirac_(w, 3)\\n',\n","   'Id': 449,\n","   'Question': 'How to use torch.nn.init.dirac_, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/nn.init.html'},\n","  {'Answer': \">>> w = torch.empty(3, 5)\\n>>> nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain('relu'))\\n\",\n","   'Id': 450,\n","   'Question': 'How to use torch.nn.init.xavier_uniform_, give an example?',\n","   'context': ' Also known as Glorot initialization.',\n","   'source': 'https://pytorch.org/docs/stable/nn.init.html'},\n","  {'Answer': '>>> w = torch.empty(3, 5)\\n>>> nn.init.xavier_normal_(w)\\n',\n","   'Id': 451,\n","   'Question': 'How to use torch.nn.init.xavier_normal_, give an example?',\n","   'context': ' Also known as Glorot initialization.',\n","   'source': 'https://pytorch.org/docs/stable/nn.init.html'},\n","  {'Answer': \">>> w = torch.empty(3, 5)\\n>>> nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu')\\n\",\n","   'Id': 452,\n","   'Question': 'How to use torch.nn.init.kaiming_uniform_, give an example?',\n","   'context': ' Also known as He initialization.',\n","   'source': 'https://pytorch.org/docs/stable/nn.init.html'},\n","  {'Answer': \">>> w = torch.empty(3, 5)\\n>>> nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu')\\n\",\n","   'Id': 453,\n","   'Question': 'How to use torch.nn.init.kaiming_normal_, give an example?',\n","   'context': ' Also known as He initialization.',\n","   'source': 'https://pytorch.org/docs/stable/nn.init.html'},\n","  {'Answer': '>>> w = torch.empty(3, 5)\\n>>> nn.init.orthogonal_(w)\\n',\n","   'Id': 454,\n","   'Question': 'How to use torch.nn.init.orthogonal_, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/nn.init.html'},\n","  {'Answer': '>>> w = torch.empty(3, 5)\\n>>> nn.init.sparse_(w, sparsity=0.1)\\n',\n","   'Id': 455,\n","   'Question': 'How to use torch.nn.init.sparse_, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/nn.init.html'},\n","  {'Answer': '>>> M = torch.randn(3, 5)\\n>>> batch1 = torch.randn(10, 3, 4)\\n>>> batch2 = torch.randn(10, 4, 5)\\n>>> torch.addbmm(M, batch1, batch2)\\ntensor([[  6.6311,   0.0503,   6.9768, -12.0362,  -2.1653],\\n        [ -4.8185,  -1.4255,  -6.6760,   8.9453,   2.5743],\\n        [ -3.8202,   4.3691,   1.0943,  -1.1109,   5.4730]])\\n',\n","   'Id': 456,\n","   'Question': 'How to use torch.addbmm, give an example?',\n","   'context': ' This operator supportsTensorFloat32.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.addbmm.html#torch.addbmm'},\n","  {'Answer': 'torch.linalg.lstsq(A, B).solution == A.pinv() @ B\\n',\n","   'Id': 457,\n","   'Question': 'How to use torch.linalg.pinv, give an example?',\n","   'context': ' Consider usingtorch.linalg.lstsq()if possible for multiplying a matrix on the left by\\nthe the pseudoinverse, as:',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv'},\n","  {'Answer': \">>> A = torch.randn(3, 5)\\n>>> A\\ntensor([[ 0.5495,  0.0979, -1.4092, -0.1128,  0.4132],\\n        [-1.1143, -0.3662,  0.3042,  1.6374, -0.9294],\\n        [-0.3269, -0.5745, -0.0382, -0.5922, -0.6759]])\\n>>> torch.linalg.pinv(A)\\ntensor([[ 0.0600, -0.1933, -0.2090],\\n        [-0.0903, -0.0817, -0.4752],\\n        [-0.7124, -0.1631, -0.2272],\\n        [ 0.1356,  0.3933, -0.5023],\\n        [-0.0308, -0.1725, -0.5216]])\\n\\nBatched linalg.pinv example\\n>>> A = torch.randn(2, 6, 3)\\n>>> B = torch.linalg.pinv(A)\\n>>> torch.matmul(B, A).round()\\ntensor([[[1., -0., 0.],\\n         [0., 1., -0.],\\n         [0., 0., 1.]],\\n\\n        [[1., -0., 0.],\\n         [-0., 1., 0.],\\n         [-0., -0., 1.]]])\\n\\nHermitian input example\\n>>> A = torch.randn(3, 3, dtype=torch.complex64)\\n>>> A = A + A.t().conj()  # creates a Hermitian matrix\\n>>> B = torch.linalg.pinv(A, hermitian=True)\\n>>> torch.matmul(B, A)\\ntensor([[ 1.0000e+00+0.0000e+00j, -1.1921e-07-2.3842e-07j,\\n        5.9605e-08-2.3842e-07j],\\n        [ 5.9605e-08+2.3842e-07j,  1.0000e+00+2.3842e-07j,\\n        -4.7684e-07+1.1921e-07j],\\n        [-1.1921e-07+0.0000e+00j, -2.3842e-07-2.9802e-07j,\\n        1.0000e+00-1.7897e-07j]])\\n\\nNon-default rcond example\\n>>> rcond = 0.5\\n>>> A = torch.randn(3, 3)\\n>>> torch.linalg.pinv(A)\\ntensor([[ 0.2971, -0.4280, -2.0111],\\n        [-0.0090,  0.6426, -0.1116],\\n        [-0.7832, -0.2465,  1.0994]])\\n>>> torch.linalg.pinv(A, rcond)\\ntensor([[-0.2672, -0.2351, -0.0539],\\n        [-0.0211,  0.6467, -0.0698],\\n        [-0.4400, -0.3638, -0.0910]])\\n\\nMatrix-wise rcond example\\n>>> A = torch.randn(5, 6, 2, 3, 3)\\n>>> rcond = torch.rand(2)  # different rcond values for each matrix in a[:, :, 0] and a[:, :, 1]\\n>>> torch.linalg.pinv(A, rcond)\\n>>> rcond = torch.randn(5, 6, 2) # different rcond value for each matrix in 'a'\\n>>> torch.linalg.pinv(A, rcond)\\n\",\n","   'Id': 458,\n","   'Question': 'How  The singular values (or the norm of the eigenvalues whenhermitian= True)\\nthat are below the specifiedrcondthreshold are treated as zero and discarded in the computation., give an example?',\n","   'context': ' The singular values (or the norm of the eigenvalues whenhermitian= True)\\nthat are below the specifiedrcondthreshold are treated as zero and discarded in the computation.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv'},\n","  {'Answer': '>>> torch.result_type(torch.tensor([1, 2], dtype=torch.int), 1.0)\\ntorch.float32\\n>>> torch.result_type(torch.tensor([1, 2], dtype=torch.uint8), torch.tensor(1))\\ntorch.uint8\\n',\n","   'Id': 459,\n","   'Question': 'How to use torch.result_type, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.result_type.html#torch.result_type'},\n","  {'Answer': '>>> # initial default for floating point is torch.float32\\n>>> torch.tensor([1.2, 3]).dtype\\ntorch.float32\\n>>> # initial default for floating point is torch.complex64\\n>>> torch.tensor([1.2, 3j]).dtype\\ntorch.complex64\\n>>> torch.set_default_dtype(torch.float64)\\n>>> torch.tensor([1.2, 3]).dtype    # a new floating point tensor\\ntorch.float64\\n>>> torch.tensor([1.2, 3j]).dtype   # a new complex tensor\\ntorch.complex128\\n',\n","   'Id': 460,\n","   'Question': 'How to use torch.set_default_dtype, give an example?',\n","   'context': ' The default floating point dtype is initiallytorch.float32.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.set_default_dtype.html#torch.set_default_dtype'},\n","  {'Answer': '>>> a = torch.randn(10)\\n>>> a\\ntensor([-0.2284, -0.6628,  0.0975,  0.2680, -1.3298, -0.4220, -0.3885,  1.1762,\\n     0.9165,  1.6684])\\n>>> torch.cummin(a, dim=0)\\ntorch.return_types.cummin(\\n    values=tensor([-0.2284, -0.6628, -0.6628, -0.6628, -1.3298, -1.3298, -1.3298, -1.3298,\\n    -1.3298, -1.3298]),\\n    indices=tensor([0, 1, 1, 1, 4, 4, 4, 4, 4, 4]))\\n',\n","   'Id': 461,\n","   'Question': 'How to use torch.cummin, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.cummin.html#torch.cummin'},\n","  {'Answer': '>>> a = torch.randint(10, (5,))\\n>>> a\\ntensor([6, 5, 1, 0, 2])\\n>>> b = a + (torch.randn(50, 1) * 5).long()\\n>>> torch.mode(b, 0)\\ntorch.return_types.mode(values=tensor([6, 5, 1, 0, 2]), indices=tensor([2, 2, 2, 2, 2]))\\n',\n","   'Id': 462,\n","   'Question': 'How to use torch.mode, give an example?',\n","   'context': ' IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\\nin the output tensors having 1 fewer dimension thaninput.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.mode.html#torch.mode'},\n","  {'Answer': '>>> x=torch.tensor([1,2,3])\\n>>> torch.is_tensor(x)\\nTrue\\n',\n","   'Id': 463,\n","   'Question': 'How  Note that this function is simply doingisinstance(obj,Tensor).\\nUsing thatisinstancecheck is better for typechecking with mypy,\\nand more explicit - so it’s recommended to use that instead ofis_tensor., give an example?',\n","   'context': ' Note that this function is simply doingisinstance(obj,Tensor).\\nUsing thatisinstancecheck is better for typechecking with mypy,\\nand more explicit - so it’s recommended to use that instead ofis_tensor.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.is_tensor.html#torch.is_tensor'},\n","  {'Answer': '>>> import torch.nn as nn\\n>>> from torch.distributed.optim import ZeroRedundancyOptimizer\\n>>> from torch.nn.parallel import DistributedDataParallel as DDP\\n\\n>>> model = nn.Sequential(*[nn.Linear(2000, 2000).to(rank) for _ in range(20)])\\n>>> ddp = DDP(model, device_ids=[rank])\\n>>> opt = ZeroRedundancyOptimizer(\\n>>>     ddp.parameters(),\\n>>>     optimizer_class=torch.optim.Adam,\\n>>>     lr=0.01\\n>>> )\\n>>> ddp(inputs).sum().backward()\\n>>> opt.step()\\n',\n","   'Id': 464,\n","   'Question': 'How to use torch.distributed.optim.ZeroRedundancyOptimizer, give an example?',\n","   'context': ' ZeroRedundancyOptimizeruse a greedy algorithm to pack a number of\\nparameters at each rank. Each parameter belongs to a single rank and is not\\ndivided among ranks. The partition is arbitrary and might not match the\\nthe parameter registration or usage order.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.optim.html'},\n","  {'Answer': '>>> torch.arange(5)\\ntensor([ 0,  1,  2,  3,  4])\\n>>> torch.arange(1, 4)\\ntensor([ 1,  2,  3])\\n>>> torch.arange(1, 2.5, 0.5)\\ntensor([ 1.0000,  1.5000,  2.0000])\\n',\n","   'Id': 465,\n","   'Question': 'How to use torch.arange, give an example?',\n","   'context': ' Note that non-integerstepis subject to floating point rounding errors when\\ncomparing againstend; to avoid inconsistency, we advise adding a small epsilon toendin such cases.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.arange.html#torch.arange'},\n","  {'Answer': '>>> a = torch.hypot(torch.tensor([4.0]), torch.tensor([3.0, 4.0, 5.0]))\\ntensor([5.0000, 5.6569, 6.4031])\\n',\n","   'Id': 466,\n","   'Question': 'How to use torch.hypot, give an example?',\n","   'context': ' The shapes ofinputandothermust bebroadcastable.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.hypot.html#torch.hypot'},\n","  {'Answer': '>>> x = torch.zeros(1, requires_grad=True)\\n>>> with torch.no_grad():\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> is_train = False\\n>>> with torch.set_grad_enabled(is_train):\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> torch.set_grad_enabled(True)  # this can also be used as a function\\n>>> y = x * 2\\n>>> y.requires_grad\\nTrue\\n\\n>>> torch.set_grad_enabled(False)\\n>>> y = x * 2\\n>>> y.requires_grad\\nFalse\\n',\n","   'Id': 467,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/torch.html#in-place-random-sampling'},\n","  {'Answer': '>>> x = torch.randn(3, 3)\\n>>> x\\ntensor([[ 0.9039,  0.6291,  1.0795],\\n        [ 0.1586,  2.1939, -0.4900],\\n        [-0.1909, -0.7503,  1.9355]])\\n>>> t = torch.as_strided(x, (2, 2), (1, 2))\\n>>> t\\ntensor([[0.9039, 1.0795],\\n        [0.6291, 0.1586]])\\n>>> t = torch.as_strided(x, (2, 2), (1, 2), 1)\\ntensor([[0.6291, 0.1586],\\n        [1.0795, 2.1939]])\\n',\n","   'Id': 468,\n","   'Question': 'How to use torch.as_strided, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.as_strided.html#torch.as_strided'},\n","  {'Answer': '>>> x = torch.randn(2, 3)\\n>>> x\\ntensor([[ 1.0028, -0.9893,  0.5809],\\n        [-0.1669,  0.7299,  0.4942]])\\n>>> torch.transpose(x, 0, 1)\\ntensor([[ 1.0028, -0.1669],\\n        [-0.9893,  0.7299],\\n        [ 0.5809,  0.4942]])\\n',\n","   'Id': 469,\n","   'Question': 'How to use torch.transpose, give an example?',\n","   'context': ' The resultingouttensor shares its underlying storage with theinputtensor, so changing the content of one would change the content\\nof the other.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.transpose.html#torch.transpose'},\n","  {'Answer': '>>> a = torch.randn(4, 3)\\n>>> a\\ntensor([[-0.3956,  1.1455,  1.6895],\\n        [-0.5849,  1.3672,  0.3599],\\n        [-1.1626,  0.7180, -0.0521],\\n        [-0.1339,  0.9902, -2.0225]])\\n>>> b = torch.randn(4, 3)\\n>>> b\\ntensor([[-0.0257, -1.4725, -1.2251],\\n        [-1.1479, -0.7005, -1.9757],\\n        [-1.3904,  0.3726, -1.1836],\\n        [-0.9688, -0.7153,  0.2159]])\\n>>> torch.cross(a, b, dim=1)\\ntensor([[ 1.0844, -0.5281,  0.6120],\\n        [-2.4490, -1.5687,  1.9792],\\n        [-0.8304, -1.3037,  0.5650],\\n        [-1.2329,  1.9883,  1.0551]])\\n>>> torch.cross(a, b)\\ntensor([[ 1.0844, -0.5281,  0.6120],\\n        [-2.4490, -1.5687,  1.9792],\\n        [-0.8304, -1.3037,  0.5650],\\n        [-1.2329,  1.9883,  1.0551]])\\n',\n","   'Id': 470,\n","   'Question': 'How to use torch.cross, give an example?',\n","   'context': ' Ifdimis not given, it defaults to the first dimension found with the\\nsize 3. Note that this might be unexpected.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.cross.html#torch.cross'},\n","  {'Answer': '>>> x = torch.arange(4).view(2, 2)\\n>>> x\\ntensor([[0, 1],\\n        [2, 3]])\\n>>> torch.rot90(x, 1, [0, 1])\\ntensor([[1, 3],\\n        [0, 2]])\\n\\n>>> x = torch.arange(8).view(2, 2, 2)\\n>>> x\\ntensor([[[0, 1],\\n         [2, 3]],\\n\\n        [[4, 5],\\n         [6, 7]]])\\n>>> torch.rot90(x, 1, [1, 2])\\ntensor([[[1, 3],\\n         [0, 2]],\\n\\n        [[5, 7],\\n         [4, 6]]])\\n',\n","   'Id': 471,\n","   'Question': 'How to use torch.rot90, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.rot90.html#torch.rot90'},\n","  {'Answer': '>>> x = torch.zeros(1, requires_grad=True)\\n>>> with torch.no_grad():\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> is_train = False\\n>>> with torch.set_grad_enabled(is_train):\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> torch.set_grad_enabled(True)  # this can also be used as a function\\n>>> y = x * 2\\n>>> y.requires_grad\\nTrue\\n\\n>>> torch.set_grad_enabled(False)\\n>>> y = x * 2\\n>>> y.requires_grad\\nFalse\\n',\n","   'Id': 472,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/torch.html#pointwise-ops'},\n","  {'Answer': 'self[index[i][j][k]][j][k] += src[i][j][k]  # if dim == 0\\nself[i][index[i][j][k]][k] += src[i][j][k]  # if dim == 1\\nself[i][j][index[i][j][k]] += src[i][j][k]  # if dim == 2\\n',\n","   'Id': 473,\n","   'Question': 'How to use torch.Tensor.scatter_add_, give an example?',\n","   'context': ' For a 3-D tensor,selfis updated as:',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_'},\n","  {'Answer': '>>> src = torch.ones((2, 5))\\n>>> index = torch.tensor([[0, 1, 2, 0, 0]])\\n>>> torch.zeros(3, 5, dtype=src.dtype).scatter_add_(0, index, src)\\ntensor([[1., 0., 0., 1., 1.],\\n        [0., 1., 0., 0., 0.],\\n        [0., 0., 1., 0., 0.]])\\n>>> index = torch.tensor([[0, 1, 2, 0, 0], [0, 1, 2, 2, 2]])\\n>>> torch.zeros(3, 5, dtype=src.dtype).scatter_add_(0, index, src)\\ntensor([[2., 0., 0., 1., 1.],\\n        [0., 2., 0., 0., 0.],\\n        [0., 0., 2., 1., 1.]])\\n',\n","   'Id': 474,\n","   'Question': 'How  self,indexandsrcshould have same number of\\ndimensions. It is also required thatindex.size(d)<=src.size(d)for all\\ndimensionsd, and thatindex.size(d)<=self.size(d)for all dimensionsd!=dim. Note thatindexandsrcdo not broadcast., give an example?',\n","   'context': ' self,indexandsrcshould have same number of\\ndimensions. It is also required thatindex.size(d)<=src.size(d)for all\\ndimensionsd, and thatindex.size(d)<=self.size(d)for all dimensionsd!=dim. Note thatindexandsrcdo not broadcast.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_'},\n","  {'Answer': '>>> a = torch.randn(3, 3)\\n>>> a\\ntensor([[ 0.2309,  0.5207,  2.0049],\\n        [ 0.2072, -1.0680,  0.6602],\\n        [ 0.3480, -0.5211, -0.4573]])\\n>>> torch.triu(a)\\ntensor([[ 0.2309,  0.5207,  2.0049],\\n        [ 0.0000, -1.0680,  0.6602],\\n        [ 0.0000,  0.0000, -0.4573]])\\n>>> torch.triu(a, diagonal=1)\\ntensor([[ 0.0000,  0.5207,  2.0049],\\n        [ 0.0000,  0.0000,  0.6602],\\n        [ 0.0000,  0.0000,  0.0000]])\\n>>> torch.triu(a, diagonal=-1)\\ntensor([[ 0.2309,  0.5207,  2.0049],\\n        [ 0.2072, -1.0680,  0.6602],\\n        [ 0.0000, -0.5211, -0.4573]])\\n\\n>>> b = torch.randn(4, 6)\\n>>> b\\ntensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],\\n        [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],\\n        [ 0.4333,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],\\n        [-0.9888,  1.0679, -1.3337, -1.6556,  0.4798,  0.2830]])\\n>>> torch.triu(b, diagonal=1)\\ntensor([[ 0.0000, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],\\n        [ 0.0000,  0.0000, -1.2919,  1.3378, -0.1768, -1.0857],\\n        [ 0.0000,  0.0000,  0.0000, -1.0432,  0.9348, -0.4410],\\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.4798,  0.2830]])\\n>>> torch.triu(b, diagonal=-1)\\ntensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],\\n        [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],\\n        [ 0.0000,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],\\n        [ 0.0000,  0.0000, -1.3337, -1.6556,  0.4798,  0.2830]])\\n',\n","   'Id': 475,\n","   'Question': 'How to use torch.triu, give an example?',\n","   'context': ' The argumentdiagonalcontrols which diagonal to consider. Ifdiagonal= 0, all elements on and above the main diagonal are\\nretained. A positive value excludes just as many diagonals above the main\\ndiagonal, and similarly a negative value includes just as many diagonals below\\nthe main diagonal. The main diagonal are the set of indices{(i,i)}\\\\lbrace (i, i) \\\\rbrace{(i,i)}fori∈[0,min\\u2061{d1,d2}−1]i \\\\in [0, \\\\min\\\\{d_{1}, d_{2}\\\\} - 1]i∈[0,min{d1\\u200b,d2\\u200b}−1]whered1,d2d_{1}, d_{2}d1\\u200b,d2\\u200bare the dimensions of the matrix.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.triu.html#torch.triu'},\n","  {'Answer': '>>> a = torch.tensor([1, 0.5])\\n>>> torch.polygamma(1, a)\\ntensor([1.64493, 4.9348])\\n>>> torch.polygamma(2, a)\\ntensor([ -2.4041, -16.8288])\\n>>> torch.polygamma(3, a)\\ntensor([ 6.4939, 97.4091])\\n>>> torch.polygamma(4, a)\\ntensor([ -24.8863, -771.4742])\\n',\n","   'Id': 476,\n","   'Question': 'How to use torch.polygamma, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.polygamma.html#torch.polygamma'},\n","  {'Answer': '>>> a = torch.tensor([[-0.8166, -1.3802, -0.3560]])\\n>>> torch.var(a, unbiased=False)\\ntensor(0.1754)\\n',\n","   'Id': 477,\n","   'Question': 'How to use torch.var, give an example?',\n","   'context': ' IfunbiasedisTrue, Bessel’s correction will be used.\\nOtherwise, the sample deviation is calculated, without any correction.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.var.html#torch.var'},\n","  {'Answer': '>>> a = torch.arange(-0.5, 1, 0.5)\\n>>> a\\ntensor([-0.5000,  0.0000,  0.5000])\\n>>> torch.special.entr(a)\\ntensor([  -inf, 0.0000, 0.3466])\\n',\n","   'Id': 478,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.erf'},\n","  {'Answer': '>>> torch.special.erf(torch.tensor([0, -1., 10.]))\\ntensor([ 0.0000, -0.8427,  1.0000])\\n',\n","   'Id': 479,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.erf'},\n","  {'Answer': '>>> torch.special.erfc(torch.tensor([0, -1., 10.]))\\ntensor([ 1.0000, 1.8427,  0.0000])\\n',\n","   'Id': 480,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.erf'},\n","  {'Answer': '>>> torch.special.erfinv(torch.tensor([0, 0.5, -1.]))\\ntensor([ 0.0000,  0.4769,    -inf])\\n',\n","   'Id': 481,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.erf'},\n","  {'Answer': '>>> t = torch.randn(4)\\n>>> t\\ntensor([ 0.9213,  1.0887, -0.8858, -1.7683])\\n>>> torch.special.expit(t)\\ntensor([ 0.7153,  0.7481,  0.2920,  0.1458])\\n',\n","   'Id': 482,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.erf'},\n","  {'Answer': '>>> torch.special.expm1(torch.tensor([0, math.log(2.)]))\\ntensor([ 0.,  1.])\\n',\n","   'Id': 483,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.erf'},\n","  {'Answer': '>>> torch.special.exp2(torch.tensor([0, math.log2(2.), 3, 4]))\\ntensor([ 1.,  2.,  8., 16.])\\n',\n","   'Id': 484,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.erf'},\n","  {'Answer': '>>> a = torch.arange(0.5, 2, 0.5)\\n>>> torch.special.gammaln(a)\\ntensor([ 0.5724,  0.0000, -0.1208])\\n',\n","   'Id': 485,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.erf'},\n","  {'Answer': '>>> torch.special.i0e(torch.arange(5, dtype=torch.float32))\\ntensor([1.0000, 0.4658, 0.3085, 0.2430, 0.2070])\\n',\n","   'Id': 486,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.erf'},\n","  {'Answer': '>>> a = torch.rand(5)\\n>>> a\\ntensor([0.2796, 0.9331, 0.6486, 0.1523, 0.6516])\\n>>> torch.special.logit(a, eps=1e-6)\\ntensor([-0.9466,  2.6352,  0.6131, -1.7169,  0.6261])\\n',\n","   'Id': 487,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.erf'},\n","  {'Answer': \">>> x = torch.zeros(5,)\\n>>> y = torch.tensor([-1, 0, 1, float('inf'), float('nan')])\\n>>> torch.special.xlog1py(x, y)\\ntensor([0., 0., 0., 0., nan])\\n>>> x = torch.tensor([1, 2, 3])\\n>>> y = torch.tensor([3, 2, 1])\\n>>> torch.special.xlog1py(x, y)\\ntensor([1.3863, 2.1972, 2.0794])\\n>>> torch.special.xlog1py(x, 4)\\ntensor([1.6094, 3.2189, 4.8283])\\n>>> torch.special.xlog1py(2, y)\\ntensor([2.7726, 2.1972, 1.3863])\\n\",\n","   'Id': 488,\n","   'Question': 'How  Similar to SciPy’sscipy.special.xlog1py., give an example?',\n","   'context': ' Similar to SciPy’sscipy.special.xlog1py.',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.erf'},\n","  {'Answer': \">>> torch.allclose(torch.tensor([10000., 1e-07]), torch.tensor([10000.1, 1e-08]))\\nFalse\\n>>> torch.allclose(torch.tensor([10000., 1e-08]), torch.tensor([10000.1, 1e-09]))\\nTrue\\n>>> torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')]))\\nFalse\\n>>> torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')]), equal_nan=True)\\nTrue\\n\",\n","   'Id': 489,\n","   'Question': 'How to use torch.allclose, give an example?',\n","   'context': ' elementwise, for all elements ofinputandother. The behaviour of this function is analogous tonumpy.allclose',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.allclose.html#torch.allclose'},\n","  {'Answer': '>>> # vector x vector\\n>>> tensor1 = torch.randn(3)\\n>>> tensor2 = torch.randn(3)\\n>>> torch.matmul(tensor1, tensor2).size()\\ntorch.Size([])\\n>>> # matrix x vector\\n>>> tensor1 = torch.randn(3, 4)\\n>>> tensor2 = torch.randn(4)\\n>>> torch.matmul(tensor1, tensor2).size()\\ntorch.Size([3])\\n>>> # batched matrix x broadcasted vector\\n>>> tensor1 = torch.randn(10, 3, 4)\\n>>> tensor2 = torch.randn(4)\\n>>> torch.matmul(tensor1, tensor2).size()\\ntorch.Size([10, 3])\\n>>> # batched matrix x batched matrix\\n>>> tensor1 = torch.randn(10, 3, 4)\\n>>> tensor2 = torch.randn(10, 4, 5)\\n>>> torch.matmul(tensor1, tensor2).size()\\ntorch.Size([10, 3, 5])\\n>>> # batched matrix x broadcasted matrix\\n>>> tensor1 = torch.randn(10, 3, 4)\\n>>> tensor2 = torch.randn(4, 5)\\n>>> torch.matmul(tensor1, tensor2).size()\\ntorch.Size([10, 3, 5])\\n',\n","   'Id': 490,\n","   'Question': 'How to use torch.matmul, give an example?',\n","   'context': ' This operator supportsTensorFloat32.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul'},\n","  {'Answer': '>>> x = torch.zeros(3,3)\\n>>> x[torch.randn(3,3) > 0.5] = 1\\n>>> x\\ntensor([[0., 1., 1.],\\n        [0., 0., 0.],\\n        [0., 0., 1.]])\\n>>> torch.count_nonzero(x)\\ntensor(3)\\n>>> torch.count_nonzero(x, dim=0)\\ntensor([0, 1, 2])\\n',\n","   'Id': 491,\n","   'Question': 'How to use torch.count_nonzero, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.count_nonzero.html#torch.count_nonzero'},\n","  {'Answer': '>>> x = torch.tensor([1, 2, 3])\\n>>> y = torch.tensor([4, 5, 6])\\n>>> grid_x, grid_y = torch.meshgrid(x, y)\\n>>> grid_x\\ntensor([[1, 1, 1],\\n        [2, 2, 2],\\n        [3, 3, 3]])\\n>>> grid_y\\ntensor([[4, 5, 6],\\n        [4, 5, 6],\\n        [4, 5, 6]])\\n',\n","   'Id': 492,\n","   'Question': 'How to use torch.meshgrid, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid'},\n","  {'Answer': '>>> torch.is_nonzero(torch.tensor([0.]))\\nFalse\\n>>> torch.is_nonzero(torch.tensor([1.5]))\\nTrue\\n>>> torch.is_nonzero(torch.tensor([False]))\\nFalse\\n>>> torch.is_nonzero(torch.tensor([3]))\\nTrue\\n>>> torch.is_nonzero(torch.tensor([1, 3, 5]))\\nTraceback (most recent call last):\\n...\\nRuntimeError: bool value of Tensor with more than one value is ambiguous\\n>>> torch.is_nonzero(torch.tensor([]))\\nTraceback (most recent call last):\\n...\\nRuntimeError: bool value of Tensor with no values is ambiguous\\n',\n","   'Id': 493,\n","   'Question': 'How to use torch.is_nonzero, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.is_nonzero.html#torch.is_nonzero'},\n","  {'Answer': '>>> torch.tensor([[1., -1.], [1., -1.]])\\ntensor([[ 1.0000, -1.0000],\\n        [ 1.0000, -1.0000]])\\n>>> torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\\ntensor([[ 1,  2,  3],\\n        [ 4,  5,  6]])\\n',\n","   'Id': 494,\n","   'Question': 'How to use A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor:, give an example?',\n","   'context': ' A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor:',\n","   'source': 'https://pytorch.org/docs/stable/tensors.html'},\n","  {'Answer': \">>> torch.zeros([2, 4], dtype=torch.int32)\\ntensor([[ 0,  0,  0,  0],\\n        [ 0,  0,  0,  0]], dtype=torch.int32)\\n>>> cuda0 = torch.device('cuda:0')\\n>>> torch.ones([2, 4], dtype=torch.float64, device=cuda0)\\ntensor([[ 1.0000,  1.0000,  1.0000,  1.0000],\\n        [ 1.0000,  1.0000,  1.0000,  1.0000]], dtype=torch.float64, device='cuda:0')\\n\",\n","   'Id': 495,\n","   'Question': 'How to use A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor:A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\\nconstructor or tensor creation op:, give an example?',\n","   'context': ' A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\\nconstructor or tensor creation op:',\n","   'source': 'https://pytorch.org/docs/stable/tensors.html'},\n","  {'Answer': '>>> x = torch.tensor([[1, 2, 3], [4, 5, 6]])\\n>>> print(x[1][2])\\ntensor(6)\\n>>> x[0][1] = 8\\n>>> print(x)\\ntensor([[ 1,  8,  3],\\n        [ 4,  5,  6]])\\n',\n","   'Id': 496,\n","   'Question': 'How to use For more information about building Tensors, seeCreation OpsThe contents of a tensor can be accessed and modified using Python’s indexing\\nand slicing notation:, give an example?',\n","   'context': ' For more information about building Tensors, seeCreation OpsThe contents of a tensor can be accessed and modified using Python’s indexing\\nand slicing notation:',\n","   'source': 'https://pytorch.org/docs/stable/tensors.html'},\n","  {'Answer': '>>> x = torch.tensor([[1]])\\n>>> x\\ntensor([[ 1]])\\n>>> x.item()\\n1\\n>>> x = torch.tensor(2.5)\\n>>> x\\ntensor(2.5000)\\n>>> x.item()\\n2.5\\n',\n","   'Id': 497,\n","   'Question': 'How to use The contents of a tensor can be accessed and modified using Python’s indexing\\nand slicing notation:Usetorch.Tensor.item()to get a Python number from a tensor containing a\\nsingle value:, give an example?',\n","   'context': ' The contents of a tensor can be accessed and modified using Python’s indexing\\nand slicing notation:Usetorch.Tensor.item()to get a Python number from a tensor containing a\\nsingle value:',\n","   'source': 'https://pytorch.org/docs/stable/tensors.html'},\n","  {'Answer': '>>> x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)\\n>>> out = x.pow(2).sum()\\n>>> out.backward()\\n>>> x.grad\\ntensor([[ 2.0000, -2.0000],\\n        [ 2.0000,  2.0000]])\\n',\n","   'Id': 498,\n","   'Question': 'How to use For more information about indexing, seeIndexing, Slicing, Joining, Mutating OpsA tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation., give an example?',\n","   'context': ' For more information about indexing, seeIndexing, Slicing, Joining, Mutating OpsA tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation.',\n","   'source': 'https://pytorch.org/docs/stable/tensors.html'},\n","  {'Answer': '>>> i = [[0, 1, 1],\\n         [2, 0, 2]]\\n>>> v =  [3, 4, 5]\\n>>> s = torch.sparse_coo_tensor(i, v, (2, 3))\\n>>> s\\ntensor(indices=tensor([[0, 1, 1],\\n                       [2, 0, 2]]),\\n       values=tensor([3, 4, 5]),\\n       size=(2, 3), nnz=3, layout=torch.sparse_coo)\\n>>> s.to_dense()\\ntensor([[0, 0, 3],\\n        [4, 0, 5]])\\n',\n","   'Id': 499,\n","   'Question': 'How to use A sparse COO tensor can be constructed by providing the two tensors of\\nindices and values, as well as the size of the sparse tensor (when it\\ncannot be inferred from the indices and values tensors) to a functiontorch.sparse_coo_tensor().Suppose we want to define a sparse tensor with the entry 3 at location\\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\\nUnspecified elements are assumed to have the same value, fill value,\\nwhich is zero by default. We would then write:, give an example?',\n","   'context': ' Suppose we want to define a sparse tensor with the entry 3 at location\\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\\nUnspecified elements are assumed to have the same value, fill value,\\nwhich is zero by default. We would then write:',\n","   'source': 'https://pytorch.org/docs/stable/sparse.html'},\n","  {'Answer': '>>> i = [[0, 2], [1, 0], [1, 2]]\\n>>> v =  [3,      4,      5    ]\\n>>> s = torch.sparse_coo_tensor(list(zip(*i)), v, (2, 3))\\n>>> # Or another equivalent formulation to get s\\n>>> s = torch.sparse_coo_tensor(torch.tensor(i).t(), v, (2, 3))\\n>>> torch.sparse_coo_tensor(i.t(), v, torch.Size([2,3])).to_dense()\\ntensor([[0, 0, 3],\\n        [4, 0, 5]])\\n',\n","   'Id': 500,\n","   'Question': 'How to use Suppose we want to define a sparse tensor with the entry 3 at location\\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\\nUnspecified elements are assumed to have the same value, fill value,\\nwhich is zero by default. We would then write:Note that the inputiis NOT a list of index tuples.  If you want\\nto write your indices this way, you should transpose before passing them to\\nthe sparse constructor:, give an example?',\n","   'context': ' Suppose we want to define a sparse tensor with the entry 3 at location\\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\\nUnspecified elements are assumed to have the same value, fill value,\\nwhich is zero by default. We would then write:Note that the inputiis NOT a list of index tuples.  If you want\\nto write your indices this way, you should transpose before passing them to\\nthe sparse constructor:',\n","   'source': 'https://pytorch.org/docs/stable/sparse.html'},\n","  {'Answer': '>>> torch.sparse_coo_tensor(size=(2, 3))\\ntensor(indices=tensor([], size=(2, 0)),\\n       values=tensor([], size=(0,)),\\n       size=(2, 3), nnz=0, layout=torch.sparse_coo)\\n',\n","   'Id': 501,\n","   'Question': 'How to use Note that the inputiis NOT a list of index tuples.  If you want\\nto write your indices this way, you should transpose before passing them to\\nthe sparse constructor:An empty sparse COO tensor can be constructed by specifying its size\\nonly:, give an example?',\n","   'context': ' Note that the inputiis NOT a list of index tuples.  If you want\\nto write your indices this way, you should transpose before passing them to\\nthe sparse constructor:An empty sparse COO tensor can be constructed by specifying its size\\nonly:',\n","   'source': 'https://pytorch.org/docs/stable/sparse.html'},\n","  {'Answer': '>>> i = [[0, 1, 1],\\n         [2, 0, 2]]\\n>>> v =  [[3, 4], [5, 6], [7, 8]]\\n>>> s = torch.sparse_coo_tensor(i, v, (2, 3, 2))\\n>>> s\\ntensor(indices=tensor([[0, 1, 1],\\n                       [2, 0, 2]]),\\n       values=tensor([[3, 4],\\n                      [5, 6],\\n                      [7, 8]]),\\n       size=(2, 3, 2), nnz=3, layout=torch.sparse_coo)\\n',\n","   'Id': 502,\n","   'Question': 'How to use PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\\nthevaluestensor to be a multi-dimensional tensor so that we\\nhave:Suppose we want to create a (2 + 1)-dimensional tensor with the entry\\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\\n[7, 8] at location (1, 2). We would write, give an example?',\n","   'context': ' PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\\nthevaluestensor to be a multi-dimensional tensor so that we\\nhave:Suppose we want to create a (2 + 1)-dimensional tensor with the entry\\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\\n[7, 8] at location (1, 2). We would write',\n","   'source': 'https://pytorch.org/docs/stable/sparse.html'},\n","  {'Answer': '>>> s.to_dense()\\ntensor([[[0, 0],\\n         [0, 0],\\n         [3, 4]],\\n        [[5, 6],\\n         [0, 0],\\n         [7, 8]]])\\n',\n","   'Id': 503,\n","   'Question': 'How  PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\\nthevaluestensor to be a multi-dimensional tensor so that we\\nhave:Suppose we want to create a (2 + 1)-dimensional tensor with the entry\\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\\n[7, 8] at location (1, 2). We would write, give an example?',\n","   'context': ' PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\\nthevaluestensor to be a multi-dimensional tensor so that we\\nhave:Suppose we want to create a (2 + 1)-dimensional tensor with the entry\\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\\n[7, 8] at location (1, 2). We would write',\n","   'source': 'https://pytorch.org/docs/stable/sparse.html'},\n","  {'Answer': '>>> i = [[1, 1]]\\n>>> v =  [3, 4]\\n>>> s=torch.sparse_coo_tensor(i, v, (3,))\\n>>> s\\ntensor(indices=tensor([[1, 1]]),\\n       values=tensor(  [3, 4]),\\n       size=(3,), nnz=2, layout=torch.sparse_coo)\\n',\n","   'Id': 504,\n","   'Question': 'How to use PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\\nwhere there may be duplicate coordinates in the indices; in this case,\\nthe interpretation is that the value at that index is the sum of all\\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\\nuncoalesced tensor:, give an example?',\n","   'context': ' PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\\nwhere there may be duplicate coordinates in the indices; in this case,\\nthe interpretation is that the value at that index is the sum of all\\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\\nuncoalesced tensor:',\n","   'source': 'https://pytorch.org/docs/stable/sparse.html'},\n","  {'Answer': '>>> s.coalesce()\\ntensor(indices=tensor([[1]]),\\n       values=tensor([7]),\\n       size=(3,), nnz=1, layout=torch.sparse_coo)\\n',\n","   'Id': 505,\n","   'Question': 'How to use PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\\nwhere there may be duplicate coordinates in the indices; in this case,\\nthe interpretation is that the value at that index is the sum of all\\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\\nuncoalesced tensor:while the coalescing process will accumulate the multi-valued elements\\ninto a single value using summation:, give an example?',\n","   'context': ' while the coalescing process will accumulate the multi-valued elements\\ninto a single value using summation:',\n","   'source': 'https://pytorch.org/docs/stable/sparse.html'},\n","  {'Answer': '>>> a = torch.sparse_coo_tensor([[1, 1]], [5, 6], (2,))\\n>>> b = torch.sparse_coo_tensor([[0, 0]], [7, 8], (2,))\\n>>> a + b\\ntensor(indices=tensor([[0, 0, 1, 1]]),\\n       values=tensor([7, 8, 5, 6]),\\n       size=(2,), nnz=4, layout=torch.sparse_coo)\\n',\n","   'Id': 506,\n","   'Question': 'How to use However, some operations can be implemented more efficiently on\\nuncoalesced tensors, and some on coalesced tensors.For instance, addition of sparse COO tensors is implemented by\\nsimply concatenating the indices and values tensors:, give an example?',\n","   'context': ' However, some operations can be implemented more efficiently on\\nuncoalesced tensors, and some on coalesced tensors.For instance, addition of sparse COO tensors is implemented by\\nsimply concatenating the indices and values tensors:',\n","   'source': 'https://pytorch.org/docs/stable/sparse.html'},\n","  {'Answer': '>>> i = [[0, 1, 1],\\n         [2, 0, 2]]\\n>>> v =  [[3, 4], [5, 6], [7, 8]]\\n>>> s = torch.sparse_coo_tensor(i, v, (2, 3, 2))\\n',\n","   'Id': 507,\n","   'Question': 'How to use Let’s consider the following example:, give an example?',\n","   'context': ' Let’s consider the following example:',\n","   'source': 'https://pytorch.org/docs/stable/sparse.html'},\n","  {'Answer': '>>> isinstance(s, torch.Tensor)\\nTrue\\n>>> s.is_sparse\\nTrue\\n>>> s.layout == torch.sparse_coo\\nTrue\\n',\n","   'Id': 508,\n","   'Question': 'How to use Let’s consider the following example:As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties:, give an example?',\n","   'context': ' As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties:',\n","   'source': 'https://pytorch.org/docs/stable/sparse.html'},\n","  {'Answer': '>>> s.sparse_dim(), s.dense_dim()\\n(2, 1)\\n',\n","   'Id': 509,\n","   'Question': 'How to use As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties:The number of sparse and dense dimensions can be acquired using\\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance:, give an example?',\n","   'context': ' As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties:The number of sparse and dense dimensions can be acquired using\\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance:',\n","   'source': 'https://pytorch.org/docs/stable/sparse.html'},\n","  {'Answer': '>>> s.indices()\\nRuntimeError: Cannot get indices on an uncoalesced tensor, please call .coalesce() first\\n',\n","   'Id': 510,\n","   'Question': 'How to use NoteCurrently, one can acquire the COO format data only when the tensor\\ninstance is coalesced:, give an example?',\n","   'context': ' Currently, one can acquire the COO format data only when the tensor\\ninstance is coalesced:',\n","   'source': 'https://pytorch.org/docs/stable/sparse.html'},\n","  {'Answer': '>>> s._indices()\\ntensor([[0, 1, 1],\\n        [2, 0, 2]])\\n',\n","   'Id': 511,\n","   'Question': 'How to use Currently, one can acquire the COO format data only when the tensor\\ninstance is coalesced:For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()andtorch.Tensor._indices():, give an example?',\n","   'context': ' Currently, one can acquire the COO format data only when the tensor\\ninstance is coalesced:For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()andtorch.Tensor._indices():',\n","   'source': 'https://pytorch.org/docs/stable/sparse.html'},\n","  {'Answer': '>>> s.is_coalesced()\\nFalse\\n',\n","   'Id': 512,\n","   'Question': 'How to use Ifsis a sparse COO tensor then its COO format data can be\\nacquired using methodstorch.Tensor.indices()andtorch.Tensor.values().Constructing a new sparse COO tensor results a tensor that is not\\ncoalesced:, give an example?',\n","   'context': ' Ifsis a sparse COO tensor then its COO format data can be\\nacquired using methodstorch.Tensor.indices()andtorch.Tensor.values().Constructing a new sparse COO tensor results a tensor that is not\\ncoalesced:',\n","   'source': 'https://pytorch.org/docs/stable/sparse.html'},\n","  {'Answer': '>>> s2 = s.coalesce()\\n>>> s2.indices()\\ntensor([[0, 1, 1],\\n       [2, 0, 2]])\\n',\n","   'Id': 513,\n","   'Question': 'How to use Constructing a new sparse COO tensor results a tensor that is not\\ncoalesced:but one can construct a coalesced copy of a sparse COO tensor using\\nthetorch.Tensor.coalesce()method:, give an example?',\n","   'context': ' Constructing a new sparse COO tensor results a tensor that is not\\ncoalesced:but one can construct a coalesced copy of a sparse COO tensor using\\nthetorch.Tensor.coalesce()method:',\n","   'source': 'https://pytorch.org/docs/stable/sparse.html'},\n","  {'Answer': '>>> s[1]\\ntensor(indices=tensor([[0, 2]]),\\n       values=tensor([[5, 6],\\n                      [7, 8]]),\\n       size=(3, 2), nnz=2, layout=torch.sparse_coo)\\n>>> s[1, 0, 1]\\ntensor(6)\\n>>> s[1, 0, 1:]\\ntensor([6])\\n',\n","   'Id': 514,\n","   'Question': 'How to use When working with uncoalesced sparse COO tensors, one must take into\\nan account the additive nature of uncoalesced data: the values of the\\nsame indices are the terms of a sum that evaluation gives the value of\\nthe corresponding tensor element. For example, the scalar\\nmultiplication on an uncoalesced sparse tensor could be implemented by\\nmultiplying all the uncoalesced values with the scalar becausec*(a+b)==c*a+c*bholds. However, any nonlinear operation,\\nsay, a square root, cannot be implemented by applying the operation to\\nuncoalesced data becausesqrt(a+b)==sqrt(a)+sqrt(b)does not\\nhold in general.Slicing (with positive step) of a sparse COO tensor is supported only\\nfor dense dimensions. Indexing is supported for both sparse and dense\\ndimensions:, give an example?',\n","   'context': ' When working with uncoalesced sparse COO tensors, one must take into\\nan account the additive nature of uncoalesced data: the values of the\\nsame indices are the terms of a sum that evaluation gives the value of\\nthe corresponding tensor element. For example, the scalar\\nmultiplication on an uncoalesced sparse tensor could be implemented by\\nmultiplying all the uncoalesced values with the scalar becausec*(a+b)==c*a+c*bholds. However, any nonlinear operation,\\nsay, a square root, cannot be implemented by applying the operation to\\nuncoalesced data becausesqrt(a+b)==sqrt(a)+sqrt(b)does not\\nhold in general.Slicing (with positive step) of a sparse COO tensor is supported only\\nfor dense dimensions. Indexing is supported for both sparse and dense\\ndimensions:',\n","   'source': 'https://pytorch.org/docs/stable/sparse.html'},\n","  {'Answer': '>>> crow_indices = torch.tensor([0, 2, 4])\\n>>> col_indices = torch.tensor([0, 1, 0, 1])\\n>>> values = torch.tensor([1, 2, 3, 4])\\n>>> csr = torch._sparse_csr_tensor(crow_indices, col_indices, values, dtype=torch.double)\\n>>> csr\\ntensor(crow_indices=tensor([0, 2, 4]),\\n      col_indices=tensor([0, 1, 0, 1]),\\n      values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4,\\n      dtype=torch.float64)\\n>>> csr.to_dense()\\ntensor([[1., 2.],\\n        [3., 4.]], dtype=torch.float64)\\n',\n","   'Id': 515,\n","   'Question': 'How to use Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present., give an example?',\n","   'context': ' Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present.',\n","   'source': 'https://pytorch.org/docs/stable/sparse.html'},\n","  {'Answer': '>>> a = torch.tensor([[0, 0, 1, 0], [1, 2, 0, 0], [0, 0, 0, 0]], dtype = torch.float64)\\n>>> sp = a._to_sparse_csr()\\n>>> sp\\ntensor(crow_indices=tensor([0, 1, 3, 3]),\\n      col_indices=tensor([2, 0, 1]),\\n      values=tensor([1., 1., 2.]), size=(3, 4), nnz=3, dtype=torch.float64)\\n',\n","   'Id': 516,\n","   'Question': 'How to use The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\\nbe interpreted as missing values in the sparse tensor:, give an example?',\n","   'context': ' The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\\nbe interpreted as missing values in the sparse tensor:',\n","   'source': 'https://pytorch.org/docs/stable/sparse.html'},\n","  {'Answer': '>>> vec = torch.randn(4, 1, dtype=torch.float64)\\n>>> sp.matmul(vec)\\ntensor([[0.9078],\\n        [1.3180],\\n        [0.0000]], dtype=torch.float64)\\n',\n","   'Id': 517,\n","   'Question': 'How to use The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\\nbe interpreted as missing values in the sparse tensor:The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\\nsupported on CSR tensors., give an example?',\n","   'context': ' The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\\nsupported on CSR tensors.',\n","   'source': 'https://pytorch.org/docs/stable/sparse.html'},\n","  {'Answer': '>>> a = torch.randn(4)\\n>>> a\\ntensor([-0.5461,  0.1347, -2.7266, -0.2746])\\n>>> torch.sin(a)\\ntensor([-0.5194,  0.1343, -0.4032, -0.2711])\\n',\n","   'Id': 518,\n","   'Question': 'How to use torch.sin, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.sin.html#torch.sin'},\n","  {'Answer': '>>> t = torch.tensor([1, 2, 4, 7, 11, 16], dtype=torch.float)\\n>>> torch.gradient(t)\\ntensor([1. , 1.5, 2.5, 3.5, 4.5, 5. ])\\n>>> coords = torch.tensor([0., 1., 1.5, 3.5, 4., 6.], dtype=torch.float)\\n>>> torch.gradient(t, spacing=(coords,))\\ntensor([1. ,  3. ,  3.5,  6.7,  6.9,  2.5])\\n',\n","   'Id': 519,\n","   'Question': 'How to use torch.gradient, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.gradient.html#torch.gradient'},\n","  {'Answer': '>>> input = torch.randint(0, 8, (5,), dtype=torch.int64)\\n>>> weights = torch.linspace(0, 1, steps=5)\\n>>> input, weights\\n(tensor([4, 3, 6, 3, 4]),\\n tensor([ 0.0000,  0.2500,  0.5000,  0.7500,  1.0000])\\n\\n>>> torch.bincount(input)\\ntensor([0, 0, 0, 2, 2, 0, 1])\\n\\n>>> input.bincount(weights)\\ntensor([0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.5000])\\n',\n","   'Id': 520,\n","   'Question': 'How to use torch.bincount, give an example?',\n","   'context': ' The number of bins (size 1) is one larger than the largest value ininputunlessinputis empty, in which case the result is a\\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand ifinputis empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount'},\n","  {'Answer': '>>> torch.conj(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))\\ntensor([-1 - 1j, -2 - 2j, 3 + 3j])\\n',\n","   'Id': 521,\n","   'Question': 'How to use torch.conj, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.conj.html#torch.conj'},\n","  {'Answer': '>>> t = torch.randn(5)\\n>>> t\\ntensor([ 0.0090, -0.2262, -0.0682, -0.2866,  0.3940])\\n>>> torch.positive(t)\\ntensor([ 0.0090, -0.2262, -0.0682, -0.2866,  0.3940])\\n',\n","   'Id': 522,\n","   'Question': 'How to use torch.positive, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.positive.html#torch.positive'},\n","  {'Answer': '>>> a = torch.randn(4)\\n>>> a\\ntensor([ 0.2341,  0.2539, -0.6256, -0.6448])\\n>>> torch.atan(a)\\ntensor([ 0.2299,  0.2487, -0.5591, -0.5727])\\n',\n","   'Id': 523,\n","   'Question': 'How to use torch.atan, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.atan.html#torch.atan'},\n","  {'Answer': \">>> torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])\\ntensor([[ 0.1000,  1.2000],\\n        [ 2.2000,  3.1000],\\n        [ 4.9000,  5.2000]])\\n\\n>>> torch.tensor([0, 1])  # Type inference on data\\ntensor([ 0,  1])\\n\\n>>> torch.tensor([[0.11111, 0.222222, 0.3333333]],\\n...              dtype=torch.float64,\\n...              device=torch.device('cuda:0'))  # creates a torch.cuda.DoubleTensor\\ntensor([[ 0.1111,  0.2222,  0.3333]], dtype=torch.float64, device='cuda:0')\\n\\n>>> torch.tensor(3.14159)  # Create a scalar (zero-dimensional tensor)\\ntensor(3.1416)\\n\\n>>> torch.tensor([])  # Create an empty tensor (of size (0,))\\ntensor([])\\n\",\n","   'Id': 524,\n","   'Question': 'How to use torch.tensor, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor'},\n","  {'Answer': '>>> a = torch.triu_indices(3, 3)\\n>>> a\\ntensor([[0, 0, 0, 1, 1, 2],\\n        [0, 1, 2, 1, 2, 2]])\\n\\n>>> a = torch.triu_indices(4, 3, -1)\\n>>> a\\ntensor([[0, 0, 0, 1, 1, 1, 2, 2, 3],\\n        [0, 1, 2, 0, 1, 2, 1, 2, 2]])\\n\\n>>> a = torch.triu_indices(4, 3, 1)\\n>>> a\\ntensor([[0, 0, 1],\\n        [1, 2, 2]])\\n',\n","   'Id': 525,\n","   'Question': 'How to use torch.triu_indices, give an example?',\n","   'context': ' The argumentoffsetcontrols which diagonal to consider. Ifoffset= 0, all elements on and above the main diagonal are\\nretained. A positive value excludes just as many diagonals above the main\\ndiagonal, and similarly a negative value includes just as many diagonals below\\nthe main diagonal. The main diagonal are the set of indices{(i,i)}\\\\lbrace (i, i) \\\\rbrace{(i,i)}fori∈[0,min\\u2061{d1,d2}−1]i \\\\in [0, \\\\min\\\\{d_{1}, d_{2}\\\\} - 1]i∈[0,min{d1\\u200b,d2\\u200b}−1]whered1,d2d_{1}, d_{2}d1\\u200b,d2\\u200bare the dimensions of the matrix.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices'},\n","  {'Answer': '>>> torch.remainder(torch.tensor([-3., -2, -1, 1, 2, 3]), 2)\\ntensor([ 1.,  0.,  1.,  1.,  0.,  1.])\\n>>> torch.remainder(torch.tensor([1, 2, 3, 4, 5]), 1.5)\\ntensor([ 1.0000,  0.5000,  0.0000,  1.0000,  0.5000])\\n',\n","   'Id': 526,\n","   'Question': 'How to use torch.remainder, give an example?',\n","   'context': ' Supportsbroadcasting to a common shape,type promotion, and integer and float inputs.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder'},\n","  {'Answer': '>>> torch.eye(3)\\ntensor([[ 1.,  0.,  0.],\\n        [ 0.,  1.,  0.],\\n        [ 0.,  0.,  1.]])\\n',\n","   'Id': 527,\n","   'Question': 'How to use torch.eye, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye'},\n","  {'Answer': '>>> a = torch.randn(4)\\n>>> a\\ntensor([ 3.4742,  0.5466, -0.8008, -0.9079])\\n>>> torch.trunc(a)\\ntensor([ 3.,  0., -0., -0.])\\n',\n","   'Id': 528,\n","   'Question': 'How to use torch.trunc, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.trunc.html#torch.trunc'},\n","  {'Answer': '>>> a = torch.rand(5)\\n>>> a\\ntensor([ 0.8419,  0.8003,  0.9971,  0.5287,  0.0490])\\n\\n\\n>>> torch.log2(a)\\ntensor([-0.2483, -0.3213, -0.0042, -0.9196, -4.3504])\\n',\n","   'Id': 529,\n","   'Question': 'How to use torch.log2, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.log2.html#torch.log2'},\n","  {'Answer': \">>> i = torch.tensor([[0, 1, 1],\\n...                   [2, 0, 2]])\\n>>> v = torch.tensor([3, 4, 5], dtype=torch.float32)\\n>>> torch.sparse_coo_tensor(i, v, [2, 4])\\ntensor(indices=tensor([[0, 1, 1],\\n                       [2, 0, 2]]),\\n       values=tensor([3., 4., 5.]),\\n       size=(2, 4), nnz=3, layout=torch.sparse_coo)\\n\\n>>> torch.sparse_coo_tensor(i, v)  # Shape inference\\ntensor(indices=tensor([[0, 1, 1],\\n                       [2, 0, 2]]),\\n       values=tensor([3., 4., 5.]),\\n       size=(2, 3), nnz=3, layout=torch.sparse_coo)\\n\\n>>> torch.sparse_coo_tensor(i, v, [2, 4],\\n...                         dtype=torch.float64,\\n...                         device=torch.device('cuda:0'))\\ntensor(indices=tensor([[0, 1, 1],\\n                       [2, 0, 2]]),\\n       values=tensor([3., 4., 5.]),\\n       device='cuda:0', size=(2, 4), nnz=3, dtype=torch.float64,\\n       layout=torch.sparse_coo)\\n\\n# Create an empty sparse tensor with the following invariants:\\n#   1. sparse_dim + dense_dim = len(SparseTensor.shape)\\n#   2. SparseTensor._indices().shape = (sparse_dim, nnz)\\n#   3. SparseTensor._values().shape = (nnz, SparseTensor.shape[sparse_dim:])\\n#\\n# For instance, to create an empty sparse tensor with nnz = 0, dense_dim = 0 and\\n# sparse_dim = 1 (hence indices is a 2D tensor of shape = (1, 0))\\n>>> S = torch.sparse_coo_tensor(torch.empty([1, 0]), [], [1])\\ntensor(indices=tensor([], size=(1, 0)),\\n       values=tensor([], size=(0,)),\\n       size=(1,), nnz=0, layout=torch.sparse_coo)\\n\\n# and to create an empty sparse tensor with nnz = 0, dense_dim = 1 and\\n# sparse_dim = 1\\n>>> S = torch.sparse_coo_tensor(torch.empty([1, 0]), torch.empty([0, 2]), [1, 2])\\ntensor(indices=tensor([], size=(1, 0)),\\n       values=tensor([], size=(0, 2)),\\n       size=(1, 2), nnz=0, layout=torch.sparse_coo)\\n\",\n","   'Id': 530,\n","   'Question': 'How to use torch.sparse_coo_tensor, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor'},\n","  {'Answer': '>>> torch.logical_or(torch.tensor([True, False, True]), torch.tensor([True, False, False]))\\ntensor([ True, False,  True])\\n>>> a = torch.tensor([0, 1, 10, 0], dtype=torch.int8)\\n>>> b = torch.tensor([4, 0, 1, 0], dtype=torch.int8)\\n>>> torch.logical_or(a, b)\\ntensor([ True,  True,  True, False])\\n>>> torch.logical_or(a.double(), b.double())\\ntensor([ True,  True,  True, False])\\n>>> torch.logical_or(a.double(), b)\\ntensor([ True,  True,  True, False])\\n>>> torch.logical_or(a, b, out=torch.empty(4, dtype=torch.bool))\\ntensor([ True,  True,  True, False])\\n',\n","   'Id': 531,\n","   'Question': 'How to use torch.logical_or, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.logical_or.html#torch.logical_or'},\n","  {'Answer': 'X = torch.linalg.solve(A, B)\\n',\n","   'Id': 532,\n","   'Question': 'How to use torch.solve, give an example?',\n","   'context': ' torch.solve()is deprecated in favor oftorch.linalg.solve()and will be removed in a future PyTorch release.torch.linalg.solve()has its arguments reversed and does not return the\\nLU factorization of the input. To get the LU factorization seetorch.lu(),\\nwhich may be used withtorch.lu_solve()andtorch.lu_unpack().X=torch.solve(B,A).solutionshould be replaced with',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve'},\n","  {'Answer': '>>> A = torch.tensor([[6.80, -2.11,  5.66,  5.97,  8.23],\\n...                   [-6.05, -3.30,  5.36, -4.44,  1.08],\\n...                   [-0.45,  2.58, -2.70,  0.27,  9.04],\\n...                   [8.32,  2.71,  4.35,  -7.17,  2.14],\\n...                   [-9.67, -5.14, -7.26,  6.08, -6.87]]).t()\\n>>> B = torch.tensor([[4.02,  6.19, -8.22, -7.57, -3.03],\\n...                   [-1.56,  4.00, -8.67,  1.75,  2.86],\\n...                   [9.81, -4.09, -4.57, -8.61,  8.99]]).t()\\n>>> X, LU = torch.solve(B, A)\\n>>> torch.dist(B, torch.mm(A, X))\\ntensor(1.00000e-06 *\\n       7.0977)\\n\\n>>> # Batched solver example\\n>>> A = torch.randn(2, 3, 1, 4, 4)\\n>>> B = torch.randn(2, 3, 1, 4, 6)\\n>>> X, LU = torch.solve(B, A)\\n>>> torch.dist(B, A.matmul(X))\\ntensor(1.00000e-06 *\\n   3.6386)\\n',\n","   'Id': 533,\n","   'Question': 'How  Supports real-valued and complex-valued inputs., give an example?',\n","   'context': ' Supports real-valued and complex-valued inputs.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve'},\n","  {'Answer': '>>> a = torch.arange(-0.5, 1, 0.5)\\n>>> a\\ntensor([-0.5000,  0.0000,  0.5000])\\n>>> torch.special.entr(a)\\ntensor([  -inf, 0.0000, 0.3466])\\n',\n","   'Id': 534,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.logit'},\n","  {'Answer': '>>> torch.special.erf(torch.tensor([0, -1., 10.]))\\ntensor([ 0.0000, -0.8427,  1.0000])\\n',\n","   'Id': 535,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.logit'},\n","  {'Answer': '>>> torch.special.erfc(torch.tensor([0, -1., 10.]))\\ntensor([ 1.0000, 1.8427,  0.0000])\\n',\n","   'Id': 536,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.logit'},\n","  {'Answer': '>>> torch.special.erfinv(torch.tensor([0, 0.5, -1.]))\\ntensor([ 0.0000,  0.4769,    -inf])\\n',\n","   'Id': 537,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.logit'},\n","  {'Answer': '>>> t = torch.randn(4)\\n>>> t\\ntensor([ 0.9213,  1.0887, -0.8858, -1.7683])\\n>>> torch.special.expit(t)\\ntensor([ 0.7153,  0.7481,  0.2920,  0.1458])\\n',\n","   'Id': 538,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.logit'},\n","  {'Answer': '>>> torch.special.expm1(torch.tensor([0, math.log(2.)]))\\ntensor([ 0.,  1.])\\n',\n","   'Id': 539,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.logit'},\n","  {'Answer': '>>> torch.special.exp2(torch.tensor([0, math.log2(2.), 3, 4]))\\ntensor([ 1.,  2.,  8., 16.])\\n',\n","   'Id': 540,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.logit'},\n","  {'Answer': '>>> a = torch.arange(0.5, 2, 0.5)\\n>>> torch.special.gammaln(a)\\ntensor([ 0.5724,  0.0000, -0.1208])\\n',\n","   'Id': 541,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.logit'},\n","  {'Answer': '>>> torch.special.i0e(torch.arange(5, dtype=torch.float32))\\ntensor([1.0000, 0.4658, 0.3085, 0.2430, 0.2070])\\n',\n","   'Id': 542,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.logit'},\n","  {'Answer': '>>> a = torch.rand(5)\\n>>> a\\ntensor([0.2796, 0.9331, 0.6486, 0.1523, 0.6516])\\n>>> torch.special.logit(a, eps=1e-6)\\ntensor([-0.9466,  2.6352,  0.6131, -1.7169,  0.6261])\\n',\n","   'Id': 543,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.logit'},\n","  {'Answer': \">>> x = torch.zeros(5,)\\n>>> y = torch.tensor([-1, 0, 1, float('inf'), float('nan')])\\n>>> torch.special.xlog1py(x, y)\\ntensor([0., 0., 0., 0., nan])\\n>>> x = torch.tensor([1, 2, 3])\\n>>> y = torch.tensor([3, 2, 1])\\n>>> torch.special.xlog1py(x, y)\\ntensor([1.3863, 2.1972, 2.0794])\\n>>> torch.special.xlog1py(x, 4)\\ntensor([1.6094, 3.2189, 4.8283])\\n>>> torch.special.xlog1py(2, y)\\ntensor([2.7726, 2.1972, 1.3863])\\n\",\n","   'Id': 544,\n","   'Question': 'How  Similar to SciPy’sscipy.special.xlog1py., give an example?',\n","   'context': ' Similar to SciPy’sscipy.special.xlog1py.',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.logit'},\n","  {'Answer': '>>> x = torch.tensor([1, 2, 3, 5])\\n>>> torch.vander(x)\\ntensor([[  1,   1,   1,   1],\\n        [  8,   4,   2,   1],\\n        [ 27,   9,   3,   1],\\n        [125,  25,   5,   1]])\\n>>> torch.vander(x, N=3)\\ntensor([[ 1,  1,  1],\\n        [ 4,  2,  1],\\n        [ 9,  3,  1],\\n        [25,  5,  1]])\\n>>> torch.vander(x, N=3, increasing=True)\\ntensor([[ 1,  1,  1],\\n        [ 1,  2,  4],\\n        [ 1,  3,  9],\\n        [ 1,  5, 25]])\\n',\n","   'Id': 545,\n","   'Question': 'How to use torch.vander, give an example?',\n","   'context': ' The columns of the output matrix are elementwise powers of the input vectorx(N−1),x(N−2),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N−1),x(N−2),...,x0.\\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N−1)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N−1). Such a\\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander'},\n","  {'Answer': '>>> x = torch.arange(9.)\\n>>> mantissa, exponent = torch.frexp(x)\\n>>> mantissa\\ntensor([0.0000, 0.5000, 0.5000, 0.7500, 0.5000, 0.6250, 0.7500, 0.8750, 0.5000])\\n>>> exponent\\ntensor([0, 1, 2, 2, 3, 3, 3, 3, 4], dtype=torch.int32)\\n>>> torch.ldexp(mantissa, exponent)\\ntensor([0., 1., 2., 3., 4., 5., 6., 7., 8.])\\n',\n","   'Id': 546,\n","   'Question': 'How to use torch.frexp, give an example?',\n","   'context': ' Supports float inputs.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.frexp.html#torch.frexp'},\n","  {'Answer': '>>> t = torch.randn(1, 3)\\n>>> t1 = torch.randn(3, 1)\\n>>> t2 = torch.randn(1, 3)\\n>>> torch.addcdiv(t, t1, t2, value=0.1)\\ntensor([[-0.2312, -3.6496,  0.1312],\\n        [-1.0428,  3.4292, -0.1030],\\n        [-0.5369, -0.9829,  0.0430]])\\n',\n","   'Id': 547,\n","   'Question': 'How to use torch.addcdiv, give an example?',\n","   'context': ' For inputs of typeFloatTensororDoubleTensor,valuemust be\\na real number, otherwise an integer.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv'},\n","  {'Answer': '>>> t = torch.randn(3,2,1)\\n>>> t\\ntensor([[[-0.3362],\\n        [-0.8437]],\\n\\n        [[-0.9627],\\n        [ 0.1727]],\\n\\n        [[ 0.5173],\\n        [-0.1398]]])\\n>>> torch.moveaxis(t, 1, 0).shape\\ntorch.Size([2, 3, 1])\\n>>> torch.moveaxis(t, 1, 0)\\ntensor([[[-0.3362],\\n        [-0.9627],\\n        [ 0.5173]],\\n\\n        [[-0.8437],\\n        [ 0.1727],\\n        [-0.1398]]])\\n>>> torch.moveaxis(t, (1, 2), (0, 1)).shape\\ntorch.Size([2, 1, 3])\\n>>> torch.moveaxis(t, (1, 2), (0, 1))\\ntensor([[[-0.3362, -0.9627,  0.5173]],\\n\\n        [[-0.8437,  0.1727, -0.1398]]])\\n',\n","   'Id': 548,\n","   'Question': 'How to use torch.moveaxis, give an example?',\n","   'context': ' This function is equivalent to NumPy’s moveaxis function.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.moveaxis.html#torch.moveaxis'},\n","  {'Answer': '>>> a = torch.arange(0.5, 2, 0.5)\\n>>> torch.lgamma(a)\\ntensor([ 0.5724,  0.0000, -0.1208])\\n',\n","   'Id': 549,\n","   'Question': 'How to use torch.lgamma, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.lgamma.html#torch.lgamma'},\n","  {'Answer': '>>> import torch\\n>>>\\n>>> def callback(fut):\\n>>>     print(f\"This will run after the future has finished.\")\\n>>>     print(fut.wait())\\n>>>\\n>>> fut = torch.futures.Future()\\n>>> fut.add_done_callback(callback)\\n>>> fut.set_result(5)\\n>>>\\n>>> # Outputs are:\\n>>> This will run after the future has finished.\\n>>> 5\\n',\n","   'Id': 550,\n","   'Question': 'How to use torch.futures.Future.add_done_callback, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/futures.html'},\n","  {'Answer': '>>> import torch\\n>>>\\n>>> fut = torch.futures.Future()\\n>>> fut.set_exception(ValueError(\"foo\"))\\n>>> fut.wait()\\n>>>\\n>>> # Output:\\n>>> # This will run after the future has finished.\\n>>> ValueError: foo\\n',\n","   'Id': 551,\n","   'Question': 'How to use torch.futures.Future.set_exception, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/futures.html'},\n","  {'Answer': '>>> import threading\\n>>> import time\\n>>> import torch\\n>>>\\n>>> def slow_set_future(fut, value):\\n>>>     time.sleep(0.5)\\n>>>     fut.set_result(value)\\n>>>\\n>>> fut = torch.futures.Future()\\n>>> t = threading.Thread(\\n>>>     target=slow_set_future,\\n>>>     args=(fut, torch.ones(2) * 3)\\n>>> )\\n>>> t.start()\\n>>>\\n>>> print(fut.wait())  # tensor([3., 3.])\\n>>> t.join()\\n',\n","   'Id': 552,\n","   'Question': 'How to use torch.futures.Future.set_result, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/futures.html'},\n","  {'Answer': '>>> import torch\\n>>>\\n>>> def callback(fut):\\n>>>     print(f\"RPC return value is {fut.wait()}.\")\\n>>>\\n>>> fut = torch.futures.Future()\\n>>> # The inserted callback will print the return value when\\n>>> # receiving the response from \"worker1\"\\n>>> cb_fut = fut.then(callback)\\n>>> chain_cb_fut = cb_fut.then(\\n>>>     lambda x : print(f\"Chained cb done. {x.wait()}\")\\n>>> )\\n>>> fut.set_result(5)\\n>>>\\n>>> # Outputs are:\\n>>> # RPC return value is 5.\\n>>> # Chained cb done. None\\n',\n","   'Id': 553,\n","   'Question': 'How to use torch.futures.Future.then, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/futures.html'},\n","  {'Answer': '>>> import torch\\n>>>\\n>>> fut0 = torch.futures.Future()\\n>>> fut1 = torch.futures.Future()\\n>>>\\n>>> fut = torch.futures.collect_all([fut0, fut1])\\n>>>\\n>>> fut0.set_result(0)\\n>>> fut1.set_result(1)\\n>>>\\n>>> fut_list = fut.wait()\\n>>> print(f\"fut0 result = {fut_list[0].wait()}\")\\n>>> print(f\"fut1 result = {fut_list[1].wait()}\")\\n>>> # outputs:\\n>>> # fut0 result = 0\\n>>> # fut1 result = 1\\n',\n","   'Id': 554,\n","   'Question': 'How to use torch.futures.collect_all, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/futures.html'},\n","  {'Answer': '>>> torch.fmod(torch.tensor([-3., -2, -1, 1, 2, 3]), 2)\\ntensor([-1., -0., -1.,  1.,  0.,  1.])\\n>>> torch.fmod(torch.tensor([1, 2, 3, 4, 5]), 1.5)\\ntensor([1.0000, 0.5000, 0.0000, 1.0000, 0.5000])\\n',\n","   'Id': 555,\n","   'Question': 'How to use torch.fmod, give an example?',\n","   'context': ' Supportsbroadcasting to a common shape,type promotion, and integer and float inputs.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod'},\n","  {'Answer': '>>> x = torch.randn(4)\\n>>> x\\ntensor([-1.5393, -0.8675,  0.5916,  1.6321])\\n>>> y = torch.randn(4)\\n>>> y\\ntensor([ 0.0967, -1.0511,  0.6295,  0.8360])\\n>>> torch.dist(x, y, 3.5)\\ntensor(1.6727)\\n>>> torch.dist(x, y, 3)\\ntensor(1.6973)\\n>>> torch.dist(x, y, 0)\\ntensor(inf)\\n>>> torch.dist(x, y, 1)\\ntensor(2.6537)\\n',\n","   'Id': 556,\n","   'Question': 'How to use torch.dist, give an example?',\n","   'context': ' The shapes ofinputandothermust bebroadcastable.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.dist.html#torch.dist'},\n","  {'Answer': \">>> a=torch.empty((2,3), dtype=torch.int32, device = 'cuda')\\n>>> torch.empty_like(a)\\ntensor([[0, 0, 0],\\n        [0, 0, 0]], device='cuda:0', dtype=torch.int32)\\n\",\n","   'Id': 557,\n","   'Question': 'How to use torch.empty, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.empty.html#torch.empty'},\n","  {'Answer': \">>> a = torch.tensor([1., 2., float('nan'), 4.])\\n>>> torch.nansum(a)\\ntensor(7.)\\n\",\n","   'Id': 558,\n","   'Question': 'How to use torch.nansum, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.nansum.html#torch.nansum'},\n","  {'Answer': '>>> torch.nansum(torch.tensor([1., float(\"nan\")]))\\n1.0\\n>>> a = torch.tensor([[1, 2], [3., float(\"nan\")]])\\n>>> torch.nansum(a)\\ntensor(6.)\\n>>> torch.nansum(a, dim=0)\\ntensor([4., 2.])\\n>>> torch.nansum(a, dim=1)\\ntensor([3., 3.])\\n',\n","   'Id': 559,\n","   'Question': 'How  IfkeepdimisTrue, the output tensor is of the same size\\nasinputexcept in the dimension(s)dimwhere it is of size 1.\\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\\noutput tensor having 1 (orlen(dim)) fewer dimension(s)., give an example?',\n","   'context': ' IfkeepdimisTrue, the output tensor is of the same size\\nasinputexcept in the dimension(s)dimwhere it is of size 1.\\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\\noutput tensor having 1 (orlen(dim)) fewer dimension(s).',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.nansum.html#torch.nansum'},\n","  {'Answer': '>>> a = torch.randn(3, 3)\\n>>> a\\ntensor([[-1.0854,  1.1431, -0.1752],\\n        [ 0.8536, -0.0905,  0.0360],\\n        [ 0.6927, -0.3735, -0.4945]])\\n\\n\\n>>> torch.diagonal(a, 0)\\ntensor([-1.0854, -0.0905, -0.4945])\\n\\n\\n>>> torch.diagonal(a, 1)\\ntensor([ 1.1431,  0.0360])\\n\\n\\n>>> x = torch.randn(2, 5, 4, 2)\\n>>> torch.diagonal(x, offset=-1, dim1=1, dim2=2)\\ntensor([[[-1.2631,  0.3755, -1.5977, -1.8172],\\n         [-1.1065,  1.0401, -0.2235, -0.7938]],\\n\\n        [[-1.7325, -0.3081,  0.6166,  0.2335],\\n         [ 1.0500,  0.7336, -0.3836, -1.1015]]])\\n',\n","   'Id': 560,\n","   'Question': 'How to use torch.diagonal, give an example?',\n","   'context': ' Applyingtorch.diag_embed()to the output of this function with\\nthe same arguments yields a diagonal matrix with the diagonal entries\\nof the input. However,torch.diag_embed()has different default\\ndimensions, so those need to be explicitly specified.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.diagonal.html#torch.diagonal'},\n","  {'Answer': '>>> torch.le(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\\ntensor([[True, False], [True, True]])\\n',\n","   'Id': 561,\n","   'Question': 'How to use torch.le, give an example?',\n","   'context': ' The second argument can be a number or a tensor whose shape isbroadcastablewith the first argument.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.le.html#torch.le'},\n","  {'Answer': '>>> x = torch.zeros(1, requires_grad=True)\\n>>> with torch.no_grad():\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> is_train = False\\n>>> with torch.set_grad_enabled(is_train):\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> torch.set_grad_enabled(True)  # this can also be used as a function\\n>>> y = x * 2\\n>>> y.requires_grad\\nTrue\\n\\n>>> torch.set_grad_enabled(False)\\n>>> y = x * 2\\n>>> y.requires_grad\\nFalse\\n',\n","   'Id': 562,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/torch.html#locally-disabling-gradient-computation'},\n","  {'Answer': '>>> torch.rand(4)\\ntensor([ 0.5204,  0.2503,  0.3525,  0.5673])\\n>>> torch.rand(2, 3)\\ntensor([[ 0.8237,  0.5781,  0.6879],\\n        [ 0.3816,  0.7249,  0.0998]])\\n',\n","   'Id': 563,\n","   'Question': 'How to use torch.rand, give an example?',\n","   'context': ' The shape of the tensor is defined by the variable argumentsize.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand'},\n","  {'Answer': '>>> t = torch.tensor([[10, 30, 20], [60, 40, 50]])\\n>>> max_idx = torch.argmax(t)\\n>>> torch.take_along_dim(t, max_idx)\\ntensor([60])\\n>>> sorted_idx = torch.argsort(t, dim=1)\\n>>> torch.take_along_dim(t, sorted_idx, dim=1)\\ntensor([[10, 20, 30],\\n        [40, 50, 60]])\\n',\n","   'Id': 564,\n","   'Question': 'How to use torch.take_along_dim, give an example?',\n","   'context': ' Functions that return indices along a dimension, liketorch.argmax()andtorch.argsort(),\\nare designed to work with this function. See the examples below.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.take_along_dim.html#torch.take_along_dim'},\n","  {'Answer': \">>> a = torch.tensor([-float('inf'), float('inf'), 1.2])\\n>>> torch.isposinf(a)\\ntensor([False,  True, False])\\n\",\n","   'Id': 565,\n","   'Question': 'How to use torch.isposinf, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.isposinf.html#torch.isposinf'},\n","  {'Answer': '>>> torch.logical_and(torch.tensor([True, False, True]), torch.tensor([True, False, False]))\\ntensor([ True, False, False])\\n>>> a = torch.tensor([0, 1, 10, 0], dtype=torch.int8)\\n>>> b = torch.tensor([4, 0, 1, 0], dtype=torch.int8)\\n>>> torch.logical_and(a, b)\\ntensor([False, False,  True, False])\\n>>> torch.logical_and(a.double(), b.double())\\ntensor([False, False,  True, False])\\n>>> torch.logical_and(a.double(), b)\\ntensor([False, False,  True, False])\\n>>> torch.logical_and(a, b, out=torch.empty(4, dtype=torch.bool))\\ntensor([False, False,  True, False])\\n',\n","   'Id': 566,\n","   'Question': 'How to use torch.logical_and, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.logical_and.html#torch.logical_and'},\n","  {'Answer': '>>> a = torch.randn(5)\\n>>> a\\ntensor([ 0.0090, -0.2262, -0.0682, -0.2866,  0.3940])\\n>>> torch.neg(a)\\ntensor([-0.0090,  0.2262,  0.0682,  0.2866, -0.3940])\\n',\n","   'Id': 567,\n","   'Question': 'How to use torch.neg, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.neg.html#torch.neg'},\n","  {'Answer': '>>> x = torch.arange(1., 10.).view(3, 3)\\n>>> x\\ntensor([[ 1.,  2.,  3.],\\n        [ 4.,  5.,  6.],\\n        [ 7.,  8.,  9.]])\\n>>> torch.trace(x)\\ntensor(15.)\\n',\n","   'Id': 568,\n","   'Question': 'How to use torch.trace, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.trace.html#torch.trace'},\n","  {'Answer': \"import torch.distributed as dist\\n\\n# Use address of one of the machines\\ndist.init_process_group(backend, init_method='tcp://10.1.1.20:23456',\\n                        rank=args.rank, world_size=4)\\n\",\n","   'Id': 569,\n","   'Question': 'How to use There are two ways to initialize using TCP, both requiring a network address\\nreachable from all processes and a desiredworld_size. The first way\\nrequires specifying an address that belongs to the rank 0 process. This\\ninitialization method requires that all processes have manually specified ranks.Note that multicast address is not supported anymore in the latest distributed\\npackage.group_nameis deprecated as well., give an example?',\n","   'context': ' Note that multicast address is not supported anymore in the latest distributed\\npackage.group_nameis deprecated as well.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': \"import torch.distributed as dist\\n\\n# rank should always be specified\\ndist.init_process_group(backend, init_method='file:///mnt/nfs/sharedfile',\\n                        world_size=4, rank=args.rank)\\n\",\n","   'Id': 570,\n","   'Question': 'How to use Another initialization method makes use of a file system that is shared and\\nvisible from all machines in a group, along with a desiredworld_size. The URL should start\\nwithfile://and contain a path to a non-existent file (in an existing\\ndirectory) on a shared file system. File-system initialization will automatically\\ncreate that file if it doesn’t exist, but will not delete the file. Therefore, it\\nis your responsibility to make sure that the file is cleaned up before the nextinit_process_group()call on the same file path/name.Note that automatic rank assignment is not supported anymore in the latest\\ndistributed package andgroup_nameis deprecated as well., give an example?',\n","   'context': ' Note that automatic rank assignment is not supported anymore in the latest\\ndistributed package andgroup_nameis deprecated as well.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> import torch.distributed as dist\\n>>> from datetime import timedelta\\n>>> # Run on process 1 (server)\\n>>> server_store = dist.TCPStore(\"127.0.0.1\", 1234, 2, True, timedelta(seconds=30))\\n>>> # Run on process 2 (client)\\n>>> client_store = dist.TCPStore(\"127.0.0.1\", 1234, 2, False)\\n>>> # Use any of the store methods from either the client or server after initialization\\n>>> server_store.set(\"first_key\", \"first_value\")\\n>>> client_store.get(\"first_key\")\\n',\n","   'Id': 571,\n","   'Question': 'How to use Distributed Key Value Store, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> import torch.distributed as dist\\n>>> store = dist.HashStore()\\n>>> # store can be used from other threads\\n>>> # Use any of the store methods after initialization\\n>>> store.set(\"first_key\", \"first_value\")\\n',\n","   'Id': 572,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> import torch.distributed as dist\\n>>> store1 = dist.FileStore(\"/tmp/filestore\", 2)\\n>>> store2 = dist.FileStore(\"/tmp/filestore\", 2)\\n>>> # Use any of the store methods from either the client or server after initialization\\n>>> store1.set(\"first_key\", \"first_value\")\\n>>> store2.get(\"first_key\")\\n',\n","   'Id': 573,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> import torch.distributed as dist\\n>>> from datetime import timedelta\\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\\n>>> store.set(\"first_key\", \"first_value\")\\n>>> # Should return \"first_value\"\\n>>> store.get(\"first_key\")\\n',\n","   'Id': 574,\n","   'Question': 'How to use torch.distributed.Store.set, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> import torch.distributed as dist\\n>>> from datetime import timedelta\\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\\n>>> store.set(\"first_key\", \"first_value\")\\n>>> # Should return \"first_value\"\\n>>> store.get(\"first_key\")\\n',\n","   'Id': 575,\n","   'Question': 'How to use torch.distributed.Store.get, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> import torch.distributed as dist\\n>>> from datetime import timedelta\\n>>> # Using TCPStore as an example, other store types can also be used\\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\\n>>> store.add(\"first_key\", 1)\\n>>> store.add(\"first_key\", 6)\\n>>> # Should return 7\\n>>> store.get(\"first_key\")\\n',\n","   'Id': 576,\n","   'Question': 'How to use torch.distributed.Store.add, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> import torch.distributed as dist\\n>>> from datetime import timedelta\\n>>> # Using TCPStore as an example, other store types can also be used\\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\\n>>> # This will throw an exception after 30 seconds\\n>>> store.wait([\"bad_key\"])\\n',\n","   'Id': 577,\n","   'Question': 'How to use torch.distributed.Store.wait, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> import torch.distributed as dist\\n>>> from datetime import timedelta\\n>>> # Using TCPStore as an example, other store types can also be used\\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\\n>>> # This will throw an exception after 10 seconds\\n>>> store.wait([\"bad_key\"], timedelta(seconds=10))\\n',\n","   'Id': 578,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> import torch.distributed as dist\\n>>> from datetime import timedelta\\n>>> # Using TCPStore as an example, other store types can also be used\\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\\n>>> store.set(\"first_key\", \"first_value\")\\n>>> # This should return 2\\n>>> store.num_keys()\\n',\n","   'Id': 579,\n","   'Question': 'How to use torch.distributed.Store.num_keys, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> import torch.distributed as dist\\n>>> from datetime import timedelta\\n>>> # Using TCPStore as an example, HashStore can also be used\\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\\n>>> store.set(\"first_key\")\\n>>> # This should return true\\n>>> store.delete_key(\"first_key\")\\n>>> # This should return false\\n>>> store.delete_key(\"bad_key\")\\n',\n","   'Id': 580,\n","   'Question': 'How to use torch.distributed.Store.delete_key, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> import torch.distributed as dist\\n>>> from datetime import timedelta\\n>>> # Using TCPStore as an example, other store types can also be used\\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\\n>>> store.set_timeout(timedelta(seconds=10))\\n>>> # This will throw an exception after 10 seconds\\n>>> store.wait([\"bad_key\"])\\n',\n","   'Id': 581,\n","   'Question': 'How to use torch.distributed.Store.set_timeout, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '# Code runs on each rank.\\ndist.init_process_group(\"nccl\", rank=rank, world_size=2)\\noutput = torch.tensor([rank]).cuda(rank)\\ns = torch.cuda.Stream()\\nhandle = dist.all_reduce(output, async_op=True)\\n# Wait ensures the operation is enqueued, but not necessarily complete.\\nhandle.wait()\\n# Using result on non-default stream.\\nwith torch.cuda.stream(s):\\n    s.wait_stream(torch.cuda.default_stream())\\n    output.add_(100)\\nif rank == 0:\\n    # if the explicit call to wait_stream was omitted, the output below will be\\n    # non-deterministically 1 or 101, depending on whether the allreduce overwrote\\n    # the value after the add completed.\\n    print(output)\\n',\n","   'Id': 582,\n","   'Question': 'How to use The following code can serve as a reference regarding semantics for CUDA operations when using distributed collectives.\\nIt shows the explicit need to synchronize when using collective outputs on different CUDA streams:, give an example?',\n","   'context': ' The following code can serve as a reference regarding semantics for CUDA operations when using distributed collectives.\\nIt shows the explicit need to synchronize when using collective outputs on different CUDA streams:',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> # Note: Process group initialization omitted on each rank.\\n>>> import torch.distributed as dist\\n>>> if dist.get_rank() == 0:\\n>>>     # Assumes world_size of 3.\\n>>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\\n>>> else:\\n>>>     objects = [None, None, None]\\n>>> dist.broadcast_object_list(objects, src=0)\\n>>> broadcast_objects\\n[\\'foo\\', 12, {1: 2}]\\n',\n","   'Id': 583,\n","   'Question': 'How to use torch.distributed.broadcast_object_list, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> # All tensors below are of torch.int64 type.\\n>>> # We have 2 process groups, 2 ranks.\\n>>> tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank\\n>>> tensor\\ntensor([1, 2]) # Rank 0\\ntensor([3, 4]) # Rank 1\\n>>> dist.all_reduce(tensor, op=ReduceOp.SUM)\\n>>> tensor\\ntensor([4, 6]) # Rank 0\\ntensor([4, 6]) # Rank 1\\n',\n","   'Id': 584,\n","   'Question': 'How to use torch.distributed.all_reduce, give an example?',\n","   'context': ' Complex tensors are supported.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> # All tensors below are of torch.cfloat type.\\n>>> # We have 2 process groups, 2 ranks.\\n>>> tensor = torch.tensor([1+1j, 2+2j], dtype=torch.cfloat) + 2 * rank * (1+1j)\\n>>> tensor\\ntensor([1.+1.j, 2.+2.j]) # Rank 0\\ntensor([3.+3.j, 4.+4.j]) # Rank 1\\n>>> dist.all_reduce(tensor, op=ReduceOp.SUM)\\n>>> tensor\\ntensor([4.+4.j, 6.+6.j]) # Rank 0\\ntensor([4.+4.j, 6.+6.j]) # Rank 1\\n',\n","   'Id': 585,\n","   'Question': 'How  Complex tensors are supported., give an example?',\n","   'context': ' Complex tensors are supported.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> # All tensors below are of torch.int64 dtype.\\n>>> # We have 2 process groups, 2 ranks.\\n>>> tensor_list = [torch.zero(2, dtype=torch.int64) for _ in range(2)]\\n>>> tensor_list\\n[tensor([0, 0]), tensor([0, 0])] # Rank 0 and 1\\n>>> tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank\\n>>> tensor\\ntensor([1, 2]) # Rank 0\\ntensor([3, 4]) # Rank 1\\n>>> dist.all_gather(tensor_list, tensor)\\n>>> tensor_list\\n[tensor([1, 2]), tensor([3, 4])] # Rank 0\\n[tensor([1, 2]), tensor([3, 4])] # Rank 1\\n',\n","   'Id': 586,\n","   'Question': 'How to use torch.distributed.all_gather, give an example?',\n","   'context': ' Complex tensors are supported.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> # All tensors below are of torch.cfloat dtype.\\n>>> # We have 2 process groups, 2 ranks.\\n>>> tensor_list = [torch.zero(2, dtype=torch.cfloat) for _ in range(2)]\\n>>> tensor_list\\n[tensor([0.+0.j, 0.+0.j]), tensor([0.+0.j, 0.+0.j])] # Rank 0 and 1\\n>>> tensor = torch.tensor([1+1j, 2+2j], dtype=torch.cfloat) + 2 * rank * (1+1j)\\n>>> tensor\\ntensor([1.+1.j, 2.+2.j]) # Rank 0\\ntensor([3.+3.j, 4.+4.j]) # Rank 1\\n>>> dist.all_gather(tensor_list, tensor)\\n>>> tensor_list\\n[tensor([1.+1.j, 2.+2.j]), tensor([3.+3.j, 4.+4.j])] # Rank 0\\n[tensor([1.+1.j, 2.+2.j]), tensor([3.+3.j, 4.+4.j])] # Rank 1\\n',\n","   'Id': 587,\n","   'Question': 'How  Complex tensors are supported., give an example?',\n","   'context': ' Complex tensors are supported.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> # Note: Process group initialization omitted on each rank.\\n>>> import torch.distributed as dist\\n>>> # Assumes world_size of 3.\\n>>> gather_objects = [\"foo\", 12, {1: 2}] # any picklable object\\n>>> output = [None for _ in gather_objects]\\n>>> dist.all_gather_object(output, gather_objects[dist.get_rank()])\\n>>> output\\n[\\'foo\\', 12, {1: 2}]\\n',\n","   'Id': 588,\n","   'Question': 'How to use torch.distributed.all_gather_object, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> # Note: Process group initialization omitted on each rank.\\n>>> import torch.distributed as dist\\n>>> # Assumes world_size of 3.\\n>>> gather_objects = [\"foo\", 12, {1: 2}] # any picklable object\\n>>> output = [None for _ in gather_objects]\\n>>> dist.gather_object(\\n        gather_objects[dist.get_rank()],\\n        output if dist.get_rank() == 0 else None,\\n        dst=0\\n    )\\n>>> # On rank 0\\n>>> output\\n[\\'foo\\', 12, {1: 2}]\\n',\n","   'Id': 589,\n","   'Question': 'How to use torch.distributed.gather_object, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> # Note: Process group initialization omitted on each rank.\\n>>> import torch.distributed as dist\\n>>> if dist.get_rank() == 0:\\n>>>     # Assumes world_size of 3.\\n>>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\\n>>> else:\\n>>>     # Can be any list on non-src ranks, elements are not used.\\n>>>     objects = [None, None, None]\\n>>> output_list = [None]\\n>>> dist.scatter_object_list(output_list, objects, src=0)\\n>>> # Rank i gets objects[i]. For example, on rank 2:\\n>>> output_list\\n[{1: 2}]\\n',\n","   'Id': 590,\n","   'Question': 'How to use torch.distributed.scatter_object_list, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> input = torch.arange(4) + rank * 4\\n>>> input = list(input.chunk(4))\\n>>> input\\n[tensor([0]), tensor([1]), tensor([2]), tensor([3])]     # Rank 0\\n[tensor([4]), tensor([5]), tensor([6]), tensor([7])]     # Rank 1\\n[tensor([8]), tensor([9]), tensor([10]), tensor([11])]   # Rank 2\\n[tensor([12]), tensor([13]), tensor([14]), tensor([15])] # Rank 3\\n>>> output = list(torch.empty([4], dtype=torch.int64).chunk(4))\\n>>> dist.all_to_all(output, input)\\n>>> output\\n[tensor([0]), tensor([4]), tensor([8]), tensor([12])]    # Rank 0\\n[tensor([1]), tensor([5]), tensor([9]), tensor([13])]    # Rank 1\\n[tensor([2]), tensor([6]), tensor([10]), tensor([14])]   # Rank 2\\n[tensor([3]), tensor([7]), tensor([11]), tensor([15])]   # Rank 3\\n',\n","   'Id': 591,\n","   'Question': 'How to use torch.distributed.all_to_all, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> # Essentially, it is similar to following operation:\\n>>> scatter_list = input\\n>>> gather_list  = output\\n>>> for i in range(world_size):\\n>>>   dist.scatter(gather_list[i], scatter_list if i == rank else [], src = i)\\n',\n","   'Id': 592,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> input\\ntensor([0, 1, 2, 3, 4, 5])                                       # Rank 0\\ntensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1\\ntensor([20, 21, 22, 23, 24])                                     # Rank 2\\ntensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3\\n>>> input_splits\\n[2, 2, 1, 1]                                                     # Rank 0\\n[3, 2, 2, 2]                                                     # Rank 1\\n[2, 1, 1, 1]                                                     # Rank 2\\n[2, 2, 2, 1]                                                     # Rank 3\\n>>> output_splits\\n[2, 3, 2, 2]                                                     # Rank 0\\n[2, 2, 1, 2]                                                     # Rank 1\\n[1, 2, 1, 2]                                                     # Rank 2\\n[1, 2, 1, 1]                                                     # Rank 3\\n>>> input = list(input.split(input_splits))\\n>>> input\\n[tensor([0, 1]), tensor([2, 3]), tensor([4]), tensor([5])]                   # Rank 0\\n[tensor([10, 11, 12]), tensor([13, 14]), tensor([15, 16]), tensor([17, 18])] # Rank 1\\n[tensor([20, 21]), tensor([22]), tensor([23]), tensor([24])]                 # Rank 2\\n[tensor([30, 31]), tensor([32, 33]), tensor([34, 35]), tensor([36])]         # Rank 3\\n>>> output = ...\\n>>> dist.all_to_all(output, input)\\n>>> output\\n[tensor([0, 1]), tensor([10, 11, 12]), tensor([20, 21]), tensor([30, 31])]   # Rank 0\\n[tensor([2, 3]), tensor([13, 14]), tensor([22]), tensor([32, 33])]           # Rank 1\\n[tensor([4]), tensor([15, 16]), tensor([23]), tensor([34, 35])]              # Rank 2\\n[tensor([5]), tensor([17, 18]), tensor([24]), tensor([36])]                  # Rank 3\\n',\n","   'Id': 593,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': 'import torch\\nimport torch.distributed as dist\\nwith torch.profiler():\\n    tensor = torch.randn(20, 10)\\n    dist.all_reduce(tensor)\\n',\n","   'Id': 594,\n","   'Question': 'How to use Note that you can usetorch.profiler(recommended, only available after 1.8.1)  ortorch.autograd.profilerto profile collective communication and point-to-point communication APIs mentioned here. All out-of-the-box backends (gloo,nccl,mpi) are supported and collective communication usage will be rendered as expected in profiling output/traces. Profiling your code is the same as any regular torch operator:, give an example?',\n","   'context': ' Note that you can usetorch.profiler(recommended, only available after 1.8.1)  ortorch.autograd.profilerto profile collective communication and point-to-point communication APIs mentioned here. All out-of-the-box backends (gloo,nccl,mpi) are supported and collective communication usage will be rendered as expected in profiling output/traces. Profiling your code is the same as any regular torch operator:',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': 'import torch\\nimport torch.distributed as dist\\n\\ndist.init_process_group(backend=\"nccl\",\\n                        init_method=\"file:///distributed_test\",\\n                        world_size=2,\\n                        rank=0)\\ntensor_list = []\\nfor dev_idx in range(torch.cuda.device_count()):\\n    tensor_list.append(torch.FloatTensor([1]).cuda(dev_idx))\\n\\ndist.all_reduce_multigpu(tensor_list)\\n',\n","   'Id': 595,\n","   'Question': 'How to use For example, if the system we use for distributed training has 2 nodes, each\\nof which has 8 GPUs. On each of the 16 GPUs, there is a tensor that we would\\nlike to all-reduce. The following code can serve as a reference:Code running on Node 0, give an example?',\n","   'context': ' For example, if the system we use for distributed training has 2 nodes, each\\nof which has 8 GPUs. On each of the 16 GPUs, there is a tensor that we would\\nlike to all-reduce. The following code can serve as a reference:Code running on Node 0',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': 'import torch\\nimport torch.distributed as dist\\n\\ndist.init_process_group(backend=\"nccl\",\\n                        init_method=\"file:///distributed_test\",\\n                        world_size=2,\\n                        rank=1)\\ntensor_list = []\\nfor dev_idx in range(torch.cuda.device_count()):\\n    tensor_list.append(torch.FloatTensor([1]).cuda(dev_idx))\\n\\ndist.all_reduce_multigpu(tensor_list)\\n',\n","   'Id': 596,\n","   'Question': 'How to use Code running on Node 0Code running on Node 1, give an example?',\n","   'context': ' Code running on Node 0Code running on Node 1',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\\n           YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other\\n           arguments of your training script)\\n',\n","   'Id': 597,\n","   'Question': 'How to use In both cases of single-node distributed training or multi-node distributed\\ntraining, this utility will launch the given number of processes per node\\n(--nproc_per_node). If used for GPU training, this number needs to be less\\nor equal to the number of GPUs on the current system (nproc_per_node),\\nand each process will be operating on a single GPU fromGPU 0 to\\nGPU (nproc_per_node - 1).How to use this module:, give an example?',\n","   'context': ' In both cases of single-node distributed training or multi-node distributed\\ntraining, this utility will launch the given number of processes per node\\n(--nproc_per_node). If used for GPU training, this number needs to be less\\nor equal to the number of GPUs on the current system (nproc_per_node),\\nand each process will be operating on a single GPU fromGPU 0 to\\nGPU (nproc_per_node - 1).How to use this module:',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\\n           --nnodes=2 --node_rank=0 --master_addr=\"192.168.1.1\"\\n           --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3\\n           and all other arguments of your training script)\\n',\n","   'Id': 598,\n","   'Question': 'How to use How to use this module:Node 1:(IP: 192.168.1.1, and has a free port: 1234), give an example?',\n","   'context': ' How to use this module:Node 1:(IP: 192.168.1.1, and has a free port: 1234)',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\\n           --nnodes=2 --node_rank=1 --master_addr=\"192.168.1.1\"\\n           --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3\\n           and all other arguments of your training script)\\n',\n","   'Id': 599,\n","   'Question': 'How to use Node 1:(IP: 192.168.1.1, and has a free port: 1234)Node 2:, give an example?',\n","   'context': ' Node 1:(IP: 192.168.1.1, and has a free port: 1234)Node 2:',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> python -m torch.distributed.launch --help\\n',\n","   'Id': 600,\n","   'Question': 'How  Node 1:(IP: 192.168.1.1, and has a free port: 1234)Node 2:, give an example?',\n","   'context': ' Node 1:(IP: 192.168.1.1, and has a free port: 1234)Node 2:',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> import argparse\\n>>> parser = argparse.ArgumentParser()\\n>>> parser.add_argument(\"--local_rank\", type=int)\\n>>> args = parser.parse_args()\\n',\n","   'Id': 601,\n","   'Question': 'How to use 2. In your training program, you must parse the command-line argument:--local_rank=LOCAL_PROCESS_RANK, which will be provided by this module.\\nIf your training program uses GPUs, you should ensure that your code only\\nruns on the GPU device of LOCAL_PROCESS_RANK. This can be done by:Parsing the local_rank argument, give an example?',\n","   'context': ' 2. In your training program, you must parse the command-line argument:--local_rank=LOCAL_PROCESS_RANK, which will be provided by this module.\\nIf your training program uses GPUs, you should ensure that your code only\\nruns on the GPU device of LOCAL_PROCESS_RANK. This can be done by:Parsing the local_rank argument',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> torch.cuda.set_device(args.local_rank)  # before your code runs\\n',\n","   'Id': 602,\n","   'Question': 'How to use Parsing the local_rank argumentSet your device to local rank using either, give an example?',\n","   'context': ' Parsing the local_rank argumentSet your device to local rank using either',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> with torch.cuda.device(args.local_rank):\\n>>>    # your code to run\\n',\n","   'Id': 603,\n","   'Question': 'How to use Set your device to local rank using eitheror, give an example?',\n","   'context': ' Set your device to local rank using eitheror',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': \"torch.distributed.init_process_group(backend='YOUR BACKEND',\\n                                     init_method='env://')\\n\",\n","   'Id': 604,\n","   'Question': 'How to use or3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module., give an example?',\n","   'context': ' or3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': 'model = torch.nn.parallel.DistributedDataParallel(model,\\n                                                  device_ids=[args.local_rank],\\n                                                  output_device=args.local_rank)\\n',\n","   'Id': 605,\n","   'Question': 'How to use 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it., give an example?',\n","   'context': ' 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> a = torch.randn(4)\\n>>> a\\ntensor([-0.6341, -1.4208, -1.0900,  0.5826])\\n>>> torch.ceil(a)\\ntensor([-0., -1., -1.,  1.])\\n',\n","   'Id': 606,\n","   'Question': 'How to use torch.ceil, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.ceil.html#torch.ceil'},\n","  {'Answer': '>>> x = torch.zeros(1, requires_grad=True)\\n>>> with torch.no_grad():\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> is_train = False\\n>>> with torch.set_grad_enabled(is_train):\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> torch.set_grad_enabled(True)  # this can also be used as a function\\n>>> y = x * 2\\n>>> y.requires_grad\\nTrue\\n\\n>>> torch.set_grad_enabled(False)\\n>>> y = x * 2\\n>>> y.requires_grad\\nFalse\\n',\n","   'Id': 607,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/torch.html#comparison-ops'},\n","  {'Answer': '>>> a = torch.randn(3, 3)\\n>>> a\\ntensor([[ 0.9478,  0.9158, -1.1295],\\n        [ 0.9701,  0.7346, -1.8044],\\n        [-0.2337,  0.0557,  0.6929]])\\n>>> torch.linalg.det(a)\\ntensor(0.0934)\\n\\n>>> out = torch.empty(0)\\n>>> torch.linalg.det(a, out=out)\\ntensor(0.0934)\\n>>> out\\ntensor(0.0934)\\n\\n>>> a = torch.randn(3, 2, 2)\\n>>> a\\ntensor([[[ 0.9254, -0.6213],\\n         [-0.5787,  1.6843]],\\n\\n        [[ 0.3242, -0.9665],\\n         [ 0.4539, -0.0887]],\\n\\n        [[ 1.1336, -0.4025],\\n         [-0.7089,  0.9032]]])\\n>>> torch.linalg.det(a)\\ntensor([1.1990, 0.4099, 0.7386])\\n',\n","   'Id': 608,\n","   'Question': 'How to use torch.linalg.det, give an example?',\n","   'context': ' Supports input of float, double, cfloat and cdouble dtypes.\\nAlso supports batches of matrices, and ifAis a batch of matrices then\\nthe output has the same batch dimensions.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.linalg.det.html#torch.linalg.det'},\n","  {'Answer': '>>> torch.nonzero(torch.tensor([1, 1, 1, 0, 1]))\\ntensor([[ 0],\\n        [ 1],\\n        [ 2],\\n        [ 4]])\\n>>> torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],\\n...                             [0.0, 0.4, 0.0, 0.0],\\n...                             [0.0, 0.0, 1.2, 0.0],\\n...                             [0.0, 0.0, 0.0,-0.4]]))\\ntensor([[ 0,  0],\\n        [ 1,  1],\\n        [ 2,  2],\\n        [ 3,  3]])\\n>>> torch.nonzero(torch.tensor([1, 1, 1, 0, 1]), as_tuple=True)\\n(tensor([0, 1, 2, 4]),)\\n>>> torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],\\n...                             [0.0, 0.4, 0.0, 0.0],\\n...                             [0.0, 0.0, 1.2, 0.0],\\n...                             [0.0, 0.0, 0.0,-0.4]]), as_tuple=True)\\n(tensor([0, 1, 2, 3]), tensor([0, 1, 2, 3]))\\n>>> torch.nonzero(torch.tensor(5), as_tuple=True)\\n(tensor([0]),)\\n',\n","   'Id': 609,\n","   'Question': 'How to use torch.nonzero, give an example?',\n","   'context': ' As a special case, wheninputhas zero dimensions and a nonzero scalar\\nvalue, it is treated as a one-dimensional tensor with one element.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.nonzero.html#torch.nonzero'},\n","  {'Answer': '>>> a = torch.randn(4)\\n>>> a\\ntensor([-0.4595, -2.1219, -1.4314,  0.7298])\\n>>> torch.reciprocal(a)\\ntensor([-2.1763, -0.4713, -0.6986,  1.3702])\\n',\n","   'Id': 610,\n","   'Question': 'How to use torch.reciprocal, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.reciprocal.html#torch.reciprocal'},\n","  {'Answer': '>>> x = torch.zeros(1, requires_grad=True)\\n>>> with torch.no_grad():\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> is_train = False\\n>>> with torch.set_grad_enabled(is_train):\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> torch.set_grad_enabled(True)  # this can also be used as a function\\n>>> y = x * 2\\n>>> y.requires_grad\\nTrue\\n\\n>>> torch.set_grad_enabled(False)\\n>>> y = x * 2\\n>>> y.requires_grad\\nFalse\\n',\n","   'Id': 611,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/torch.html#'},\n","  {'Answer': '>>> x = torch.randn(3, 4)\\n>>> sorted, indices = torch.sort(x)\\n>>> sorted\\ntensor([[-0.2162,  0.0608,  0.6719,  2.3332],\\n        [-0.5793,  0.0061,  0.6058,  0.9497],\\n        [-0.5071,  0.3343,  0.9553,  1.0960]])\\n>>> indices\\ntensor([[ 1,  0,  2,  3],\\n        [ 3,  1,  0,  2],\\n        [ 0,  3,  1,  2]])\\n\\n>>> sorted, indices = torch.sort(x, 0)\\n>>> sorted\\ntensor([[-0.5071, -0.2162,  0.6719, -0.5793],\\n        [ 0.0608,  0.0061,  0.9497,  0.3343],\\n        [ 0.6058,  0.9553,  1.0960,  2.3332]])\\n>>> indices\\ntensor([[ 2,  0,  0,  1],\\n        [ 0,  1,  1,  2],\\n        [ 1,  2,  2,  0]])\\n>>> x = torch.tensor([0, 1] * 9)\\n>>> x.sort()\\ntorch.return_types.sort(\\n    values=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\\n    indices=tensor([ 2, 16,  4,  6, 14,  8,  0, 10, 12,  9, 17, 15, 13, 11,  7,  5,  3,  1]))\\n>>> x.sort(stable=True)\\ntorch.return_types.sort(\\n    values=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\\n    indices=tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16,  1,  3,  5,  7,  9, 11, 13, 15, 17]))\\n',\n","   'Id': 612,\n","   'Question': 'How to use torch.sort, give an example?',\n","   'context': ' A namedtuple of (values, indices) is returned, where thevaluesare the\\nsorted values andindicesare the indices of the elements in the originalinputtensor.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.sort.html#torch.sort'},\n","  {'Answer': '>>> a = torch.randint(10, (4,))\\n>>> a\\ntensor([6, 4, 7, 1])\\n>>> torch.float_power(a, 2)\\ntensor([36., 16., 49.,  1.], dtype=torch.float64)\\n\\n>>> a = torch.arange(1, 5)\\n>>> a\\ntensor([ 1,  2,  3,  4])\\n>>> exp = torch.tensor([2, -3, 4, -5])\\n>>> exp\\ntensor([ 2, -3,  4, -5])\\n>>> torch.float_power(a, exp)\\ntensor([1.0000e+00, 1.2500e-01, 8.1000e+01, 9.7656e-04], dtype=torch.float64)\\n',\n","   'Id': 613,\n","   'Question': 'How to use torch.float_power, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.float_power.html#torch.float_power'},\n","  {'Answer': '>>> a = torch.randn(2, 3)\\n>>> torch.diag_embed(a)\\ntensor([[[ 1.5410,  0.0000,  0.0000],\\n         [ 0.0000, -0.2934,  0.0000],\\n         [ 0.0000,  0.0000, -2.1788]],\\n\\n        [[ 0.5684,  0.0000,  0.0000],\\n         [ 0.0000, -1.0845,  0.0000],\\n         [ 0.0000,  0.0000, -1.3986]]])\\n\\n>>> torch.diag_embed(a, offset=1, dim1=0, dim2=2)\\ntensor([[[ 0.0000,  1.5410,  0.0000,  0.0000],\\n         [ 0.0000,  0.5684,  0.0000,  0.0000]],\\n\\n        [[ 0.0000,  0.0000, -0.2934,  0.0000],\\n         [ 0.0000,  0.0000, -1.0845,  0.0000]],\\n\\n        [[ 0.0000,  0.0000,  0.0000, -2.1788],\\n         [ 0.0000,  0.0000,  0.0000, -1.3986]],\\n\\n        [[ 0.0000,  0.0000,  0.0000,  0.0000],\\n         [ 0.0000,  0.0000,  0.0000,  0.0000]]])\\n',\n","   'Id': 614,\n","   'Question': 'How to use torch.diag_embed, give an example?',\n","   'context': ' Applyingtorch.diagonal()to the output of this function with\\nthe same arguments yields a matrix identical to input. However,torch.diagonal()has different default dimensions, so those\\nneed to be explicitly specified.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.diag_embed.html#torch.diag_embed'},\n","  {'Answer': '>>> torch.frac(torch.tensor([1, 2.5, -3.2]))\\ntensor([ 0.0000,  0.5000, -0.2000])\\n',\n","   'Id': 615,\n","   'Question': 'How to use torch.frac, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.frac.html#torch.frac'},\n","  {'Answer': '>>> a = torch.randn(2, 2)\\n>>> h, tau = torch.geqrf(a)\\n>>> q = torch.linalg.householder_product(h, tau)\\n>>> torch.allclose(q, torch.linalg.qr(a)[0])\\nTrue\\n\\n>>> h = torch.randn(3, 2, 2, dtype=torch.complex128)\\n>>> tau = torch.randn(3, 1, dtype=torch.complex128)\\n>>> q = torch.linalg.householder_product(h, tau)\\n>>> q\\ntensor([[[ 1.8034+0.4184j,  0.2588-1.0174j],\\n        [-0.6853+0.7953j,  2.0790+0.5620j]],\\n\\n        [[ 1.4581+1.6989j, -1.5360+0.1193j],\\n        [ 1.3877-0.6691j,  1.3512+1.3024j]],\\n\\n        [[ 1.4766+0.5783j,  0.0361+0.6587j],\\n        [ 0.6396+0.1612j,  1.3693+0.4481j]]], dtype=torch.complex128)\\n',\n","   'Id': 616,\n","   'Question': 'How to use torch.linalg.householder_product, give an example?',\n","   'context': ' Supports inputs of float, double, cfloat and cdouble dtypes.\\nAlso supports batches of matrices, and if the inputs are batches of matrices then\\nthe output has the same batch dimensions.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.linalg.householder_product.html#torch.linalg.householder_product'},\n","  {'Answer': '>>> a = torch.randn(3, 3)\\n>>> a\\ntensor([[-1.0813, -0.8619,  0.7105],\\n        [ 0.0935,  0.1380,  2.2112],\\n        [-0.3409, -0.9828,  0.0289]])\\n>>> torch.tril(a)\\ntensor([[-1.0813,  0.0000,  0.0000],\\n        [ 0.0935,  0.1380,  0.0000],\\n        [-0.3409, -0.9828,  0.0289]])\\n\\n>>> b = torch.randn(4, 6)\\n>>> b\\ntensor([[ 1.2219,  0.5653, -0.2521, -0.2345,  1.2544,  0.3461],\\n        [ 0.4785, -0.4477,  0.6049,  0.6368,  0.8775,  0.7145],\\n        [ 1.1502,  3.2716, -1.1243, -0.5413,  0.3615,  0.6864],\\n        [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0978]])\\n>>> torch.tril(b, diagonal=1)\\ntensor([[ 1.2219,  0.5653,  0.0000,  0.0000,  0.0000,  0.0000],\\n        [ 0.4785, -0.4477,  0.6049,  0.0000,  0.0000,  0.0000],\\n        [ 1.1502,  3.2716, -1.1243, -0.5413,  0.0000,  0.0000],\\n        [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0000]])\\n>>> torch.tril(b, diagonal=-1)\\ntensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\\n        [ 0.4785,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\\n        [ 1.1502,  3.2716,  0.0000,  0.0000,  0.0000,  0.0000],\\n        [-0.0614, -0.7344, -1.3164,  0.0000,  0.0000,  0.0000]])\\n',\n","   'Id': 617,\n","   'Question': 'How to use torch.tril, give an example?',\n","   'context': ' The argumentdiagonalcontrols which diagonal to consider. Ifdiagonal= 0, all elements on and below the main diagonal are\\nretained. A positive value includes just as many diagonals above the main\\ndiagonal, and similarly a negative value excludes just as many diagonals below\\nthe main diagonal. The main diagonal are the set of indices{(i,i)}\\\\lbrace (i, i) \\\\rbrace{(i,i)}fori∈[0,min\\u2061{d1,d2}−1]i \\\\in [0, \\\\min\\\\{d_{1}, d_{2}\\\\} - 1]i∈[0,min{d1\\u200b,d2\\u200b}−1]whered1,d2d_{1}, d_{2}d1\\u200b,d2\\u200bare the dimensions of the matrix.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.tril.html#torch.tril'},\n","  {'Answer': '>>> torch.i0(torch.arange(5, dtype=torch.float32))\\ntensor([ 1.0000,  1.2661,  2.2796,  4.8808, 11.3019])\\n',\n","   'Id': 618,\n","   'Question': 'How to use torch.i0, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.i0.html#torch.i0'},\n","  {'Answer': '>>> a = torch.tensor([1, 2, 3])\\n>>> b = torch.tensor([4, 5, 6])\\n>>> torch.hstack((a,b))\\ntensor([1, 2, 3, 4, 5, 6])\\n>>> a = torch.tensor([[1],[2],[3]])\\n>>> b = torch.tensor([[4],[5],[6]])\\n>>> torch.hstack((a,b))\\ntensor([[1, 4],\\n        [2, 5],\\n        [3, 6]])\\n',\n","   'Id': 619,\n","   'Question': 'How to use torch.hstack, give an example?',\n","   'context': ' This is equivalent to concatenation along the first axis for 1-D tensors, and along the second axis for all other tensors.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.hstack.html#torch.hstack'},\n","  {'Answer': '>>> a = torch.randn(4)\\n>>> a\\ntensor([ 0.4331,  1.2475,  0.6834, -0.2791])\\n>>> torch.pow(a, 2)\\ntensor([ 0.1875,  1.5561,  0.4670,  0.0779])\\n>>> exp = torch.arange(1., 5.)\\n\\n>>> a = torch.arange(1., 5.)\\n>>> a\\ntensor([ 1.,  2.,  3.,  4.])\\n>>> exp\\ntensor([ 1.,  2.,  3.,  4.])\\n>>> torch.pow(a, exp)\\ntensor([   1.,    4.,   27.,  256.])\\n',\n","   'Id': 620,\n","   'Question': 'How to use torch.pow, give an example?',\n","   'context': ' Whenexponentis a tensor, the shapes ofinputandexponentmust bebroadcastable.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.pow.html#torch.pow'},\n","  {'Answer': '>>> exp = torch.arange(1., 5.)\\n>>> base = 2\\n>>> torch.pow(base, exp)\\ntensor([  2.,   4.,   8.,  16.])\\n',\n","   'Id': 621,\n","   'Question': 'How  The operation applied is:, give an example?',\n","   'context': ' The operation applied is:',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.pow.html#torch.pow'},\n","  {'Answer': '>>> a = torch.arange(-0.5, 1, 0.5)\\n>>> a\\ntensor([-0.5000,  0.0000,  0.5000])\\n>>> torch.special.entr(a)\\ntensor([  -inf, 0.0000, 0.3466])\\n',\n","   'Id': 622,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html'},\n","  {'Answer': '>>> torch.special.erf(torch.tensor([0, -1., 10.]))\\ntensor([ 0.0000, -0.8427,  1.0000])\\n',\n","   'Id': 623,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html'},\n","  {'Answer': '>>> torch.special.erfc(torch.tensor([0, -1., 10.]))\\ntensor([ 1.0000, 1.8427,  0.0000])\\n',\n","   'Id': 624,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html'},\n","  {'Answer': '>>> torch.special.erfinv(torch.tensor([0, 0.5, -1.]))\\ntensor([ 0.0000,  0.4769,    -inf])\\n',\n","   'Id': 625,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html'},\n","  {'Answer': '>>> t = torch.randn(4)\\n>>> t\\ntensor([ 0.9213,  1.0887, -0.8858, -1.7683])\\n>>> torch.special.expit(t)\\ntensor([ 0.7153,  0.7481,  0.2920,  0.1458])\\n',\n","   'Id': 626,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html'},\n","  {'Answer': '>>> torch.special.expm1(torch.tensor([0, math.log(2.)]))\\ntensor([ 0.,  1.])\\n',\n","   'Id': 627,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html'},\n","  {'Answer': '>>> torch.special.exp2(torch.tensor([0, math.log2(2.), 3, 4]))\\ntensor([ 1.,  2.,  8., 16.])\\n',\n","   'Id': 628,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html'},\n","  {'Answer': '>>> a = torch.arange(0.5, 2, 0.5)\\n>>> torch.special.gammaln(a)\\ntensor([ 0.5724,  0.0000, -0.1208])\\n',\n","   'Id': 629,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html'},\n","  {'Answer': '>>> torch.special.i0e(torch.arange(5, dtype=torch.float32))\\ntensor([1.0000, 0.4658, 0.3085, 0.2430, 0.2070])\\n',\n","   'Id': 630,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html'},\n","  {'Answer': '>>> a = torch.rand(5)\\n>>> a\\ntensor([0.2796, 0.9331, 0.6486, 0.1523, 0.6516])\\n>>> torch.special.logit(a, eps=1e-6)\\ntensor([-0.9466,  2.6352,  0.6131, -1.7169,  0.6261])\\n',\n","   'Id': 631,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html'},\n","  {'Answer': \">>> x = torch.zeros(5,)\\n>>> y = torch.tensor([-1, 0, 1, float('inf'), float('nan')])\\n>>> torch.special.xlog1py(x, y)\\ntensor([0., 0., 0., 0., nan])\\n>>> x = torch.tensor([1, 2, 3])\\n>>> y = torch.tensor([3, 2, 1])\\n>>> torch.special.xlog1py(x, y)\\ntensor([1.3863, 2.1972, 2.0794])\\n>>> torch.special.xlog1py(x, 4)\\ntensor([1.6094, 3.2189, 4.8283])\\n>>> torch.special.xlog1py(2, y)\\ntensor([2.7726, 2.1972, 1.3863])\\n\",\n","   'Id': 632,\n","   'Question': 'How  Similar to SciPy’sscipy.special.xlog1py., give an example?',\n","   'context': ' Similar to SciPy’sscipy.special.xlog1py.',\n","   'source': 'https://pytorch.org/docs/stable/special.html'},\n","  {'Answer': '>>> a = torch.randn(4)\\n>>> a\\ntensor([-0.8166,  1.5308, -0.2530, -0.2091])\\n>>> torch.floor(a)\\ntensor([-1.,  1., -1., -1.])\\n',\n","   'Id': 633,\n","   'Question': 'How to use torch.floor, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.floor.html#torch.floor'},\n","  {'Answer': '>>> import torch\\n>>> x = torch.ones(1, 2, 3, requires_grad=True)\\n>>> with torch.inference_mode():\\n...   y = x * x\\n>>> y.requires_grad\\nFalse\\n>>> y._version\\nTraceback (most recent call last):\\nFile \"<stdin>\", line 1, in <module>\\nRuntimeError: Inference tensors do not track version counter.\\n>>> @torch.inference_mode()\\n... def func(x):\\n...   return x * x\\n>>> out = func(x)\\n>>> out.requires_grad\\nFalse\\n',\n","   'Id': 634,\n","   'Question': 'How to use inference mode, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode'},\n","  {'Answer': \">>> a = numpy.array([1, 2, 3])\\n>>> t = torch.as_tensor(a)\\n>>> t\\ntensor([ 1,  2,  3])\\n>>> t[0] = -1\\n>>> a\\narray([-1,  2,  3])\\n\\n>>> a = numpy.array([1, 2, 3])\\n>>> t = torch.as_tensor(a, device=torch.device('cuda'))\\n>>> t\\ntensor([ 1,  2,  3])\\n>>> t[0] = -1\\n>>> a\\narray([1,  2,  3])\\n\",\n","   'Id': 635,\n","   'Question': 'How to use torch.as_tensor, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.as_tensor.html#torch.as_tensor'},\n","  {'Answer': '>>> y = torch.randn((2, 3))\\n>>> y\\ntensor([[-2.1156,  0.6857, -0.2700],\\n        [-1.2145,  0.5540,  2.0431]])\\n>>> x = torch.tensor([[1, 3, 4], [1, 2, 3]])\\n>>> torch.trapz(y, x)\\ntensor([-1.2220,  0.9683])\\n',\n","   'Id': 636,\n","   'Question': 'How to use torch.trapz, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.trapz.html#torch.trapz'},\n","  {'Answer': '>>> a = torch.randn(3)\\n>>> a\\ntensor([ 0.2015, -0.4255,  2.6087])\\n>>> torch.mul(a, 100)\\ntensor([  20.1494,  -42.5491,  260.8663])\\n',\n","   'Id': 637,\n","   'Question': 'How to use torch.mul, give an example?',\n","   'context': ' Ifinputis of typeFloatTensororDoubleTensor,othershould be a real number, otherwise it should be an integer',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul'},\n","  {'Answer': '>>> a = torch.randn(4, 1)\\n>>> a\\ntensor([[ 1.1207],\\n        [-0.3137],\\n        [ 0.0700],\\n        [ 0.8378]])\\n>>> b = torch.randn(1, 4)\\n>>> b\\ntensor([[ 0.5146,  0.1216, -0.5244,  2.2382]])\\n>>> torch.mul(a, b)\\ntensor([[ 0.5767,  0.1363, -0.5877,  2.5083],\\n        [-0.1614, -0.0382,  0.1645, -0.7021],\\n        [ 0.0360,  0.0085, -0.0367,  0.1567],\\n        [ 0.4312,  0.1019, -0.4394,  1.8753]])\\n',\n","   'Id': 638,\n","   'Question': 'How  The shapes ofinputandothermust bebroadcastable., give an example?',\n","   'context': ' The shapes ofinputandothermust bebroadcastable.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul'},\n","  {'Answer': '>>> a = torch.randn(3, 3)\\n>>> torch.logsumexp(a, 1)\\ntensor([ 0.8442,  1.4322,  0.8711])\\n',\n","   'Id': 639,\n","   'Question': 'How to use torch.logsumexp, give an example?',\n","   'context': ' IfkeepdimisTrue, the output tensor is of the same size\\nasinputexcept in the dimension(s)dimwhere it is of size 1.\\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\\noutput tensor having 1 (orlen(dim)) fewer dimension(s).',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.logsumexp.html#torch.logsumexp'},\n","  {'Answer': '>>> a = torch.randn(1, 3)\\n>>> a\\ntensor([[ 0.6763,  0.7445, -2.2369]])\\n>>> torch.max(a)\\ntensor(0.7445)\\n',\n","   'Id': 640,\n","   'Question': 'How to use torch.max, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.max.html#torch.max'},\n","  {'Answer': '>>> a = torch.randn(4, 4)\\n>>> a\\ntensor([[-1.2360, -0.2942, -0.1222,  0.8475],\\n        [ 1.1949, -1.1127, -2.2379, -0.6702],\\n        [ 1.5717, -0.9207,  0.1297, -1.8768],\\n        [-0.6172,  1.0036, -0.6060, -0.2432]])\\n>>> torch.max(a, 1)\\ntorch.return_types.max(values=tensor([0.8475, 1.1949, 1.5717, 1.0036]), indices=tensor([3, 0, 0, 1]))\\n',\n","   'Id': 641,\n","   'Question': 'How  IfkeepdimisTrue, the output tensors are of the same size\\nasinputexcept in the dimensiondimwhere they are of size 1.\\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\\nin the output tensors having 1 fewer dimension thaninput., give an example?',\n","   'context': ' IfkeepdimisTrue, the output tensors are of the same size\\nasinputexcept in the dimensiondimwhere they are of size 1.\\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\\nin the output tensors having 1 fewer dimension thaninput.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.max.html#torch.max'},\n","  {'Answer': \">>> state_dict = torch.hub.load_state_dict_from_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')\\n\",\n","   'Id': 642,\n","   'Question': 'How to use torch.utils.model_zoo.load_url, give an example?',\n","   'context': ' If the object is already present inmodel_dir, it’s deserialized and\\nreturned.\\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir().',\n","   'source': 'https://pytorch.org/docs/stable/model_zoo.html'},\n","  {'Answer': '>>> a = torch.randn(1, 3)\\n>>> a\\ntensor([[ 0.2294, -0.5481,  1.3288]])\\n>>> torch.mean(a)\\ntensor(0.3367)\\n',\n","   'Id': 643,\n","   'Question': 'How to use torch.mean, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean'},\n","  {'Answer': '>>> a = torch.randn(4, 4)\\n>>> a\\ntensor([[-0.3841,  0.6320,  0.4254, -0.7384],\\n        [-0.9644,  1.0131, -0.6549, -1.4279],\\n        [-0.2951, -1.3350, -0.7694,  0.5600],\\n        [ 1.0842, -0.9580,  0.3623,  0.2343]])\\n>>> torch.mean(a, 1)\\ntensor([-0.0163, -0.5085, -0.4599,  0.1807])\\n>>> torch.mean(a, 1, True)\\ntensor([[-0.0163],\\n        [-0.5085],\\n        [-0.4599],\\n        [ 0.1807]])\\n',\n","   'Id': 644,\n","   'Question': 'How  IfkeepdimisTrue, the output tensor is of the same size\\nasinputexcept in the dimension(s)dimwhere it is of size 1.\\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\\noutput tensor having 1 (orlen(dim)) fewer dimension(s)., give an example?',\n","   'context': ' IfkeepdimisTrue, the output tensor is of the same size\\nasinputexcept in the dimension(s)dimwhere it is of size 1.\\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\\noutput tensor having 1 (orlen(dim)) fewer dimension(s).',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean'},\n","  {'Answer': '>>> a = torch.randn(2, 3)\\n>>> a\\ntensor([[ 0.0795, -1.2117,  0.9765],\\n        [ 1.1707,  0.6706,  0.4884]])\\n>>> q = torch.tensor([0.25, 0.5, 0.75])\\n>>> torch.quantile(a, q, dim=1, keepdim=True)\\ntensor([[[-0.5661],\\n        [ 0.5795]],\\n\\n        [[ 0.0795],\\n        [ 0.6706]],\\n\\n        [[ 0.5280],\\n        [ 0.9206]]])\\n>>> torch.quantile(a, q, dim=1, keepdim=True).shape\\ntorch.Size([3, 2, 1])\\n>>> a = torch.arange(4.)\\n>>> a\\ntensor([0., 1., 2., 3.])\\n',\n","   'Id': 645,\n","   'Question': 'How to use torch.quantile, give an example?',\n","   'context': ' Ifqis a 1D tensor, the first dimension of the output represents the quantiles and has size\\nequal to the size ofq, the remaining dimensions are what remains from the reduction.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile'},\n","  {'Answer': '>>> torch.vdot(torch.tensor([2, 3]), torch.tensor([2, 1]))\\ntensor(7)\\n>>> a = torch.tensor((1 +2j, 3 - 1j))\\n>>> b = torch.tensor((2 +1j, 4 - 0j))\\n>>> torch.vdot(a, b)\\ntensor([16.+1.j])\\n>>> torch.vdot(b, a)\\ntensor([16.-1.j])\\n',\n","   'Id': 646,\n","   'Question': 'How to use torch.vdot, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.vdot.html#torch.vdot'},\n","  {'Answer': '>>> a = torch.randn(4)\\n>>> a\\ntensor([ 0.1606, -1.4267, -1.0899, -1.0250 ])\\n>>> torch.asinh(a)\\ntensor([ 0.1599, -1.1534, -0.9435, -0.8990 ])\\n',\n","   'Id': 647,\n","   'Question': 'How to use torch.asinh, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.asinh.html#torch.asinh'},\n","  {'Answer': 'X = torch.linalg.lstsq(A, B).solution\\n',\n","   'Id': 648,\n","   'Question': 'How to use torch.lstsq, give an example?',\n","   'context': ' torch.lstsq()is deprecated in favor oftorch.linalg.lstsq()and will be removed in a future PyTorch release.torch.linalg.lstsq()has reversed arguments and does not return the QR decomposition in the returned tuple,\\n(it returns other information about the problem).\\nThe returnedsolutionintorch.lstsq()stores the residuals of the solution in the\\nlastm - ncolumns in the casem > n. Intorch.linalg.lstsq(), the residuals\\nare in the field ‘residuals’ of the returned named tuple.Unpacking the solution as``X = torch.lstsq(B, A).solution[:A.size(1)]`` should be replaced with',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.lstsq.html#torch.lstsq'},\n","  {'Answer': '>>> A = torch.tensor([[1., 1, 1],\\n...                   [2, 3, 4],\\n...                   [3, 5, 2],\\n...                   [4, 2, 5],\\n...                   [5, 4, 3]])\\n>>> B = torch.tensor([[-10., -3],\\n...                   [ 12, 14],\\n...                   [ 14, 12],\\n...                   [ 16, 16],\\n...                   [ 18, 16]])\\n>>> X, _ = torch.lstsq(B, A)\\n>>> X\\ntensor([[  2.0000,   1.0000],\\n        [  1.0000,   1.0000],\\n        [  1.0000,   2.0000],\\n        [ 10.9635,   4.8501],\\n        [  8.9332,   5.2418]])\\n',\n","   'Id': 649,\n","   'Question': 'How  Returned tensorXXXhas shape(max\\u2061(m,n)×k)(\\\\max(m, n) \\\\times k)(max(m,n)×k). The firstnnnrows ofXXXcontains the solution. Ifm≥nm \\\\geq nm≥n, the residual sum of squares\\nfor the solution in each column is given by the sum of squares of elements in the\\nremainingm−nm - nm−nrows of that column., give an example?',\n","   'context': ' Returned tensorXXXhas shape(max\\u2061(m,n)×k)(\\\\max(m, n) \\\\times k)(max(m,n)×k). The firstnnnrows ofXXXcontains the solution. Ifm≥nm \\\\geq nm≥n, the residual sum of squares\\nfor the solution in each column is given by the sum of squares of elements in the\\nremainingm−nm - nm−nrows of that column.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.lstsq.html#torch.lstsq'},\n","  {'Answer': '>>> x=torch.randn(4, dtype=torch.cfloat)\\n>>> x\\ntensor([(0.4737-0.3839j), (-0.2098-0.6699j), (0.3470-0.9451j), (-0.5174-1.3136j)])\\n>>> torch.view_as_real(x)\\ntensor([[ 0.4737, -0.3839],\\n        [-0.2098, -0.6699],\\n        [ 0.3470, -0.9451],\\n        [-0.5174, -1.3136]])\\n',\n","   'Id': 650,\n","   'Question': 'How to use torch.view_as_real, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.view_as_real.html#torch.view_as_real'},\n","  {'Answer': \">>> torch.isfinite(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))\\ntensor([True,  False,  True,  False,  False])\\n\",\n","   'Id': 651,\n","   'Question': 'How to use torch.isfinite, give an example?',\n","   'context': ' Real values are finite when they are not NaN, negative infinity, or infinity.\\nComplex values are finite when both their real and imaginary parts are finite.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.isfinite.html#torch.isfinite'},\n","  {'Answer': '>>> x = torch.arange(4).view(2, 2)\\n>>> x\\ntensor([[0, 1],\\n        [2, 3]])\\n>>> torch.flipud(x)\\ntensor([[2, 3],\\n        [0, 1]])\\n',\n","   'Id': 652,\n","   'Question': 'How to use torch.flipud, give an example?',\n","   'context': ' Flip the entries in each column in the up/down direction.\\nRows are preserved, but appear in a different order than before.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.flipud.html#torch.flipud'},\n","  {'Answer': '>>> torch.full((2, 3), 3.141592)\\ntensor([[ 3.1416,  3.1416,  3.1416],\\n        [ 3.1416,  3.1416,  3.1416]])\\n',\n","   'Id': 653,\n","   'Question': 'How to use torch.full, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.full.html#torch.full'},\n","  {'Answer': \">>> g_cpu = torch.Generator()\\n>>> g_cuda = torch.Generator(device='cuda')\\n\",\n","   'Id': 654,\n","   'Question': 'How to use torch.Generator, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator'},\n","  {'Answer': \">>> g_cpu = torch.Generator()\\n>>> g_cpu.device\\ndevice(type='cpu')\\n\",\n","   'Id': 655,\n","   'Question': 'How  Gets the current device of the generator., give an example?',\n","   'context': ' Gets the current device of the generator.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator'},\n","  {'Answer': '>>> g_cpu = torch.Generator()\\n>>> g_cpu.get_state()\\n',\n","   'Id': 656,\n","   'Question': 'How to use torch.Generator.get_state, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator'},\n","  {'Answer': '>>> g_cpu = torch.Generator()\\n>>> g_cpu.initial_seed()\\n2147483647\\n',\n","   'Id': 657,\n","   'Question': 'How to use torch.Generator.initial_seed, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator'},\n","  {'Answer': '>>> g_cpu = torch.Generator()\\n>>> g_cpu.manual_seed(2147483647)\\n',\n","   'Id': 658,\n","   'Question': 'How to use torch.Generator.manual_seed, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator'},\n","  {'Answer': '>>> g_cpu = torch.Generator()\\n>>> g_cpu.seed()\\n1516516984916\\n',\n","   'Id': 659,\n","   'Question': 'How to use torch.Generator.seed, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator'},\n","  {'Answer': '>>> g_cpu = torch.Generator()\\n>>> g_cpu_other = torch.Generator()\\n>>> g_cpu.set_state(g_cpu_other.get_state())\\n',\n","   'Id': 660,\n","   'Question': 'How to use torch.Generator.set_state, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator'},\n","  {'Answer': '>>> a = torch.tensor([[180.0, -180.0], [360.0, -360.0], [90.0, -90.0]])\\n>>> torch.deg2rad(a)\\ntensor([[ 3.1416, -3.1416],\\n        [ 6.2832, -6.2832],\\n        [ 1.5708, -1.5708]])\\n',\n","   'Id': 661,\n","   'Question': 'How to use torch.deg2rad, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.deg2rad.html#torch.deg2rad'},\n","  {'Answer': '>>> a = torch.tensor([[-0.8166, -1.3802, -0.3560]])\\n>>> torch.var_mean(a, unbiased=False)\\n(tensor(0.1754), tensor(-0.8509))\\n',\n","   'Id': 662,\n","   'Question': 'How to use torch.var_mean, give an example?',\n","   'context': ' IfunbiasedisTrue, Bessel’s correction will be used.\\nOtherwise, the sample deviation is calculated, without any correction.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean'},\n","  {'Answer': '>>> real = torch.tensor([1, 2], dtype=torch.float32)\\n>>> imag = torch.tensor([3, 4], dtype=torch.float32)\\n>>> z = torch.complex(real, imag)\\n>>> z\\ntensor([(1.+3.j), (2.+4.j)])\\n>>> z.dtype\\ntorch.complex64\\n',\n","   'Id': 663,\n","   'Question': 'How to use torch.complex, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.complex.html#torch.complex'},\n","  {'Answer': '>>> A = torch.randn(2, 2).triu()\\n>>> A\\ntensor([[ 1.1527, -1.0753],\\n        [ 0.0000,  0.7986]])\\n>>> b = torch.randn(2, 3)\\n>>> b\\ntensor([[-0.0210,  2.3513, -1.5492],\\n        [ 1.5429,  0.7403, -1.0243]])\\n>>> torch.triangular_solve(b, A)\\ntorch.return_types.triangular_solve(\\nsolution=tensor([[ 1.7841,  2.9046, -2.5405],\\n        [ 1.9320,  0.9270, -1.2826]]),\\ncloned_coefficient=tensor([[ 1.1527, -1.0753],\\n        [ 0.0000,  0.7986]]))\\n',\n","   'Id': 664,\n","   'Question': 'How to use torch.triangular_solve, give an example?',\n","   'context': ' Supports input of float, double, cfloat and cdouble data types.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve'},\n","  {'Answer': '>>> torch.ones(2, 3)\\ntensor([[ 1.,  1.,  1.],\\n        [ 1.,  1.,  1.]])\\n\\n>>> torch.ones(5)\\ntensor([ 1.,  1.,  1.,  1.,  1.])\\n',\n","   'Id': 665,\n","   'Question': 'How to use torch.ones, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones'},\n","  {'Answer': '>>> x = torch.randn(())\\n>>> x\\ntensor(0.1995)\\n>>> torch.t(x)\\ntensor(0.1995)\\n>>> x = torch.randn(3)\\n>>> x\\ntensor([ 2.4320, -0.4608,  0.7702])\\n>>> torch.t(x)\\ntensor([ 2.4320, -0.4608,  0.7702])\\n>>> x = torch.randn(2, 3)\\n>>> x\\ntensor([[ 0.4875,  0.9158, -0.5872],\\n        [ 0.3938, -0.6929,  0.6932]])\\n>>> torch.t(x)\\ntensor([[ 0.4875,  0.3938],\\n        [ 0.9158, -0.6929],\\n        [-0.5872,  0.6932]])\\n',\n","   'Id': 666,\n","   'Question': 'How to use torch.t, give an example?',\n","   'context': ' 0-D and 1-D tensors are returned as is. When input is a 2-D tensor this\\nis equivalent totranspose(input,0,1).',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.t.html#torch.t'},\n","  {'Answer': '>>> a = torch.arange(-0.5, 1, 0.5)\\n>>> a\\ntensor([-0.5000,  0.0000,  0.5000])\\n>>> torch.special.entr(a)\\ntensor([  -inf, 0.0000, 0.3466])\\n',\n","   'Id': 667,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.expm1'},\n","  {'Answer': '>>> torch.special.erf(torch.tensor([0, -1., 10.]))\\ntensor([ 0.0000, -0.8427,  1.0000])\\n',\n","   'Id': 668,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.expm1'},\n","  {'Answer': '>>> torch.special.erfc(torch.tensor([0, -1., 10.]))\\ntensor([ 1.0000, 1.8427,  0.0000])\\n',\n","   'Id': 669,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.expm1'},\n","  {'Answer': '>>> torch.special.erfinv(torch.tensor([0, 0.5, -1.]))\\ntensor([ 0.0000,  0.4769,    -inf])\\n',\n","   'Id': 670,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.expm1'},\n","  {'Answer': '>>> t = torch.randn(4)\\n>>> t\\ntensor([ 0.9213,  1.0887, -0.8858, -1.7683])\\n>>> torch.special.expit(t)\\ntensor([ 0.7153,  0.7481,  0.2920,  0.1458])\\n',\n","   'Id': 671,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.expm1'},\n","  {'Answer': '>>> torch.special.expm1(torch.tensor([0, math.log(2.)]))\\ntensor([ 0.,  1.])\\n',\n","   'Id': 672,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.expm1'},\n","  {'Answer': '>>> torch.special.exp2(torch.tensor([0, math.log2(2.), 3, 4]))\\ntensor([ 1.,  2.,  8., 16.])\\n',\n","   'Id': 673,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.expm1'},\n","  {'Answer': '>>> a = torch.arange(0.5, 2, 0.5)\\n>>> torch.special.gammaln(a)\\ntensor([ 0.5724,  0.0000, -0.1208])\\n',\n","   'Id': 674,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.expm1'},\n","  {'Answer': '>>> torch.special.i0e(torch.arange(5, dtype=torch.float32))\\ntensor([1.0000, 0.4658, 0.3085, 0.2430, 0.2070])\\n',\n","   'Id': 675,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.expm1'},\n","  {'Answer': '>>> a = torch.rand(5)\\n>>> a\\ntensor([0.2796, 0.9331, 0.6486, 0.1523, 0.6516])\\n>>> torch.special.logit(a, eps=1e-6)\\ntensor([-0.9466,  2.6352,  0.6131, -1.7169,  0.6261])\\n',\n","   'Id': 676,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.expm1'},\n","  {'Answer': \">>> x = torch.zeros(5,)\\n>>> y = torch.tensor([-1, 0, 1, float('inf'), float('nan')])\\n>>> torch.special.xlog1py(x, y)\\ntensor([0., 0., 0., 0., nan])\\n>>> x = torch.tensor([1, 2, 3])\\n>>> y = torch.tensor([3, 2, 1])\\n>>> torch.special.xlog1py(x, y)\\ntensor([1.3863, 2.1972, 2.0794])\\n>>> torch.special.xlog1py(x, 4)\\ntensor([1.6094, 3.2189, 4.8283])\\n>>> torch.special.xlog1py(2, y)\\ntensor([2.7726, 2.1972, 1.3863])\\n\",\n","   'Id': 677,\n","   'Question': 'How  Similar to SciPy’sscipy.special.xlog1py., give an example?',\n","   'context': ' Similar to SciPy’sscipy.special.xlog1py.',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.expm1'},\n","  {'Answer': '>>> torch.gt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\\ntensor([[False, True], [False, False]])\\n',\n","   'Id': 678,\n","   'Question': 'How to use torch.gt, give an example?',\n","   'context': ' The second argument can be a number or a tensor whose shape isbroadcastablewith the first argument.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.gt.html#torch.gt'},\n","  {'Answer': '>>> a = torch.randn(4)\\n>>> a\\ntensor([ 0.9920,  0.6077,  0.9734, -1.0362])\\n>>> torch.round(a)\\ntensor([ 1.,  1.,  1., -1.])\\n',\n","   'Id': 679,\n","   'Question': 'How to use torch.round, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.round.html#torch.round'},\n","  {'Answer': '- Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\\n  structure with randomness: probabilistic algorithms for\\n  constructing approximate matrix decompositions,\\n  arXiv:0909.4061 [math.NA; math.PR], 2009 (available at\\n  `arXiv <http://arxiv.org/abs/0909.4061>`_).\\n',\n","   'Id': 680,\n","   'Question': 'How to use torch.pca_lowrank, give an example?',\n","   'context': ' This function returns a namedtuple(U,S,V)which is the\\nnearly optimal approximation of a singular value decomposition of\\na centered matrixAAAsuch thatA=Udiag(S)VTA = U diag(S) V^TA=Udiag(S)VT.References:',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank'},\n","  {'Answer': '>>> x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\\n>>> torch.narrow(x, 0, 0, 2)\\ntensor([[ 1,  2,  3],\\n        [ 4,  5,  6]])\\n>>> torch.narrow(x, 1, 1, 2)\\ntensor([[ 2,  3],\\n        [ 5,  6],\\n        [ 8,  9]])\\n',\n","   'Id': 681,\n","   'Question': 'How to use torch.narrow, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.narrow.html#torch.narrow'},\n","  {'Answer': '>>> a = torch.randn(1, 3)\\n>>> a\\ntensor([[ 0.6750,  1.0857,  1.7197]])\\n>>> torch.min(a)\\ntensor(0.6750)\\n',\n","   'Id': 682,\n","   'Question': 'How to use torch.min, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.min.html#torch.min'},\n","  {'Answer': '>>> a = torch.randn(4, 4)\\n>>> a\\ntensor([[-0.6248,  1.1334, -1.1899, -0.2803],\\n        [-1.4644, -0.2635, -0.3651,  0.6134],\\n        [ 0.2457,  0.0384,  1.0128,  0.7015],\\n        [-0.1153,  2.9849,  2.1458,  0.5788]])\\n>>> torch.min(a, 1)\\ntorch.return_types.min(values=tensor([-1.1899, -1.4644,  0.0384, -0.1153]), indices=tensor([2, 0, 1, 0]))\\n',\n","   'Id': 683,\n","   'Question': 'How  IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\\nthe output tensors having 1 fewer dimension thaninput., give an example?',\n","   'context': ' IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\\nthe output tensors having 1 fewer dimension thaninput.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.min.html#torch.min'},\n","  {'Answer': '>>> weights = torch.tensor([0, 10, 3, 0], dtype=torch.float) # create a tensor of weights\\n>>> torch.multinomial(weights, 2)\\ntensor([1, 2])\\n>>> torch.multinomial(weights, 4) # ERROR!\\nRuntimeError: invalid argument 2: invalid multinomial distribution (with replacement=False,\\nnot enough non-negative category to sample) at ../aten/src/TH/generic/THTensorRandom.cpp:320\\n>>> torch.multinomial(weights, 4, replacement=True)\\ntensor([ 2,  1,  1,  1])\\n',\n","   'Id': 684,\n","   'Question': 'How to use torch.multinomial, give an example?',\n","   'context': ' If not, they are drawn without replacement, which means that when a\\nsample index is drawn for a row, it cannot be drawn again for that row.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial'},\n","  {'Answer': 'import torch\\nfrom torch import nn\\n\\nclass MyLinear(nn.Module):\\n  def __init__(self, in_features, out_features):\\n    super().__init__()\\n    self.weight = nn.Parameter(torch.randn(in_features, out_features))\\n    self.bias = nn.Parameter(torch.randn(out_features))\\n\\n  def forward(self, input):\\n    return (input @ self.weight) + self.bias\\n',\n","   'Id': 685,\n","   'Question': 'How to use To get started, let’s look at a simpler, custom version of PyTorch’sLinearmodule.\\nThis module applies an affine transformation to its input., give an example?',\n","   'context': ' To get started, let’s look at a simpler, custom version of PyTorch’sLinearmodule.\\nThis module applies an affine transformation to its input.',\n","   'source': 'https://pytorch.org/docs/stable/notes/modules.html'},\n","  {'Answer': 'm = MyLinear(4, 3)\\nsample_input = torch.randn(4)\\nm(sample_input)\\n: tensor([-0.3037, -1.0413, -4.2057], grad_fn=<AddBackward0>)\\n',\n","   'Id': 686,\n","   'Question': 'How to use This simple module has the following fundamental characteristics of modules:This simple module demonstrates how modules package state and computation together. Instances of this module can be\\nconstructed and called:, give an example?',\n","   'context': ' This simple module has the following fundamental characteristics of modules:This simple module demonstrates how modules package state and computation together. Instances of this module can be\\nconstructed and called:',\n","   'source': 'https://pytorch.org/docs/stable/notes/modules.html'},\n","  {'Answer': \"for parameter in m.named_parameters():\\n  print(parameter)\\n: ('weight', Parameter containing:\\ntensor([[ 1.0597,  1.1796,  0.8247],\\n        [-0.5080, -1.2635, -1.1045],\\n        [ 0.0593,  0.2469, -1.4299],\\n        [-0.4926, -0.5457,  0.4793]], requires_grad=True))\\n('bias', Parameter containing:\\ntensor([ 0.3634,  0.2015, -0.8525], requires_grad=True))\\n\",\n","   'Id': 687,\n","   'Question': 'How to use Note that the module itself is callable, and that calling it invokes itsforward()function.\\nThis name is in reference to the concepts of “forward pass” and “backward pass”, which apply to each module.\\nThe “forward pass” is responsible for applying the computation represented by the module\\nto the given input(s) (as shown in the above snippet). The “backward pass” computes gradients of\\nmodule outputs with respect to its inputs, which can be used for “training” parameters through gradient\\ndescent methods. PyTorch’s autograd system automatically takes care of this backward pass computation, so it\\nis not required to manually implement abackward()function for each module. The process of training\\nmodule parameters through successive forward / backward passes is covered in detail inNeural Network Training with Modules.The full set of parameters registered by the module can be iterated through via a call toparameters()ornamed_parameters(),\\nwhere the latter includes each parameter’s name:, give an example?',\n","   'context': ' Note that the module itself is callable, and that calling it invokes itsforward()function.\\nThis name is in reference to the concepts of “forward pass” and “backward pass”, which apply to each module.\\nThe “forward pass” is responsible for applying the computation represented by the module\\nto the given input(s) (as shown in the above snippet). The “backward pass” computes gradients of\\nmodule outputs with respect to its inputs, which can be used for “training” parameters through gradient\\ndescent methods. PyTorch’s autograd system automatically takes care of this backward pass computation, so it\\nis not required to manually implement abackward()function for each module. The process of training\\nmodule parameters through successive forward / backward passes is covered in detail inNeural Network Training with Modules.The full set of parameters registered by the module can be iterated through via a call toparameters()ornamed_parameters(),\\nwhere the latter includes each parameter’s name:',\n","   'source': 'https://pytorch.org/docs/stable/notes/modules.html'},\n","  {'Answer': 'net = nn.Sequential(\\n  MyLinear(4, 3),\\n  nn.ReLU(),\\n  MyLinear(3, 1)\\n)\\n\\nsample_input = torch.randn(4)\\nnet(sample_input)\\n: tensor([-0.6749], grad_fn=<AddBackward0>)\\n',\n","   'Id': 688,\n","   'Question': 'How to use Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\\nmultiple modules:, give an example?',\n","   'context': ' Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\\nmultiple modules:',\n","   'source': 'https://pytorch.org/docs/stable/notes/modules.html'},\n","  {'Answer': 'import torch.nn.functional as F\\n\\nclass Net(nn.Module):\\n  def __init__(self):\\n    super().__init__()\\n    self.l0 = MyLinear(4, 3)\\n    self.l1 = MyLinear(3, 1)\\n  def forward(self, x):\\n    x = self.l0(x)\\n    x = F.relu(x)\\n    x = self.l1(x)\\n    return x\\n',\n","   'Id': 689,\n","   'Question': 'How to use In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\\nfull flexibility on how submodules are used for a module’s computation.For example, here’s a simple neural network implemented as a custom module:, give an example?',\n","   'context': ' In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\\nfull flexibility on how submodules are used for a module’s computation.For example, here’s a simple neural network implemented as a custom module:',\n","   'source': 'https://pytorch.org/docs/stable/notes/modules.html'},\n","  {'Answer': \"net = Net()\\nfor child in net.named_children():\\n  print(child)\\n: ('l0', MyLinear())\\n('l1', MyLinear())\\n\",\n","   'Id': 690,\n","   'Question': 'How to use For example, here’s a simple neural network implemented as a custom module:This module is composed of two “children” or “submodules” (l0andl1) that define the layers of\\nthe neural network and are utilized for computation within the module’sforward()method. Immediate\\nchildren of a module can be iterated through via a call tochildren()ornamed_children():, give an example?',\n","   'context': ' For example, here’s a simple neural network implemented as a custom module:This module is composed of two “children” or “submodules” (l0andl1) that define the layers of\\nthe neural network and are utilized for computation within the module’sforward()method. Immediate\\nchildren of a module can be iterated through via a call tochildren()ornamed_children():',\n","   'source': 'https://pytorch.org/docs/stable/notes/modules.html'},\n","  {'Answer': \"class BigNet(nn.Module):\\n  def __init__(self):\\n    super().__init__()\\n    self.l1 = MyLinear(5, 4)\\n    self.net = Net()\\n  def forward(self, x):\\n    return self.net(self.l1(x))\\n\\nbig_net = BigNet()\\nfor module in big_net.named_modules():\\n  print(module)\\n: ('', BigNet(\\n  (l1): MyLinear()\\n  (net): Net(\\n    (l0): MyLinear()\\n    (l1): MyLinear()\\n  )\\n))\\n('l1', MyLinear())\\n('net', Net(\\n  (l0): MyLinear()\\n  (l1): MyLinear()\\n))\\n('net.l0', MyLinear())\\n('net.l1', MyLinear())\\n\",\n","   'Id': 691,\n","   'Question': 'How to use This module is composed of two “children” or “submodules” (l0andl1) that define the layers of\\nthe neural network and are utilized for computation within the module’sforward()method. Immediate\\nchildren of a module can be iterated through via a call tochildren()ornamed_children():To go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules:, give an example?',\n","   'context': ' This module is composed of two “children” or “submodules” (l0andl1) that define the layers of\\nthe neural network and are utilized for computation within the module’sforward()method. Immediate\\nchildren of a module can be iterated through via a call tochildren()ornamed_children():To go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules:',\n","   'source': 'https://pytorch.org/docs/stable/notes/modules.html'},\n","  {'Answer': \"class DynamicNet(nn.Module):\\n  def __init__(self, num_layers):\\n    super().__init__()\\n    self.linears = nn.ModuleList(\\n      [MyLinear(4, 4) for _ in range(num_layers)])\\n    self.activations = nn.ModuleDict({\\n      'relu': nn.ReLU(),\\n      'lrelu': nn.LeakyReLU()\\n    })\\n    self.final = MyLinear(4, 1)\\n  def forward(self, x, act):\\n    for linear in self.linears:\\n      x = linear(x)\\n    x = self.activations[act](x)\\n    x = self.final(x)\\n    return x\\n\\ndynamic_net = DynamicNet(3)\\nsample_input = torch.randn(4)\\noutput = dynamic_net(sample_input, 'relu')\\n\",\n","   'Id': 692,\n","   'Question': 'How to use To go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules:Sometimes, it’s necessary for a module to dynamically define submodules.\\nTheModuleListandModuleDictmodules are useful here; they\\nregister submodules from a list or dict:, give an example?',\n","   'context': ' To go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules:Sometimes, it’s necessary for a module to dynamically define submodules.\\nTheModuleListandModuleDictmodules are useful here; they\\nregister submodules from a list or dict:',\n","   'source': 'https://pytorch.org/docs/stable/notes/modules.html'},\n","  {'Answer': \"for parameter in dynamic_net.named_parameters():\\n  print(parameter)\\n: ('linears.0.weight', Parameter containing:\\ntensor([[-1.2051,  0.7601,  1.1065,  0.1963],\\n        [ 3.0592,  0.4354,  1.6598,  0.9828],\\n        [-0.4446,  0.4628,  0.8774,  1.6848],\\n        [-0.1222,  1.5458,  1.1729,  1.4647]], requires_grad=True))\\n('linears.0.bias', Parameter containing:\\ntensor([ 1.5310,  1.0609, -2.0940,  1.1266], requires_grad=True))\\n('linears.1.weight', Parameter containing:\\ntensor([[ 2.1113, -0.0623, -1.0806,  0.3508],\\n        [-0.0550,  1.5317,  1.1064, -0.5562],\\n        [-0.4028, -0.6942,  1.5793, -1.0140],\\n        [-0.0329,  0.1160, -1.7183, -1.0434]], requires_grad=True))\\n('linears.1.bias', Parameter containing:\\ntensor([ 0.0361, -0.9768, -0.3889,  1.1613], requires_grad=True))\\n('linears.2.weight', Parameter containing:\\ntensor([[-2.6340, -0.3887, -0.9979,  0.0767],\\n        [-0.3526,  0.8756, -1.5847, -0.6016],\\n        [-0.3269, -0.1608,  0.2897, -2.0829],\\n        [ 2.6338,  0.9239,  0.6943, -1.5034]], requires_grad=True))\\n('linears.2.bias', Parameter containing:\\ntensor([ 1.0268,  0.4489, -0.9403,  0.1571], requires_grad=True))\\n('final.weight', Parameter containing:\\ntensor([[ 0.2509], [-0.5052], [ 0.3088], [-1.4951]], requires_grad=True))\\n('final.bias', Parameter containing:\\ntensor([0.3381], requires_grad=True))\\n\",\n","   'Id': 693,\n","   'Question': 'How to use Sometimes, it’s necessary for a module to dynamically define submodules.\\nTheModuleListandModuleDictmodules are useful here; they\\nregister submodules from a list or dict:For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\\nThis means that calls toparameters()andnamed_parameters()will\\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network:, give an example?',\n","   'context': ' Sometimes, it’s necessary for a module to dynamically define submodules.\\nTheModuleListandModuleDictmodules are useful here; they\\nregister submodules from a list or dict:For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\\nThis means that calls toparameters()andnamed_parameters()will\\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network:',\n","   'source': 'https://pytorch.org/docs/stable/notes/modules.html'},\n","  {'Answer': \"# Move all parameters to a CUDA device\\ndynamic_net.to(device='cuda')\\n\\n# Change precision of all parameters\\ndynamic_net.to(dtype=torch.float64)\\n\\ndynamic_net(torch.randn(5, device='cuda', dtype=torch.float64))\\n: tensor([6.5166], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\\n\",\n","   'Id': 694,\n","   'Question': 'How to use For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\\nThis means that calls toparameters()andnamed_parameters()will\\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network:It’s also easy to move all parameters to a different device or change their precision usingto():, give an example?',\n","   'context': ' For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\\nThis means that calls toparameters()andnamed_parameters()will\\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network:It’s also easy to move all parameters to a different device or change their precision usingto():',\n","   'source': 'https://pytorch.org/docs/stable/notes/modules.html'},\n","  {'Answer': '# Create the network (from previous section) and optimizer\\nnet = Net()\\noptimizer = torch.optim.SGD(net.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.9)\\n\\n# Run a sample training loop that \"teaches\" the network\\n# to output the constant zero function\\nfor _ in range(10000):\\n  input = torch.randn(4)\\n  output = net(input)\\n  loss = torch.abs(output)\\n  net.zero_grad()\\n  loss.backward()\\n  optimizer.step()\\n',\n","   'Id': 695,\n","   'Question': 'How to use Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch’s\\nOptimizers fromtorch.optim:, give an example?',\n","   'context': ' Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch’s\\nOptimizers fromtorch.optim:',\n","   'source': 'https://pytorch.org/docs/stable/notes/modules.html'},\n","  {'Answer': 'print(net.l1.weight)\\n: Parameter containing:\\ntensor([[-0.0013],\\n        [ 0.0030],\\n        [-0.0008]], requires_grad=True)\\n',\n","   'Id': 696,\n","   'Question': 'How to use In this simplified example, the network learns to simply output zero, as any non-zero output is “penalized” according\\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\\nkey parts of training are present:After the above snippet has been run, note that the network’s parameters have changed. In particular, examining the\\nvalue ofl1’sweightparameter shows that its values are now much closer to 0 (as may be expected):, give an example?',\n","   'context': ' In this simplified example, the network learns to simply output zero, as any non-zero output is “penalized” according\\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\\nkey parts of training are present:After the above snippet has been run, note that the network’s parameters have changed. In particular, examining the\\nvalue ofl1’sweightparameter shows that its values are now much closer to 0 (as may be expected):',\n","   'source': 'https://pytorch.org/docs/stable/notes/modules.html'},\n","  {'Answer': \"# Save the module\\ntorch.save(net.state_dict(), 'net.pt')\\n\\n...\\n\\n# Load the module later on\\nnew_net = Net()\\nnew_net.load_state_dict(torch.load('net.pt'))\\n: <All keys matched successfully>\\n\",\n","   'Id': 697,\n","   'Question': 'How to use In the previous section, we demonstrated training a module’s “parameters”, or learnable aspects of computation.\\nNow, if we want to save the trained model to disk, we can do so by saving itsstate_dict(i.e. “state dictionary”):, give an example?',\n","   'context': ' In the previous section, we demonstrated training a module’s “parameters”, or learnable aspects of computation.\\nNow, if we want to save the trained model to disk, we can do so by saving itsstate_dict(i.e. “state dictionary”):',\n","   'source': 'https://pytorch.org/docs/stable/notes/modules.html'},\n","  {'Answer': \"class RunningMean(nn.Module):\\n  def __init__(self, num_features, momentum=0.9):\\n    super().__init__()\\n    self.momentum = momentum\\n    self.register_buffer('mean', torch.zeros(num_features))\\n  def forward(self, x):\\n    self.mean = self.momentum * self.mean + (1.0 - self.momentum) * x\\n    return self.mean\\n\",\n","   'Id': 698,\n","   'Question': 'How to use A module’sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\\nmodule’s parameters. For some modules, it may be useful to have state beyond parameters that affects module\\ncomputation but is not learnable. For such cases, PyTorch provides the concept of “buffers”, both “persistent”\\nand “non-persistent”. Following is an overview of the various types of state a module can have:As a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want\\nthe current value of the running mean to be considered part of the module’sstate_dictso that it will be\\nrestored when loading a serialized form of the module, but we don’t want it to be learnable.\\nThis snippet shows how to useregister_buffer()to accomplish this:, give an example?',\n","   'context': ' A module’sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\\nmodule’s parameters. For some modules, it may be useful to have state beyond parameters that affects module\\ncomputation but is not learnable. For such cases, PyTorch provides the concept of “buffers”, both “persistent”\\nand “non-persistent”. Following is an overview of the various types of state a module can have:As a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want\\nthe current value of the running mean to be considered part of the module’sstate_dictso that it will be\\nrestored when loading a serialized form of the module, but we don’t want it to be learnable.\\nThis snippet shows how to useregister_buffer()to accomplish this:',\n","   'source': 'https://pytorch.org/docs/stable/notes/modules.html'},\n","  {'Answer': \"m = RunningMean(4)\\nfor _ in range(10):\\n  input = torch.randn(4)\\n  m(input)\\n\\nprint(m.state_dict())\\n: OrderedDict([('mean', tensor([ 0.1041, -0.1113, -0.0647,  0.1515]))]))\\n\\n# Serialized form will contain the 'mean' tensor\\ntorch.save(m.state_dict(), 'mean.pt')\\n\\nm_loaded = RunningMean(4)\\nm_loaded.load_state_dict(torch.load('mean.pt'))\\nassert(torch.all(m.mean == m_loaded.mean))\\n\",\n","   'Id': 699,\n","   'Question': 'How to use As a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want\\nthe current value of the running mean to be considered part of the module’sstate_dictso that it will be\\nrestored when loading a serialized form of the module, but we don’t want it to be learnable.\\nThis snippet shows how to useregister_buffer()to accomplish this:Now, the current value of the running mean is considered part of the module’sstate_dictand will be properly restored when loading the module from disk:, give an example?',\n","   'context': ' As a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want\\nthe current value of the running mean to be considered part of the module’sstate_dictso that it will be\\nrestored when loading a serialized form of the module, but we don’t want it to be learnable.\\nThis snippet shows how to useregister_buffer()to accomplish this:Now, the current value of the running mean is considered part of the module’sstate_dictand will be properly restored when loading the module from disk:',\n","   'source': 'https://pytorch.org/docs/stable/notes/modules.html'},\n","  {'Answer': \"self.register_buffer('unserialized_thing', torch.randn(5), persistent=False)\\n\",\n","   'Id': 700,\n","   'Question': 'How to use Now, the current value of the running mean is considered part of the module’sstate_dictand will be properly restored when loading the module from disk:As mentioned previously, buffers can be left out of the module’sstate_dictby marking them as non-persistent:, give an example?',\n","   'context': ' Now, the current value of the running mean is considered part of the module’sstate_dictand will be properly restored when loading the module from disk:As mentioned previously, buffers can be left out of the module’sstate_dictby marking them as non-persistent:',\n","   'source': 'https://pytorch.org/docs/stable/notes/modules.html'},\n","  {'Answer': \"# Moves all module parameters and buffers to the specified device / dtype\\nm.to(device='cuda', dtype=torch.float64)\\n\",\n","   'Id': 701,\n","   'Question': 'How to use As mentioned previously, buffers can be left out of the module’sstate_dictby marking them as non-persistent:Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied withto():, give an example?',\n","   'context': ' As mentioned previously, buffers can be left out of the module’sstate_dictby marking them as non-persistent:Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied withto():',\n","   'source': 'https://pytorch.org/docs/stable/notes/modules.html'},\n","  {'Answer': '>>> torch.tensor([[1., -1.], [1., -1.]])\\ntensor([[ 1.0000, -1.0000],\\n        [ 1.0000, -1.0000]])\\n>>> torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\\ntensor([[ 1,  2,  3],\\n        [ 4,  5,  6]])\\n',\n","   'Id': 702,\n","   'Question': 'How  A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor:, give an example?',\n","   'context': ' A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor:',\n","   'source': 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor'},\n","  {'Answer': \">>> torch.zeros([2, 4], dtype=torch.int32)\\ntensor([[ 0,  0,  0,  0],\\n        [ 0,  0,  0,  0]], dtype=torch.int32)\\n>>> cuda0 = torch.device('cuda:0')\\n>>> torch.ones([2, 4], dtype=torch.float64, device=cuda0)\\ntensor([[ 1.0000,  1.0000,  1.0000,  1.0000],\\n        [ 1.0000,  1.0000,  1.0000,  1.0000]], dtype=torch.float64, device='cuda:0')\\n\",\n","   'Id': 703,\n","   'Question': 'How  A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\\nconstructor or tensor creation op:, give an example?',\n","   'context': ' A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\\nconstructor or tensor creation op:',\n","   'source': 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor'},\n","  {'Answer': '>>> x = torch.tensor([[1, 2, 3], [4, 5, 6]])\\n>>> print(x[1][2])\\ntensor(6)\\n>>> x[0][1] = 8\\n>>> print(x)\\ntensor([[ 1,  8,  3],\\n        [ 4,  5,  6]])\\n',\n","   'Id': 704,\n","   'Question': 'How  For more information about building Tensors, seeCreation OpsThe contents of a tensor can be accessed and modified using Python’s indexing\\nand slicing notation:, give an example?',\n","   'context': ' For more information about building Tensors, seeCreation OpsThe contents of a tensor can be accessed and modified using Python’s indexing\\nand slicing notation:',\n","   'source': 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor'},\n","  {'Answer': '>>> x = torch.tensor([[1]])\\n>>> x\\ntensor([[ 1]])\\n>>> x.item()\\n1\\n>>> x = torch.tensor(2.5)\\n>>> x\\ntensor(2.5000)\\n>>> x.item()\\n2.5\\n',\n","   'Id': 705,\n","   'Question': 'How  The contents of a tensor can be accessed and modified using Python’s indexing\\nand slicing notation:Usetorch.Tensor.item()to get a Python number from a tensor containing a\\nsingle value:, give an example?',\n","   'context': ' The contents of a tensor can be accessed and modified using Python’s indexing\\nand slicing notation:Usetorch.Tensor.item()to get a Python number from a tensor containing a\\nsingle value:',\n","   'source': 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor'},\n","  {'Answer': '>>> x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)\\n>>> out = x.pow(2).sum()\\n>>> out.backward()\\n>>> x.grad\\ntensor([[ 2.0000, -2.0000],\\n        [ 2.0000,  2.0000]])\\n',\n","   'Id': 706,\n","   'Question': 'How  For more information about indexing, seeIndexing, Slicing, Joining, Mutating OpsA tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation., give an example?',\n","   'context': ' For more information about indexing, seeIndexing, Slicing, Joining, Mutating OpsA tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation.',\n","   'source': 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor'},\n","  {'Answer': '>>> x = torch.arange(8).view(2, 2, 2)\\n>>> x\\ntensor([[[ 0,  1],\\n         [ 2,  3]],\\n\\n        [[ 4,  5],\\n         [ 6,  7]]])\\n>>> torch.flip(x, [0, 1])\\ntensor([[[ 6,  7],\\n         [ 4,  5]],\\n\\n        [[ 2,  3],\\n         [ 0,  1]]])\\n',\n","   'Id': 707,\n","   'Question': 'How to use torch.flip, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip'},\n","  {'Answer': '>>> a = torch.arange(-0.5, 1, 0.5)\\n>>> a\\ntensor([-0.5000,  0.0000,  0.5000])\\n>>> torch.special.entr(a)\\ntensor([  -inf, 0.0000, 0.3466])\\n',\n","   'Id': 708,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.erfc'},\n","  {'Answer': '>>> torch.special.erf(torch.tensor([0, -1., 10.]))\\ntensor([ 0.0000, -0.8427,  1.0000])\\n',\n","   'Id': 709,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.erfc'},\n","  {'Answer': '>>> torch.special.erfc(torch.tensor([0, -1., 10.]))\\ntensor([ 1.0000, 1.8427,  0.0000])\\n',\n","   'Id': 710,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.erfc'},\n","  {'Answer': '>>> torch.special.erfinv(torch.tensor([0, 0.5, -1.]))\\ntensor([ 0.0000,  0.4769,    -inf])\\n',\n","   'Id': 711,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.erfc'},\n","  {'Answer': '>>> t = torch.randn(4)\\n>>> t\\ntensor([ 0.9213,  1.0887, -0.8858, -1.7683])\\n>>> torch.special.expit(t)\\ntensor([ 0.7153,  0.7481,  0.2920,  0.1458])\\n',\n","   'Id': 712,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.erfc'},\n","  {'Answer': '>>> torch.special.expm1(torch.tensor([0, math.log(2.)]))\\ntensor([ 0.,  1.])\\n',\n","   'Id': 713,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.erfc'},\n","  {'Answer': '>>> torch.special.exp2(torch.tensor([0, math.log2(2.), 3, 4]))\\ntensor([ 1.,  2.,  8., 16.])\\n',\n","   'Id': 714,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.erfc'},\n","  {'Answer': '>>> a = torch.arange(0.5, 2, 0.5)\\n>>> torch.special.gammaln(a)\\ntensor([ 0.5724,  0.0000, -0.1208])\\n',\n","   'Id': 715,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.erfc'},\n","  {'Answer': '>>> torch.special.i0e(torch.arange(5, dtype=torch.float32))\\ntensor([1.0000, 0.4658, 0.3085, 0.2430, 0.2070])\\n',\n","   'Id': 716,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.erfc'},\n","  {'Answer': '>>> a = torch.rand(5)\\n>>> a\\ntensor([0.2796, 0.9331, 0.6486, 0.1523, 0.6516])\\n>>> torch.special.logit(a, eps=1e-6)\\ntensor([-0.9466,  2.6352,  0.6131, -1.7169,  0.6261])\\n',\n","   'Id': 717,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.erfc'},\n","  {'Answer': \">>> x = torch.zeros(5,)\\n>>> y = torch.tensor([-1, 0, 1, float('inf'), float('nan')])\\n>>> torch.special.xlog1py(x, y)\\ntensor([0., 0., 0., 0., nan])\\n>>> x = torch.tensor([1, 2, 3])\\n>>> y = torch.tensor([3, 2, 1])\\n>>> torch.special.xlog1py(x, y)\\ntensor([1.3863, 2.1972, 2.0794])\\n>>> torch.special.xlog1py(x, 4)\\ntensor([1.6094, 3.2189, 4.8283])\\n>>> torch.special.xlog1py(2, y)\\ntensor([2.7726, 2.1972, 1.3863])\\n\",\n","   'Id': 718,\n","   'Question': 'How  Similar to SciPy’sscipy.special.xlog1py., give an example?',\n","   'context': ' Similar to SciPy’sscipy.special.xlog1py.',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.erfc'},\n","  {'Answer': 'U, S, Vh = torch.linalg.svd(A, full_matrices=not some)\\nV = Vh.transpose(-2, -1).conj()\\n',\n","   'Id': 719,\n","   'Question': 'How to use torch.svd, give an example?',\n","   'context': ' torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release.U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd'},\n","  {'Answer': 'S = torch.svdvals(A)\\n',\n","   'Id': 720,\n","   'Question': 'How  U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with_,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with, give an example?',\n","   'context': ' U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with_,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd'},\n","  {'Answer': '>>> a = torch.randn(5, 3)\\n>>> a\\ntensor([[ 0.2364, -0.7752,  0.6372],\\n        [ 1.7201,  0.7394, -0.0504],\\n        [-0.3371, -1.0584,  0.5296],\\n        [ 0.3550, -0.4022,  1.5569],\\n        [ 0.2445, -0.0158,  1.1414]])\\n>>> u, s, v = torch.svd(a)\\n>>> u\\ntensor([[ 0.4027,  0.0287,  0.5434],\\n        [-0.1946,  0.8833,  0.3679],\\n        [ 0.4296, -0.2890,  0.5261],\\n        [ 0.6604,  0.2717, -0.2618],\\n        [ 0.4234,  0.2481, -0.4733]])\\n>>> s\\ntensor([2.3289, 2.0315, 0.7806])\\n>>> v\\ntensor([[-0.0199,  0.8766,  0.4809],\\n        [-0.5080,  0.4054, -0.7600],\\n        [ 0.8611,  0.2594, -0.4373]])\\n>>> torch.dist(a, torch.mm(torch.mm(u, torch.diag(s)), v.t()))\\ntensor(8.6531e-07)\\n>>> a_big = torch.randn(7, 5, 3)\\n>>> u, s, v = torch.svd(a_big)\\n>>> torch.dist(a_big, torch.matmul(torch.matmul(u, torch.diag_embed(s)), v.transpose(-2, -1)))\\ntensor(2.6503e-06)\\n',\n","   'Id': 721,\n","   'Question': 'How  Supportsinputof float, double, cfloat and cdouble data types.\\nThe dtypes ofUandVare the same asinput’s.Swill\\nalways be real-valued, even ifinputis complex., give an example?',\n","   'context': ' Supportsinputof float, double, cfloat and cdouble data types.\\nThe dtypes ofUandVare the same asinput’s.Swill\\nalways be real-valued, even ifinputis complex.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd'},\n","  {'Answer': '>>> x=torch.randn(4, dtype=torch.cfloat)\\n>>> x\\ntensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])\\n>>> x.real\\ntensor([ 0.3100, -0.5445, -1.6492, -0.0638])\\n',\n","   'Id': 722,\n","   'Question': 'How to use torch.real, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.real.html#torch.real'},\n","  {'Answer': \">>> x = torch.zeros(5,)\\n>>> y = torch.tensor([-1, 0, 1, float('inf'), float('nan')])\\n>>> torch.xlogy(x, y)\\ntensor([0., 0., 0., 0., nan])\\n>>> x = torch.tensor([1, 2, 3])\\n>>> y = torch.tensor([3, 2, 1])\\n>>> torch.xlogy(x, y)\\ntensor([1.0986, 1.3863, 0.0000])\\n>>> torch.xlogy(x, 4)\\ntensor([1.3863, 2.7726, 4.1589])\\n>>> torch.xlogy(2, y)\\ntensor([2.1972, 1.3863, 0.0000])\\n\",\n","   'Id': 723,\n","   'Question': 'How to use torch.xlogy, give an example?',\n","   'context': ' Similar to SciPy’sscipy.special.xlogy.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.xlogy.html#torch.xlogy'},\n","  {'Answer': '>>> x = torch.tensor([[-1.0, 0.0], [1.0, 2.0]])\\n>>> torch.quantize_per_channel(x, torch.tensor([0.1, 0.01]), torch.tensor([10, 0]), 0, torch.quint8)\\ntensor([[-1.,  0.],\\n        [ 1.,  2.]], size=(2, 2), dtype=torch.quint8,\\n       quantization_scheme=torch.per_channel_affine,\\n       scale=tensor([0.1000, 0.0100], dtype=torch.float64),\\n       zero_point=tensor([10,  0]), axis=0)\\n>>> torch.quantize_per_channel(x, torch.tensor([0.1, 0.01]), torch.tensor([10, 0]), 0, torch.quint8).int_repr()\\ntensor([[  0,  10],\\n        [100, 200]], dtype=torch.uint8)\\n',\n","   'Id': 724,\n","   'Question': 'How to use torch.quantize_per_channel, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.quantize_per_channel.html#torch.quantize_per_channel'},\n","  {'Answer': '>>> x = torch.tensor([1.], requires_grad=True)\\n>>> with torch.no_grad():\\n...   with torch.enable_grad():\\n...     y = x * 2\\n>>> y.requires_grad\\nTrue\\n>>> y.backward()\\n>>> x.grad\\n>>> @torch.enable_grad()\\n... def doubler(x):\\n...     return x * 2\\n>>> with torch.no_grad():\\n...     z = doubler(x)\\n>>> z.requires_grad\\nTrue\\n',\n","   'Id': 725,\n","   'Question': 'How to use torch.enable_grad, give an example?',\n","   'context': ' Also functions as a decorator. (Make sure to instantiate with parenthesis.)',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.enable_grad.html#torch.enable_grad'},\n","  {'Answer': 'L_complex = torch.linalg.eigvals(A)\\n',\n","   'Id': 726,\n","   'Question': 'How to use torch.eig, give an example?',\n","   'context': ' torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors.L,_=torch.eig(A)should be replaced with',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig'},\n","  {'Answer': 'L_complex, V_complex = torch.linalg.eig(A)\\n',\n","   'Id': 727,\n","   'Question': 'How  L,_=torch.eig(A)should be replaced withL,V=torch.eig(A,eigenvectors=True)should be replaced with, give an example?',\n","   'context': ' L,_=torch.eig(A)should be replaced withL,V=torch.eig(A,eigenvectors=True)should be replaced with',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig'},\n","  {'Answer': 'Trivial example with a diagonal matrix. By default, only eigenvalues are computed:\\n\\n>>> a = torch.diag(torch.tensor([1, 2, 3], dtype=torch.double))\\n>>> e, v = torch.eig(a)\\n>>> e\\ntensor([[1., 0.],\\n        [2., 0.],\\n        [3., 0.]], dtype=torch.float64)\\n>>> v\\ntensor([], dtype=torch.float64)\\n\\nCompute also the eigenvectors:\\n\\n>>> e, v = torch.eig(a, eigenvectors=True)\\n>>> e\\ntensor([[1., 0.],\\n        [2., 0.],\\n        [3., 0.]], dtype=torch.float64)\\n>>> v\\ntensor([[1., 0., 0.],\\n        [0., 1., 0.],\\n        [0., 0., 1.]], dtype=torch.float64)\\n',\n","   'Id': 728,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig'},\n","  {'Answer': 'DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,\\n           batch_sampler=None, num_workers=0, collate_fn=None,\\n           pin_memory=False, drop_last=False, timeout=0,\\n           worker_init_fn=None, *, prefetch_factor=2,\\n           persistent_workers=False)\\n',\n","   'Id': 729,\n","   'Question': 'How to use At the heart of PyTorch data loading utility is thetorch.utils.data.DataLoaderclass.  It represents a Python iterable over a dataset, with support forThese options are configured by the constructor arguments of aDataLoader, which has signature:, give an example?',\n","   'context': ' These options are configured by the constructor arguments of aDataLoader, which has signature:',\n","   'source': 'https://pytorch.org/docs/stable/data.html'},\n","  {'Answer': 'for indices in batch_sampler:\\n    yield collate_fn([dataset[i] for i in indices])\\n',\n","   'Id': 730,\n","   'Question': 'How to use After fetching a list of samples using the indices from sampler, the function\\npassed as thecollate_fnargument is used to collate lists of samples\\ninto batches.In this case, loading from a map-style dataset is roughly equivalent with:, give an example?',\n","   'context': ' After fetching a list of samples using the indices from sampler, the function\\npassed as thecollate_fnargument is used to collate lists of samples\\ninto batches.In this case, loading from a map-style dataset is roughly equivalent with:',\n","   'source': 'https://pytorch.org/docs/stable/data.html'},\n","  {'Answer': 'dataset_iter = iter(dataset)\\nfor indices in batch_sampler:\\n    yield collate_fn([next(dataset_iter) for _ in indices])\\n',\n","   'Id': 731,\n","   'Question': 'How to use In this case, loading from a map-style dataset is roughly equivalent with:and loading from an iterable-style dataset is roughly equivalent with:, give an example?',\n","   'context': ' In this case, loading from a map-style dataset is roughly equivalent with:and loading from an iterable-style dataset is roughly equivalent with:',\n","   'source': 'https://pytorch.org/docs/stable/data.html'},\n","  {'Answer': 'for index in sampler:\\n    yield collate_fn(dataset[index])\\n',\n","   'Id': 732,\n","   'Question': 'How to use When automatic batching is disabled, the defaultcollate_fnsimply\\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched.In this case, loading from a map-style dataset is roughly equivalent with:, give an example?',\n","   'context': ' When automatic batching is disabled, the defaultcollate_fnsimply\\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched.In this case, loading from a map-style dataset is roughly equivalent with:',\n","   'source': 'https://pytorch.org/docs/stable/data.html'},\n","  {'Answer': 'for data in iter(dataset):\\n    yield collate_fn(data)\\n',\n","   'Id': 733,\n","   'Question': 'How  In this case, loading from a map-style dataset is roughly equivalent with:and loading from an iterable-style dataset is roughly equivalent with:, give an example?',\n","   'context': ' In this case, loading from a map-style dataset is roughly equivalent with:and loading from an iterable-style dataset is roughly equivalent with:',\n","   'source': 'https://pytorch.org/docs/stable/data.html'},\n","  {'Answer': 'class SimpleCustomBatch:\\n    def __init__(self, data):\\n        transposed_data = list(zip(*data))\\n        self.inp = torch.stack(transposed_data[0], 0)\\n        self.tgt = torch.stack(transposed_data[1], 0)\\n\\n    # custom memory pinning method on custom type\\n    def pin_memory(self):\\n        self.inp = self.inp.pin_memory()\\n        self.tgt = self.tgt.pin_memory()\\n        return self\\n\\ndef collate_wrapper(batch):\\n    return SimpleCustomBatch(batch)\\n\\ninps = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\\ntgts = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\\ndataset = TensorDataset(inps, tgts)\\n\\nloader = DataLoader(dataset, batch_size=2, collate_fn=collate_wrapper,\\n                    pin_memory=True)\\n\\nfor batch_ndx, sample in enumerate(loader):\\n    print(sample.inp.is_pinned())\\n    print(sample.tgt.is_pinned())\\n',\n","   'Id': 734,\n","   'Question': 'How to use See the example below., give an example?',\n","   'context': ' See the example below.',\n","   'source': 'https://pytorch.org/docs/stable/data.html'},\n","  {'Answer': '>>> class MyIterableDataset(torch.utils.data.IterableDataset):\\n...     def __init__(self, start, end):\\n...         super(MyIterableDataset).__init__()\\n...         assert end > start, \"this example code only works with end >= start\"\\n...         self.start = start\\n...         self.end = end\\n...\\n...     def __iter__(self):\\n...         worker_info = torch.utils.data.get_worker_info()\\n...         if worker_info is None:  # single-process data loading, return the full iterator\\n...             iter_start = self.start\\n...             iter_end = self.end\\n...         else:  # in a worker process\\n...             # split workload\\n...             per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers)))\\n...             worker_id = worker_info.id\\n...             iter_start = self.start + worker_id * per_worker\\n...             iter_end = min(iter_start + per_worker, self.end)\\n...         return iter(range(iter_start, iter_end))\\n...\\n>>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6].\\n>>> ds = MyIterableDataset(start=3, end=7)\\n\\n>>> # Single-process loading\\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\\n[3, 4, 5, 6]\\n\\n>>> # Mult-process loading with two worker processes\\n>>> # Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].\\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\\n[3, 5, 4, 6]\\n\\n>>> # With even more workers\\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=20)))\\n[3, 4, 5, 6]\\n',\n","   'Id': 735,\n","   'Question': 'How to use torch.utils.data.IterableDataset, give an example?',\n","   'context': ' When a subclass is used withDataLoader, each\\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\\ndifferent copy of the dataset object, so it is often desired to configure\\neach copy independently to avoid having duplicate data returned from the\\nworkers.get_worker_info(), when called in a worker\\nprocess, returns information about the worker. It can be used in either the\\ndataset’s__iter__()method or theDataLoader‘sworker_init_fnoption to modify each copy’s behavior.',\n","   'source': 'https://pytorch.org/docs/stable/data.html'},\n","  {'Answer': '>>> class MyIterableDataset(torch.utils.data.IterableDataset):\\n...     def __init__(self, start, end):\\n...         super(MyIterableDataset).__init__()\\n...         assert end > start, \"this example code only works with end >= start\"\\n...         self.start = start\\n...         self.end = end\\n...\\n...     def __iter__(self):\\n...         return iter(range(self.start, self.end))\\n...\\n>>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6].\\n>>> ds = MyIterableDataset(start=3, end=7)\\n\\n>>> # Single-process loading\\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\\n[3, 4, 5, 6]\\n>>>\\n>>> # Directly doing multi-process loading yields duplicate data\\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\\n[3, 3, 4, 4, 5, 5, 6, 6]\\n\\n>>> # Define a `worker_init_fn` that configures each dataset copy differently\\n>>> def worker_init_fn(worker_id):\\n...     worker_info = torch.utils.data.get_worker_info()\\n...     dataset = worker_info.dataset  # the dataset copy in this worker process\\n...     overall_start = dataset.start\\n...     overall_end = dataset.end\\n...     # configure the dataset to only process the split workload\\n...     per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers)))\\n...     worker_id = worker_info.id\\n...     dataset.start = overall_start + worker_id * per_worker\\n...     dataset.end = min(dataset.start + per_worker, overall_end)\\n...\\n\\n>>> # Mult-process loading with the custom `worker_init_fn`\\n>>> # Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].\\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=2, worker_init_fn=worker_init_fn)))\\n[3, 5, 4, 6]\\n\\n>>> # With even more workers\\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=20, worker_init_fn=worker_init_fn)))\\n[3, 4, 5, 6]\\n',\n","   'Id': 736,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/data.html'},\n","  {'Answer': '>>> random_split(range(10), [3, 7], generator=torch.Generator().manual_seed(42))\\n',\n","   'Id': 737,\n","   'Question': 'How to use torch.utils.data.random_split, give an example?',\n","   'context': ' Randomly split a dataset into non-overlapping new datasets of given lengths.\\nOptionally fix the generator for reproducible results, e.g.:',\n","   'source': 'https://pytorch.org/docs/stable/data.html'},\n","  {'Answer': '>>> list(WeightedRandomSampler([0.1, 0.9, 0.4, 0.7, 3.0, 0.6], 5, replacement=True))\\n[4, 4, 1, 4, 5]\\n>>> list(WeightedRandomSampler([0.9, 0.4, 0.05, 0.2, 0.3, 0.1], 5, replacement=False))\\n[0, 1, 4, 3, 2]\\n',\n","   'Id': 738,\n","   'Question': 'How to use torch.utils.data.WeightedRandomSampler, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/data.html'},\n","  {'Answer': '>>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False))\\n[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\\n>>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True))\\n[[0, 1, 2], [3, 4, 5], [6, 7, 8]]\\n',\n","   'Id': 739,\n","   'Question': 'How to use torch.utils.data.BatchSampler, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/data.html'},\n","  {'Answer': '>>> sampler = DistributedSampler(dataset) if is_distributed else None\\n>>> loader = DataLoader(dataset, shuffle=(sampler is None),\\n...                     sampler=sampler)\\n>>> for epoch in range(start_epoch, n_epochs):\\n...     if is_distributed:\\n...         sampler.set_epoch(epoch)\\n...     train(loader)\\n',\n","   'Id': 740,\n","   'Question': 'How to use torch.utils.data.distributed.DistributedSampler, give an example?',\n","   'context': ' It is especially useful in conjunction withtorch.nn.parallel.DistributedDataParallel. In such a case, each\\nprocess can pass aDistributedSamplerinstance as aDataLoadersampler, and load a subset of the\\noriginal dataset that is exclusive to it.',\n","   'source': 'https://pytorch.org/docs/stable/data.html'},\n","  {'Answer': '>>> a = torch.randn(1, 3)\\n>>> a\\ntensor([[ 0.1133, -0.9567,  0.2958]])\\n>>> torch.sum(a)\\ntensor(-0.5475)\\n',\n","   'Id': 741,\n","   'Question': 'How to use torch.sum, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum'},\n","  {'Answer': '>>> a = torch.randn(4, 4)\\n>>> a\\ntensor([[ 0.0569, -0.2475,  0.0737, -0.3429],\\n        [-0.2993,  0.9138,  0.9337, -1.6864],\\n        [ 0.1132,  0.7892, -0.1003,  0.5688],\\n        [ 0.3637, -0.9906, -0.4752, -1.5197]])\\n>>> torch.sum(a, 1)\\ntensor([-0.4598, -0.1381,  1.3708, -2.6217])\\n>>> b = torch.arange(4 * 5 * 6).view(4, 5, 6)\\n>>> torch.sum(b, (2, 1))\\ntensor([  435.,  1335.,  2235.,  3135.])\\n',\n","   'Id': 742,\n","   'Question': 'How  IfkeepdimisTrue, the output tensor is of the same size\\nasinputexcept in the dimension(s)dimwhere it is of size 1.\\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\\noutput tensor having 1 (orlen(dim)) fewer dimension(s)., give an example?',\n","   'context': ' IfkeepdimisTrue, the output tensor is of the same size\\nasinputexcept in the dimension(s)dimwhere it is of size 1.\\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\\noutput tensor having 1 (orlen(dim)) fewer dimension(s).',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum'},\n","  {'Answer': '>>> torch.bitwise_not(torch.tensor([-1, -2, 3], dtype=torch.int8))\\ntensor([ 0,  1, -4], dtype=torch.int8)\\n',\n","   'Id': 743,\n","   'Question': 'How to use torch.bitwise_not, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.bitwise_not.html#torch.bitwise_not'},\n","  {'Answer': '>>> t = torch.arange(16.0).reshape(4,4)\\n>>> t\\ntensor([[ 0.,  1.,  2.,  3.],\\n        [ 4.,  5.,  6.,  7.],\\n        [ 8.,  9., 10., 11.],\\n        [12., 13., 14., 15.]])\\n>>> torch.vsplit(t, 2)\\n(tensor([[0., 1., 2., 3.],\\n         [4., 5., 6., 7.]]),\\n tensor([[ 8.,  9., 10., 11.],\\n         [12., 13., 14., 15.]]))\\n>>> torch.vsplit(t, [3, 6])\\n(tensor([[ 0.,  1.,  2.,  3.],\\n         [ 4.,  5.,  6.,  7.],\\n         [ 8.,  9., 10., 11.]]),\\n tensor([[12., 13., 14., 15.]]),\\n tensor([], size=(0, 4)))\\n',\n","   'Id': 744,\n","   'Question': 'How to use torch.vsplit, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.vsplit.html#torch.vsplit'},\n","  {'Answer': '>>> t = torch.arange(16.0).reshape(4,4)\\n>>> t\\ntensor([[ 0.,  1.,  2.,  3.],\\n        [ 4.,  5.,  6.,  7.],\\n        [ 8.,  9., 10., 11.],\\n        [12., 13., 14., 15.]])\\n>>> torch.hsplit(t, 2)\\n(tensor([[ 0.,  1.],\\n         [ 4.,  5.],\\n         [ 8.,  9.],\\n         [12., 13.]]),\\n tensor([[ 2.,  3.],\\n         [ 6.,  7.],\\n         [10., 11.],\\n         [14., 15.]]))\\n>>> torch.hsplit(t, [3, 6])\\n(tensor([[ 0.,  1.,  2.],\\n         [ 4.,  5.,  6.],\\n         [ 8.,  9., 10.],\\n         [12., 13., 14.]]),\\n tensor([[ 3.],\\n         [ 7.],\\n         [11.],\\n         [15.]]),\\n tensor([], size=(4, 0)))\\n',\n","   'Id': 745,\n","   'Question': 'How to use torch.hsplit, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.hsplit.html#torch.hsplit'},\n","  {'Answer': '>>> a = torch.arange(-0.5, 1, 0.5)\\n>>> a\\ntensor([-0.5000,  0.0000,  0.5000])\\n>>> torch.special.entr(a)\\ntensor([  -inf, 0.0000, 0.3466])\\n',\n","   'Id': 746,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.expit'},\n","  {'Answer': '>>> torch.special.erf(torch.tensor([0, -1., 10.]))\\ntensor([ 0.0000, -0.8427,  1.0000])\\n',\n","   'Id': 747,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.expit'},\n","  {'Answer': '>>> torch.special.erfc(torch.tensor([0, -1., 10.]))\\ntensor([ 1.0000, 1.8427,  0.0000])\\n',\n","   'Id': 748,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.expit'},\n","  {'Answer': '>>> torch.special.erfinv(torch.tensor([0, 0.5, -1.]))\\ntensor([ 0.0000,  0.4769,    -inf])\\n',\n","   'Id': 749,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.expit'},\n","  {'Answer': '>>> t = torch.randn(4)\\n>>> t\\ntensor([ 0.9213,  1.0887, -0.8858, -1.7683])\\n>>> torch.special.expit(t)\\ntensor([ 0.7153,  0.7481,  0.2920,  0.1458])\\n',\n","   'Id': 750,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.expit'},\n","  {'Answer': '>>> torch.special.expm1(torch.tensor([0, math.log(2.)]))\\ntensor([ 0.,  1.])\\n',\n","   'Id': 751,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.expit'},\n","  {'Answer': '>>> torch.special.exp2(torch.tensor([0, math.log2(2.), 3, 4]))\\ntensor([ 1.,  2.,  8., 16.])\\n',\n","   'Id': 752,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.expit'},\n","  {'Answer': '>>> a = torch.arange(0.5, 2, 0.5)\\n>>> torch.special.gammaln(a)\\ntensor([ 0.5724,  0.0000, -0.1208])\\n',\n","   'Id': 753,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.expit'},\n","  {'Answer': '>>> torch.special.i0e(torch.arange(5, dtype=torch.float32))\\ntensor([1.0000, 0.4658, 0.3085, 0.2430, 0.2070])\\n',\n","   'Id': 754,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.expit'},\n","  {'Answer': '>>> a = torch.rand(5)\\n>>> a\\ntensor([0.2796, 0.9331, 0.6486, 0.1523, 0.6516])\\n>>> torch.special.logit(a, eps=1e-6)\\ntensor([-0.9466,  2.6352,  0.6131, -1.7169,  0.6261])\\n',\n","   'Id': 755,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.expit'},\n","  {'Answer': \">>> x = torch.zeros(5,)\\n>>> y = torch.tensor([-1, 0, 1, float('inf'), float('nan')])\\n>>> torch.special.xlog1py(x, y)\\ntensor([0., 0., 0., 0., nan])\\n>>> x = torch.tensor([1, 2, 3])\\n>>> y = torch.tensor([3, 2, 1])\\n>>> torch.special.xlog1py(x, y)\\ntensor([1.3863, 2.1972, 2.0794])\\n>>> torch.special.xlog1py(x, 4)\\ntensor([1.6094, 3.2189, 4.8283])\\n>>> torch.special.xlog1py(2, y)\\ntensor([2.7726, 2.1972, 1.3863])\\n\",\n","   'Id': 756,\n","   'Question': 'How  Similar to SciPy’sscipy.special.xlog1py., give an example?',\n","   'context': ' Similar to SciPy’sscipy.special.xlog1py.',\n","   'source': 'https://pytorch.org/docs/stable/special.html#torch.special.expit'},\n","  {'Answer': '>>> x = torch.arange(4).view(2, 2)\\n>>> x\\ntensor([[0, 1],\\n        [2, 3]])\\n>>> torch.fliplr(x)\\ntensor([[1, 0],\\n        [3, 2]])\\n',\n","   'Id': 757,\n","   'Question': 'How to use torch.fliplr, give an example?',\n","   'context': ' Flip the entries in each row in the left/right direction.\\nColumns are preserved, but appear in a different order than before.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.fliplr.html#torch.fliplr'},\n","  {'Answer': '>>> a = torch.randn(4)\\n>>> a\\ntensor([ 0.1632,  1.1835, -0.6979, -0.7325])\\n>>> torch.cosh(a)\\ntensor([ 1.0133,  1.7860,  1.2536,  1.2805])\\n',\n","   'Id': 758,\n","   'Question': 'How to use torch.cosh, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.cosh.html#torch.cosh'},\n","  {'Answer': '>>> a = torch.tensor([[3.142, -3.142], [6.283, -6.283], [1.570, -1.570]])\\n>>> torch.rad2deg(a)\\ntensor([[ 180.0233, -180.0233],\\n        [ 359.9894, -359.9894],\\n        [  89.9544,  -89.9544]])\\n',\n","   'Id': 759,\n","   'Question': 'How to use torch.rad2deg, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.rad2deg.html#torch.rad2deg'},\n","  {'Answer': '>>> torch.broadcast_shapes((2,), (3, 1), (1, 1, 1))\\ntorch.Size([1, 3, 2])\\n',\n","   'Id': 760,\n","   'Question': 'How to use torch.broadcast_shapes, give an example?',\n","   'context': ' This is equivalent totorch.broadcast_tensors(*map(torch.empty,shapes))[0].shapebut avoids the need create to intermediate tensors. This is useful for\\nbroadcasting tensors of common batch shape but different rightmost shape,\\ne.g. to broadcast mean vectors with covariance matrices.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.broadcast_shapes.html#torch.broadcast_shapes'},\n","  {'Answer': '>>> torch.get_default_dtype()  # initial default for floating point is torch.float32\\ntorch.float32\\n>>> torch.set_default_dtype(torch.float64)\\n>>> torch.get_default_dtype()  # default is now changed to torch.float64\\ntorch.float64\\n>>> torch.set_default_tensor_type(torch.FloatTensor)  # setting tensor type also affects this\\n>>> torch.get_default_dtype()  # changed to torch.float32, the dtype for torch.FloatTensor\\ntorch.float32\\n',\n","   'Id': 761,\n","   'Question': 'How to use torch.get_default_dtype, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.get_default_dtype.html#torch.get_default_dtype'},\n","  {'Answer': '>>> a = [1, 2, 3]\\n>>> b = [4, 5]\\n>>> list(itertools.product(a, b))\\n[(1, 4), (1, 5), (2, 4), (2, 5), (3, 4), (3, 5)]\\n>>> tensor_a = torch.tensor(a)\\n>>> tensor_b = torch.tensor(b)\\n>>> torch.cartesian_prod(tensor_a, tensor_b)\\ntensor([[1, 4],\\n        [1, 5],\\n        [2, 4],\\n        [2, 5],\\n        [3, 4],\\n        [3, 5]])\\n',\n","   'Id': 762,\n","   'Question': 'How to use torch.cartesian_prod, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.cartesian_prod.html#torch.cartesian_prod'},\n","  {'Answer': '>>> x = torch.zeros(1, requires_grad=True)\\n>>> with torch.no_grad():\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> is_train = False\\n>>> with torch.set_grad_enabled(is_train):\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> torch.set_grad_enabled(True)  # this can also be used as a function\\n>>> y = x * 2\\n>>> y.requires_grad\\nTrue\\n\\n>>> torch.set_grad_enabled(False)\\n>>> y = x * 2\\n>>> y.requires_grad\\nFalse\\n',\n","   'Id': 763,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/torch.html#generators'},\n","  {'Answer': '>>> input = torch.empty(2, 3)\\n>>> torch.zeros_like(input)\\ntensor([[ 0.,  0.,  0.],\\n        [ 0.,  0.,  0.]])\\n',\n","   'Id': 764,\n","   'Question': 'How to use torch.zeros_like, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like'},\n","  {'Answer': '>>> t = torch.tensor([[[1, 2],\\n...                    [3, 4]],\\n...                   [[5, 6],\\n...                    [7, 8]]])\\n>>> torch.flatten(t)\\ntensor([1, 2, 3, 4, 5, 6, 7, 8])\\n>>> torch.flatten(t, start_dim=1)\\ntensor([[1, 2, 3, 4],\\n        [5, 6, 7, 8]])\\n',\n","   'Id': 765,\n","   'Question': 'How to use torch.flatten, give an example?',\n","   'context': ' Unlike NumPy’s flatten, which always copies input’s data, this function may return the original object, a view,\\nor copy. If no dimensions are flattened, then the original objectinputis returned. Otherwise, if input can\\nbe viewed as the flattened shape, then that view is returned. Finally, only if the input cannot be viewed as the\\nflattened shape is input’s data copied. Seetorch.Tensor.view()for details on when a view will be returned.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.flatten.html#torch.flatten'},\n","  {'Answer': '>>> torch.bitwise_and(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))\\ntensor([1, 0,  3], dtype=torch.int8)\\n>>> torch.bitwise_and(torch.tensor([True, True, False]), torch.tensor([False, True, False]))\\ntensor([ False, True, False])\\n',\n","   'Id': 766,\n","   'Question': 'How to use torch.bitwise_and, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.bitwise_and.html#torch.bitwise_and'},\n","  {'Answer': '>>> a = torch.tensor([1, 0.5])\\n>>> torch.digamma(a)\\ntensor([-0.5772, -1.9635])\\n',\n","   'Id': 767,\n","   'Question': 'How to use torch.digamma, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.digamma.html#torch.digamma'},\n","  {'Answer': '>>> x = torch.zeros(1, requires_grad=True)\\n>>> with torch.no_grad():\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> is_train = False\\n>>> with torch.set_grad_enabled(is_train):\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> torch.set_grad_enabled(True)  # this can also be used as a function\\n>>> y = x * 2\\n>>> y.requires_grad\\nTrue\\n\\n>>> torch.set_grad_enabled(False)\\n>>> y = x * 2\\n>>> y.requires_grad\\nFalse\\n',\n","   'Id': 768,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/torch.html#indexing-slicing-joining-mutating-ops'},\n","  {'Answer': 'import torch\\n# Simple module for demonstration\\nclass MyModule(torch.nn.Module):\\n    def __init__(self):\\n        super().__init__()\\n        self.param = torch.nn.Parameter(torch.rand(3, 4))\\n        self.linear = torch.nn.Linear(4, 5)\\n\\n    def forward(self, x):\\n        return self.linear(x + self.param).clamp(min=0.0, max=1.0)\\n\\nmodule = MyModule()\\n\\nfrom torch.fx import symbolic_trace\\n# Symbolic tracing frontend - captures the semantics of the module\\nsymbolic_traced : torch.fx.GraphModule = symbolic_trace(module)\\n\\n# High-level intermediate representation (IR) - Graph representation\\nprint(symbolic_traced.graph)\\n\"\"\"\\ngraph(x):\\n    %param : [#users=1] = self.param\\n    %add_1 : [#users=1] = call_function[target=<built-in function add>](args = (%x, %param), kwargs = {})\\n    %linear_1 : [#users=1] = call_module[target=linear](args = (%add_1,), kwargs = {})\\n    %clamp_1 : [#users=1] = call_method[target=clamp](args = (%linear_1,), kwargs = {min: 0.0, max: 1.0})\\n    return clamp_1\\n\"\"\"\\n\\n# Code generation - valid Python code\\nprint(symbolic_traced.code)\\n\"\"\"\\ndef forward(self, x):\\n    param = self.param\\n    add_1 = x + param;  x = param = None\\n    linear_1 = self.linear(add_1);  add_1 = None\\n    clamp_1 = linear_1.clamp(min = 0.0, max = 1.0);  linear_1 = None\\n    return clamp_1\\n\"\"\"\\n',\n","   'Id': 769,\n","   'Question': 'How to use This feature is under a Beta release and its API may change.FX is a toolkit for developers to use to transformnn.Moduleinstances. FX consists of three main components: asymbolic tracer,anintermediate representation, andPython code generation. A\\ndemonstration of these components in action:, give an example?',\n","   'context': ' FX is a toolkit for developers to use to transformnn.Moduleinstances. FX consists of three main components: asymbolic tracer,anintermediate representation, andPython code generation. A\\ndemonstration of these components in action:',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': \"import torch\\nimport torch.fx\\n\\ndef transform(m: nn.Module,\\n              tracer_class : type = torch.fx.Tracer) -> torch.nn.Module:\\n    # Step 1: Acquire a Graph representing the code in `m`\\n\\n    # NOTE: torch.fx.symbolic_trace is a wrapper around a call to\\n    # fx.Tracer.trace and constructing a GraphModule. We'll\\n    # split that out in our transform to allow the caller to\\n    # customize tracing behavior.\\n    graph : torch.fx.Graph = tracer_class().trace(m)\\n\\n    # Step 2: Modify this Graph or create a new one\\n    graph = ...\\n\\n    # Step 3: Construct a Module to return\\n    return torch.fx.GraphModule(m, graph)\\n\",\n","   'Id': 770,\n","   'Question': 'How to use What is an FX transform? Essentially, it’s a function that looks like this., give an example?',\n","   'context': ' What is an FX transform? Essentially, it’s a function that looks like this.',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': 'import torch\\nimport torch.fx\\n\\ndef transform(m : nn.Module) -> nn.Module:\\n    gm : torch.fx.GraphModule = torch.fx.symbolic_trace(m)\\n\\n    # Modify gm.graph\\n    # <...>\\n\\n    # Recompile the forward() method of `gm` from its Graph\\n    gm.recompile()\\n\\n    return gm\\n',\n","   'Id': 771,\n","   'Question': 'How to use NoteIt is also possible to modify an existingGraphModuleinstead of\\ncreating a new one, like so:, give an example?',\n","   'context': ' It is also possible to modify an existingGraphModuleinstead of\\ncreating a new one, like so:',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': 'import torch\\nimport torch.fx\\n\\nclass MyModule(torch.nn.Module):\\n    def __init__(self):\\n        super().__init__()\\n        self.param = torch.nn.Parameter(torch.rand(3, 4))\\n        self.linear = torch.nn.Linear(4, 5)\\n\\n    def forward(self, x):\\n        return torch.topk(torch.sum(\\n            self.linear(x + self.linear.weight).relu(), dim=-1), 3)\\n\\nm = MyModule()\\ngm = torch.fx.symbolic_trace(m)\\n\\ngm.graph.print_tabular()\\n',\n","   'Id': 772,\n","   'Question': 'How to use Full treatment of the semantics of graphs can be found in theGraphdocumentation, but we are going to cover the basics here. AGraphis\\na data structure that represents a method on aGraphModule. The\\ninformation that this requires is:All three of these concepts are represented withNodeinstances.\\nLet’s see what we mean by that with a short example:, give an example?',\n","   'context': ' All three of these concepts are represented withNodeinstances.\\nLet’s see what we mean by that with a short example:',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': \"import torch\\nimport torch.fx\\n\\n# Sample module\\nclass M(torch.nn.Module):\\n    def forward(self, x, y):\\n        return torch.add(x, y)\\n\\ndef transform(m: torch.nn.Module,\\n              tracer_class : type = fx.Tracer) -> torch.nn.Module:\\n    graph : fx.Graph = tracer_class().trace(m)\\n    # FX represents its Graph as an ordered list of\\n    # nodes, so we can iterate through them.\\n    for node in graph.nodes:\\n        # Checks if we're calling a function (i.e:\\n        # torch.add)\\n        if node.op == 'call_function':\\n            # The target attribute is the function\\n            # that call_function calls.\\n            if node.target == torch.add:\\n                node.target = torch.mul\\n\\n    graph.lint() # Does some checks to make sure the\\n                 # Graph is well-formed.\\n\\n    return fx.GraphModule(m, graph)\\n\",\n","   'Id': 773,\n","   'Question': 'How to use One approach to building this newGraphis to directly manipulate your old\\none. To aid in this, we can simply take theGraphwe obtain from symbolic\\ntracing and modify it. For example, let’s say we desire to replacetorch.add()calls withtorch.mul()calls., give an example?',\n","   'context': ' One approach to building this newGraphis to directly manipulate your old\\none. To aid in this, we can simply take theGraphwe obtain from symbolic\\ntracing and modify it. For example, let’s say we desire to replacetorch.add()calls withtorch.mul()calls.',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': \"# Specifies the insertion point. Any nodes added to the\\n# Graph within this scope will be inserted after `node`\\nwith traced.graph.inserting_after(node):\\n    # Insert a new `call_function` node calling `torch.relu`\\n    new_node = traced.graph.call_function(\\n        torch.relu, args=(node,))\\n\\n    # We want all places that used the value of `node` to\\n    # now use that value after the `relu` call we've added.\\n    # We use the `replace_all_uses_with` API to do this.\\n    node.replace_all_uses_with(new_node)\\n\",\n","   'Id': 774,\n","   'Question': 'How to use One approach to building this newGraphis to directly manipulate your old\\none. To aid in this, we can simply take theGraphwe obtain from symbolic\\ntracing and modify it. For example, let’s say we desire to replacetorch.add()calls withtorch.mul()calls.We can also do more involvedGraphrewrites, such as\\ndeleting or appending nodes. To aid in these transformations,\\nFX has utility functions for transforming the graph that can\\nbe found in theGraphdocumentation. An\\nexample of using these APIs to append atorch.relu()call\\ncan be found below., give an example?',\n","   'context': ' We can also do more involvedGraphrewrites, such as\\ndeleting or appending nodes. To aid in these transformations,\\nFX has utility functions for transforming the graph that can\\nbe found in theGraphdocumentation. An\\nexample of using these APIs to append atorch.relu()call\\ncan be found below.',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': '# Note that this decomposition rule can be read as regular Python\\ndef relu_decomposition(x):\\n    return (x > 0) * x\\n\\ndecomposition_rules = {}\\ndecomposition_rules[F.relu] = relu_decomposition\\n\\ndef decompose(model: torch.nn.Module,\\n              tracer_class : type = fx.Tracer) -> torch.nn.Module:\\n    \"\"\"\\n    Decompose `model` into smaller constituent operations.\\n    Currently,this only supports decomposing ReLU into its\\n    mathematical definition: (x > 0) * x\\n    \"\"\"\\n    graph : fx.Graph = tracer_class().trace(model)\\n    new_graph = fx.Graph()\\n    env = {}\\n    for node in graph.nodes:\\n        if node.op == \\'call_function\\' and node.target in decomposition_rules:\\n            # By wrapping the arguments with proxies,\\n            # we can dispatch to the appropriate\\n            # decomposition rule and implicitly add it\\n            # to the Graph by symbolically tracing it.\\n            proxy_args = [\\n                fx.Proxy(env[x.name]) if isinstance(x, fx.Node) else x for x in node.args]\\n            output_proxy = decomposition_rules[node.target](*proxy_args)\\n\\n            # Operations on `Proxy` always yield new `Proxy`s, and the\\n            # return value of our decomposition rule is no exception.\\n            # We need to extract the underlying `Node` from the `Proxy`\\n            # to use it in subsequent iterations of this transform.\\n            new_node = output_proxy.node\\n            env[node.name] = new_node\\n        else:\\n            # Default case: we don\\'t have a decomposition rule for this\\n            # node, so just copy the node over into the new graph.\\n            new_node = new_graph.node_copy(node, lambda x: env[x.name])\\n            env[node.name] = new_node\\n    return fx.GraphModule(model, new_graph)\\n',\n","   'Id': 775,\n","   'Question': 'How to use Another way of manipulatingGraphs is by reusing theProxymachinery used in symbolic tracing. For example, let’s\\nimagine that we wanted to write a transformation that decomposed\\nPyTorch functions into smaller operations. It would transform everyF.relu(x)call into(x>0)*x. One possibility would be to\\nperform the requisite graph rewriting to insert the comparison and\\nmultiplication after theF.relu, and then clean up the originalF.relu. However, we can automate this process by usingProxyobjects to automatically record operations into theGraph.To use this method, we write the operations that we want inserted as regular\\nPyTorch code and invoke that code withProxyobjects as arugments.\\nTheseProxyobjects will capture the operations that are performed\\non them and append them to theGraph., give an example?',\n","   'context': ' To use this method, we write the operations that we want inserted as regular\\nPyTorch code and invoke that code withProxyobjects as arugments.\\nTheseProxyobjects will capture the operations that are performed\\non them and append them to theGraph.',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': 'import torch\\nimport torch.fx\\nfrom torch.fx.node import Node\\n\\nfrom typing import Dict\\n\\nclass ShapeProp:\\n    \"\"\"\\n    Shape propagation. This class takes a `GraphModule`.\\n    Then, its `propagate` method executes the `GraphModule`\\n    node-by-node with the given arguments. As each operation\\n    executes, the ShapeProp class stores away the shape and\\n    element type for the output values of each operation on\\n    the `shape` and `dtype` attributes of the operation\\'s\\n    `Node`.\\n    \"\"\"\\n    def __init__(self, mod):\\n        self.mod = mod\\n        self.graph = mod.graph\\n        self.modules = dict(self.mod.named_modules())\\n\\n    def propagate(self, *args):\\n        args_iter = iter(args)\\n        env : Dict[str, Node] = {}\\n\\n        def load_arg(a):\\n            return torch.fx.graph.map_arg(a, lambda n: env[n.name])\\n\\n        def fetch_attr(target : str):\\n            target_atoms = target.split(\\'.\\')\\n            attr_itr = self.mod\\n            for i, atom in enumerate(target_atoms):\\n                if not hasattr(attr_itr, atom):\\n                    raise RuntimeError(f\"Node referenced nonexistant target {\\'.\\'.join(target_atoms[:i])}\")\\n                attr_itr = getattr(attr_itr, atom)\\n            return attr_itr\\n\\n        for node in self.graph.nodes:\\n            if node.op == \\'placeholder\\':\\n                result = next(args_iter)\\n            elif node.op == \\'get_attr\\':\\n                result = fetch_attr(node.target)\\n            elif node.op == \\'call_function\\':\\n                result = node.target(*load_arg(node.args), **load_arg(node.kwargs))\\n            elif node.op == \\'call_method\\':\\n                self_obj, *args = load_arg(node.args)\\n                kwargs = load_arg(node.kwargs)\\n                result = getattr(self_obj, node.target)(*args, **kwargs)\\n            elif node.op == \\'call_module\\':\\n                result = self.modules[node.target](*load_arg(node.args), **load_arg(node.kwargs))\\n\\n            # This is the only code specific to shape propagation.\\n            # you can delete this `if` branch and this becomes\\n            # a generic GraphModule interpreter.\\n            if isinstance(result, torch.Tensor):\\n                node.shape = result.shape\\n                node.dtype = result.dtype\\n\\n            env[node.name] = result\\n\\n        return load_arg(self.graph.result)\\n',\n","   'Id': 776,\n","   'Question': 'How to use A useful code organizational pattern in FX is to loop over all theNodes\\nin aGraphand execute them. This can be used for several things including\\nruntime analysis of values flowing through the graph or transformation of the code\\nvia retracing withProxys. For example, suppose we want to run aGraphModuleand record thetorch.Tensorshape and dtype\\nproperties on the nodes as we see them at runtime. That might look like:, give an example?',\n","   'context': ' A useful code organizational pattern in FX is to loop over all theNodes\\nin aGraphand execute them. This can be used for several things including\\nruntime analysis of values flowing through the graph or transformation of the code\\nvia retracing withProxys. For example, suppose we want to run aGraphModuleand record thetorch.Tensorshape and dtype\\nproperties on the nodes as we see them at runtime. That might look like:',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': 'import torch\\nimport torch.fx\\nimport torchvision.models as models\\n\\ndef transform(m : torch.nn.Module) -> torch.nn.Module:\\n    gm = torch.fx.symbolic_trace(m)\\n\\n    # Imagine we\\'re doing some transforms here\\n    # <...>\\n\\n    gm.recompile()\\n\\n    return gm\\n\\nresnet18 = models.resnet18()\\ntransformed_resnet18 = transform(resnet18)\\n\\ninput_image = torch.randn(5, 3, 224, 224)\\n\\nassert resnet18(input_image) == transformed_resnet18(input_image)\\n\"\"\"\\nRuntimeError: Boolean value of Tensor with more than one value is ambiguous\\n\"\"\"\\n',\n","   'Id': 777,\n","   'Question': 'How to use Because the output of most deep learning modules consists of floating\\npointtorch.Tensorinstances, checking for equivalence between\\nthe results of twotorch.nn.Moduleis not as straightforward\\nas doing a simple equality check. To motivate this, let’s use an\\nexample:, give an example?',\n","   'context': ' Because the output of most deep learning modules consists of floating\\npointtorch.Tensorinstances, checking for equivalence between\\nthe results of twotorch.nn.Moduleis not as straightforward\\nas doing a simple equality check. To motivate this, let’s use an\\nexample:',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': 'assert torch.allclose(resnet18(input_image), transformed_resnet18(input_image))\\n',\n","   'Id': 778,\n","   'Question': 'How to use Because the output of most deep learning modules consists of floating\\npointtorch.Tensorinstances, checking for equivalence between\\nthe results of twotorch.nn.Moduleis not as straightforward\\nas doing a simple equality check. To motivate this, let’s use an\\nexample:Here, we’ve tried to check equality of the values of two deep learning\\nmodels with the==equality operator. However, this is not well-\\ndefined both due to the issue of that operator returning a tensor\\nand not a bool, but also because comparison of floating point values\\nshould use a margin of error (or epsilon) to account for the\\nnon-commutativity of floating point operations (seeherefor more\\ndetails). We can usetorch.allclose()instead, which will give\\nus an approximate comparison taking into account a relative and\\nabsolute tolerance threshold:, give an example?',\n","   'context': ' Here, we’ve tried to check equality of the values of two deep learning\\nmodels with the==equality operator. However, this is not well-\\ndefined both due to the issue of that operator returning a tensor\\nand not a bool, but also because comparison of floating point values\\nshould use a margin of error (or epsilon) to account for the\\nnon-commutativity of floating point operations (seeherefor more\\ndetails). We can usetorch.allclose()instead, which will give\\nus an approximate comparison taking into account a relative and\\nabsolute tolerance threshold:',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': 'import torch\\nimport torch.fx\\nimport torchvision.models as models\\n\\ndef my_pass(inp: torch.nn.Module, tracer_class : type = fx.Tracer) -> torch.nn.Module:\\n    graph = tracer_class().trace(inp)\\n    # Transformation logic here\\n    # <...>\\n\\n    # Return new Module\\n    return fx.GraphModule(inp, graph)\\n\\nmy_module = models.resnet18()\\nmy_module_transformed = my_pass(my_module)\\n\\ninput_value = torch.randn(5, 3, 224, 224)\\n\\n# When this line is executed at runtime, we will be dropped into an\\n# interactive `pdb` prompt. We can use the `step` or `s` command to\\n# step into the execution of the next line\\nimport pdb; pdb.set_trace()\\n\\nmy_module_transformed(input_value)\\n',\n","   'Id': 779,\n","   'Question': 'How to use Invokepdbto step into the running program. Although the code that\\nrepresents theGraphis not in any source file, we can still step\\ninto it manually usingpdbwhen the forward pass is invoked., give an example?',\n","   'context': ' Invokepdbto step into the running program. Although the code that\\nrepresents theGraphis not in any source file, we can still step\\ninto it manually usingpdbwhen the forward pass is invoked.',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': '# Assume that `traced` is a GraphModule that has undergone some\\n# number of transforms\\n\\n# Copy this code for later\\nprint(traced)\\n# Print the code generated from symbolic tracing. This outputs:\\n\"\"\"\\ndef forward(self, y):\\n    x = self.x\\n    add_1 = x + y;  x = y = None\\n    return add_1\\n\"\"\"\\n\\n# Subclass the original Module\\nclass SubclassM(M):\\n    def __init__(self):\\n        super().__init__()\\n\\n    # Paste the generated `forward` function (the one we printed and\\n    # copied above) here\\n    def forward(self, y):\\n        x = self.x\\n        add_1 = x + y;  x = y = None\\n        return add_1\\n\\n# Create an instance of the original, untraced Module. Then, create an\\n# instance of the Module with the copied `forward` function. We can\\n# now compare the output of both the original and the traced version.\\npre_trace = M()\\npost_trace = SubclassM()\\n',\n","   'Id': 780,\n","   'Question': 'How to use If you’d like to run the same code multiple times, then it can be\\na bit tedious to step to the right code withpdb. In that case, one\\napproach is to simply copy-paste the generatedforwardpass into\\nyour code and examine it from there., give an example?',\n","   'context': ' If you’d like to run the same code multiple times, then it can be\\na bit tedious to step to the right code withpdb. In that case, one\\napproach is to simply copy-paste the generatedforwardpass into\\nyour code and examine it from there.',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': 'm = symbolic_trace(M())\\nm.to_folder(\"foo\", \"Bar\")\\nfrom foo import Bar\\ny = Bar()\\n',\n","   'Id': 781,\n","   'Question': 'How to use GraphModule.to_folder()is a method inGraphModulethat allows\\nyou to dump out the generated FX code to a folder. Although copying the\\nforward pass into the code often suffices as inPrint the Generated Code,\\nit may be easier to examine modules and parameters usingto_folder., give an example?',\n","   'context': ' GraphModule.to_folder()is a method inGraphModulethat allows\\nyou to dump out the generated FX code to a folder. Although copying the\\nforward pass into the code often suffices as inPrint the Generated Code,\\nit may be easier to examine modules and parameters usingto_folder.',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': '# Sample Module\\nclass M(torch.nn.Module):\\n    def forward(self, x, y):\\n        return x + y\\n\\n# Create an instance of `M`\\nm = M()\\n\\n# Symbolically trace an instance of `M` (returns a GraphModule). In\\n# this example, we\\'ll only be discussing how to inspect a\\n# GraphModule, so we aren\\'t showing any sample transforms for the\\n# sake of brevity.\\ntraced = symbolic_trace(m)\\n\\n# Print the code produced by tracing the module.\\nprint(traced)\\n# The generated `forward` function is:\\n\"\"\"\\ndef forward(self, x, y):\\n    add_1 = x + y;  x = y = None\\n    return add_1\\n\"\"\"\\n\\n# Print the internal Graph.\\nprint(traced.graph)\\n# This print-out returns:\\n\"\"\"\\ngraph(x, y):\\n    %add_1 : [#users=1] = call_function[target=<built-in function add>](args = (%x, %y), kwargs = {})\\n    return add_1\\n\"\"\"\\n\\n# Print a tabular representation of the internal Graph.\\ntraced.graph.print_tabular()\\n# This gives us:\\n\"\"\"\\nopcode         name    target                   args      kwargs\\n-------------  ------  -----------------------  --------  --------\\nplaceholder    x       x                        ()        {}\\nplaceholder    y       y                        ()        {}\\ncall_function  add_1   <built-in function add>  (x, y)    {}\\n\"\"\"\\n',\n","   'Id': 782,\n","   'Question': 'How to use Now that we’ve identified that a transformation is creating incorrect\\ncode, it’s time to debug the transformation itself. First, we’ll check\\ntheLimitations of Symbolic Tracingsection in the documentation.\\nOnce we verify that tracing is working as expected, the goal\\nbecomes figuring out what went wrong during ourGraphModuletransformation. There may be a quick answer inWriting Transformations, but, if not, there are several ways to\\nexamine our traced module:, give an example?',\n","   'context': ' Now that we’ve identified that a transformation is creating incorrect\\ncode, it’s time to debug the transformation itself. First, we’ll check\\ntheLimitations of Symbolic Tracingsection in the documentation.\\nOnce we verify that tracing is working as expected, the goal\\nbecomes figuring out what went wrong during ourGraphModuletransformation. There may be a quick answer inWriting Transformations, but, if not, there are several ways to\\nexamine our traced module:',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': '# Sample user-defined function\\ndef transform_graph(module: torch.nn.Module, tracer_class : type = fx.Tracer) -> torch.nn.Module:\\n    # Get the Graph from our traced Module\\n    g = tracer_class().trace(module)\\n\\n    \"\"\"\\n    Transformations on `g` go here\\n    \"\"\"\\n\\n    return fx.GraphModule(module, g)\\n\\n# Transform the Graph\\ntransformed = transform_graph(traced)\\n\\n# Print the new code after our transforms. Check to see if it was\\n# what we expected\\nprint(transformed)\\n',\n","   'Id': 783,\n","   'Question': 'How to use Using the utility functions above, we can compare our traced Module\\nbefore and after we’ve applied our transformations. Sometimes, a\\nsimple visual comparison is enough to trace down a bug. If it’s still\\nnot clear what’s going wrong, a debugger likepdbcan be a good\\nnext step.Going off of the example above, consider the following code:, give an example?',\n","   'context': ' Using the utility functions above, we can compare our traced Module\\nbefore and after we’ve applied our transformations. Sometimes, a\\nsimple visual comparison is enough to trace down a bug. If it’s still\\nnot clear what’s going wrong, a debugger likepdbcan be a good\\nnext step.Going off of the example above, consider the following code:',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': 'def func_to_trace(x):\\n    if x.sum() > 0:\\n        return torch.relu(x)\\n    else:\\n        return torch.neg(x)\\n\\ntraced = torch.fx.symbolic_trace(func_to_trace)\\n\"\"\"\\n  <...>\\n  File \"dyn.py\", line 6, in func_to_trace\\n    if x.sum() > 0:\\n  File \"pytorch/torch/fx/proxy.py\", line 155, in __bool__\\n    return self.tracer.to_bool(self)\\n  File \"pytorch/torch/fx/proxy.py\", line 85, in to_bool\\n    raise TraceError(\\'symbolically traced variables cannot be used as inputs to control flow\\')\\ntorch.fx.proxy.TraceError: symbolically traced variables cannot be used as inputs to control flow\\n\"\"\"\\n',\n","   'Id': 784,\n","   'Question': 'How to use The main limitation of symbolic tracing is it does not currently supportdynamic control flow. That is, loops orifstatements where the\\ncondition may depend on the input values of the program.For example, let’s examine the following program:, give an example?',\n","   'context': ' For example, let’s examine the following program:',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': 'import torch\\nimport torch.fx\\n\\nclass MyModule(torch.nn.Module):\\n    def __init__(self, do_activation : bool = False):\\n        super().__init__()\\n        self.do_activation = do_activation\\n        self.linear = torch.nn.Linear(512, 512)\\n\\n    def forward(self, x):\\n        x = self.linear(x)\\n        # This if-statement is so-called static control flow.\\n        # Its condition does not depend on any input values\\n        if self.do_activation:\\n            x = torch.relu(x)\\n        return x\\n\\nwithout_activation = MyModule(do_activation=False)\\nwith_activation = MyModule(do_activation=True)\\n\\ntraced_without_activation = torch.fx.symbolic_trace(without_activation)\\nprint(traced_without_activation.code)\\n\"\"\"\\ndef forward(self, x):\\n    linear_1 = self.linear(x);  x = None\\n    return linear_1\\n\"\"\"\\n\\ntraced_with_activation = torch.fx.symbolic_trace(with_activation)\\nprint(traced_with_activation.code)\\n\"\"\"\\nimport torch\\ndef forward(self, x):\\n    linear_1 = self.linear(x);  x = None\\n    relu_1 = torch.relu(linear_1);  linear_1 = None\\n    return relu_1\\n\"\"\"\\n',\n","   'Id': 785,\n","   'Question': 'How to use On the other hand, so-calledstatic control flowis supported. Static\\ncontrol flow is loops orifstatements whose value cannot change\\nacross invocations. Typically, in PyTorch programs, this control flow\\narises for code making decisions about a model’s architecture based on\\nhyper-parameters. As a concrete example:, give an example?',\n","   'context': ' On the other hand, so-calledstatic control flowis supported. Static\\ncontrol flow is loops orifstatements whose value cannot change\\nacross invocations. Typically, in PyTorch programs, this control flow\\narises for code making decisions about a model’s architecture based on\\nhyper-parameters. As a concrete example:',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': \"def f(x, flag):\\n    if flag: return x\\n    else: return x*2\\n\\nfx.symbolic_trace(f) # Fails!\\n\\nfx.symbolic_trace(f, concrete_args={'flag': True})\\n\",\n","   'Id': 786,\n","   'Question': 'How to use The if-statementifself.do_activationdoes not depend on any\\nfunction inputs, thus it is static.do_activationcan be considered\\nto be a hyper-parameter, and the traces of different instances ofMyModulewith different values for that parameter have different\\ncode. This is a valid pattern that is supported by symbolic tracing.Many instances of dynamic control flow are semantically static control\\nflow. These instances can be made to support symbolic tracing by\\nremoving the data dependencies on input values, for example by moving\\nvalues toModuleattributes or by binding concrete values to arguments\\nduring symbolic tracing:, give an example?',\n","   'context': ' The if-statementifself.do_activationdoes not depend on any\\nfunction inputs, thus it is static.do_activationcan be considered\\nto be a hyper-parameter, and the traces of different instances ofMyModulewith different values for that parameter have different\\ncode. This is a valid pattern that is supported by symbolic tracing.Many instances of dynamic control flow are semantically static control\\nflow. These instances can be made to support symbolic tracing by\\nremoving the data dependencies on input values, for example by moving\\nvalues toModuleattributes or by binding concrete values to arguments\\nduring symbolic tracing:',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': 'import torch\\nimport torch.fx\\nfrom math import sqrt\\n\\ndef normalize(x):\\n    \"\"\"\\n    Normalize `x` by the size of the batch dimension\\n    \"\"\"\\n    return x / sqrt(len(x))\\n\\n# It\\'s valid Python code\\nnormalize(torch.rand(3, 4))\\n\\ntraced = torch.fx.symbolic_trace(normalize)\\n\"\"\"\\n  <...>\\n  File \"sqrt.py\", line 9, in normalize\\n    return x / sqrt(len(x))\\n  File \"pytorch/torch/fx/proxy.py\", line 161, in __len__\\n    raise RuntimeError(\"\\'len\\' is not supported in symbolic tracing by default. If you want \"\\nRuntimeError: \\'len\\' is not supported in symbolic tracing by default. If you want this call to be recorded, please call torch.fx.wrap(\\'len\\') at module scope\\n\"\"\"\\n',\n","   'Id': 787,\n","   'Question': 'How to use FX uses__torch_function__as the mechanism by which it intercepts\\ncalls (see thetechnical\\noverviewfor more information about this). Some functions, such as builtin Python\\nfunctions or those in themathmodule, are not covered by__torch_function__, but we would still like to capture them in\\nsymbolic tracing. For example:, give an example?',\n","   'context': ' FX uses__torch_function__as the mechanism by which it intercepts\\ncalls (see thetechnical\\noverviewfor more information about this). Some functions, such as builtin Python\\nfunctions or those in themathmodule, are not covered by__torch_function__, but we would still like to capture them in\\nsymbolic tracing. For example:',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': 'torch.fx.wrap(\\'len\\')\\ntorch.fx.wrap(\\'sqrt\\')\\n\\ntraced = torch.fx.symbolic_trace(normalize)\\n\\nprint(traced.code)\\n\"\"\"\\nimport math\\ndef forward(self, x):\\n    len_1 = len(x)\\n    sqrt_1 = math.sqrt(len_1);  len_1 = None\\n    truediv = x / sqrt_1;  x = sqrt_1 = None\\n    return truediv\\n\"\"\"\\n',\n","   'Id': 788,\n","   'Question': 'How to use FX uses__torch_function__as the mechanism by which it intercepts\\ncalls (see thetechnical\\noverviewfor more information about this). Some functions, such as builtin Python\\nfunctions or those in themathmodule, are not covered by__torch_function__, but we would still like to capture them in\\nsymbolic tracing. For example:The error tells us that the built-in functionlenis not supported.\\nWe can make it so that functions like this are recorded in the trace as\\ndirect calls using thewrap()API:, give an example?',\n","   'context': ' The error tells us that the built-in functionlenis not supported.\\nWe can make it so that functions like this are recorded in the trace as\\ndirect calls using thewrap()API:',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': \"class MyCustomTracer(torch.fx.Tracer):\\n    # Inside here you can override various methods\\n    # to customize tracing. See the `Tracer` API\\n    # reference\\n    pass\\n\\n\\n# Let's use this custom tracer to trace through this module\\nclass MyModule(torch.nn.Module):\\n    def forward(self, x):\\n        return torch.relu(x) + torch.ones(3, 4)\\n\\nmod = MyModule()\\n\\ntraced_graph = MyCustomTracer().trace(mod)\\n# trace() returns a Graph. Let's wrap it up in a\\n# GraphModule to make it runnable\\ntraced = torch.fx.GraphModule(mod, traced_graph)\\n\",\n","   'Id': 789,\n","   'Question': 'How to use TheTracerclass is the class that underlies the\\nimplementation ofsymbolic_trace. The behavior of tracing can be\\ncustomized by subclassing Tracer, like so:, give an example?',\n","   'context': ' TheTracerclass is the class that underlies the\\nimplementation ofsymbolic_trace. The behavior of tracing can be\\ncustomized by subclassing Tracer, like so:',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': 'class MySpecialSubmodule(torch.nn.Module):\\n    def forward(self, x):\\n        return torch.neg(x)\\n\\nclass MyModule(torch.nn.Module):\\n    def __init__(self):\\n        super().__init__()\\n        self.linear = torch.nn.Linear(3, 4)\\n        self.submod = MySpecialSubmodule()\\n\\n    def forward(self, x):\\n        return self.submod(self.linear(x))\\n\\ntraced = torch.fx.symbolic_trace(MyModule())\\nprint(traced.code)\\n# `linear` is preserved as a call, yet `submod` is traced though.\\n# This is because the default set of \"Leaf Modules\" includes all\\n# standard `torch.nn` modules.\\n\"\"\"\\nimport torch\\ndef forward(self, x):\\n    linear_1 = self.linear(x);  x = None\\n    neg_1 = torch.neg(linear_1);  linear_1 = None\\n    return neg_1\\n\"\"\"\\n',\n","   'Id': 790,\n","   'Question': 'How to use Leaf Modules are the modules that appear as calls in the symbolic trace\\nrather than being traced through. The default set of leaf modules is the\\nset of standardtorch.nnmodule instances. For example:, give an example?',\n","   'context': ' Leaf Modules are the modules that appear as calls in the symbolic trace\\nrather than being traced through. The default set of leaf modules is the\\nset of standardtorch.nnmodule instances. For example:',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': '@torch.fx.wrap\\ndef torch_randn(x, shape):\\n    return torch.randn(shape)\\n\\ndef f(x):\\n    return x + torch_randn(x, 5)\\nfx.symbolic_trace(f)\\n',\n","   'Id': 791,\n","   'Question': 'How to use Miscellanea, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': 'def f(a, b):\\n    if b == True:\\n        return a\\n    else:\\n        return a*2\\n',\n","   'Id': 792,\n","   'Question': 'How to use torch.fx.symbolic_trace, give an example?',\n","   'context': ' concrete_argsallows you to partially specialize your function, whether it’s to remove control flow or data structures.For example:',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': \"def f(x):\\n    out = 0\\n    for v in x.values():\\n        out += v\\n    return out\\nf = fx.symbolic_trace(f, concrete_args={'x': {'a': fx.PH, 'b': fx.PH, 'c': fx.PH}})\\nassert f({'a': 1, 'b': 2, 'c': 4}) == 7\\n\",\n","   'Id': 793,\n","   'Question': 'How  Note that although you can still pass in different values ofb, they will be ignored.We can also useconcrete_argsto eliminate data-structure handling from\\nour function. This will use pytrees to flatten your input. To avoid\\noverspecializing, pass infx.PHfor values that shouldn’t be\\nspecialized. For example:, give an example?',\n","   'context': ' Note that although you can still pass in different values ofb, they will be ignored.We can also useconcrete_argsto eliminate data-structure handling from\\nour function. This will use pytrees to flatten your input. To avoid\\noverspecializing, pass infx.PHfor values that shouldn’t be\\nspecialized. For example:',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': \"# foo/bar/baz.py\\ndef my_custom_function(x, y):\\n    return x * x + y * y\\n\\ntorch.fx.wrap('my_custom_function')\\n\\ndef fn_to_be_traced(x, y):\\n    # When symbolic tracing, the below call to my_custom_function will be inserted into\\n    # the graph rather than tracing it.\\n    return my_custom_function(x, y)\\n\",\n","   'Id': 794,\n","   'Question': 'How to use torch.fx.wrap, give an example?',\n","   'context': ' This function can be called at module-level scope to register fn_or_name as a “leaf function”.\\nA “leaf function” will be preserved as a CallFunction node in the FX trace instead of being\\ntraced through:',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': '# foo/bar/baz.py\\n@torch.fx.wrap\\ndef my_custom_function(x, y):\\n    return x * x + y * y\\n',\n","   'Id': 795,\n","   'Question': 'How  This function can also equivalently be used as a decorator:, give an example?',\n","   'context': ' This function can also equivalently be used as a decorator:',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': 'import torch\\nimport torch.fx\\n\\nclass MyModule(torch.nn.Module):\\n    def __init__(self):\\n        super().__init__()\\n        self.param = torch.nn.Parameter(torch.rand(3, 4))\\n        self.linear = torch.nn.Linear(4, 5)\\n\\n    def forward(self, x):\\n        return torch.topk(torch.sum(self.linear(x + self.linear.weight).relu(), dim=-1), 3)\\n\\nm = MyModule()\\ngm = torch.fx.symbolic_trace(m)\\n',\n","   'Id': 796,\n","   'Question': 'How to use For example, the following code, give an example?',\n","   'context': ' For example, the following code',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': 'print(gm.graph)\\n',\n","   'Id': 797,\n","   'Question': 'How to use Will produce the following Graph:, give an example?',\n","   'context': ' For example, the following codeWill produce the following Graph:',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': 'graph(x):\\n    %linear_weight : [#users=1] = self.linear.weight\\n    %add_1 : [#users=1] = call_function[target=operator.add](args = (%x, %linear_weight), kwargs = {})\\n    %linear_1 : [#users=1] = call_module[target=linear](args = (%add_1,), kwargs = {})\\n    %relu_1 : [#users=1] = call_method[target=relu](args = (%linear_1,), kwargs = {})\\n    %sum_1 : [#users=1] = call_function[target=torch.sum](args = (%relu_1,), kwargs = {dim: -1})\\n    %topk_1 : [#users=1] = call_function[target=torch.topk](args = (%sum_1, 3), kwargs = {})\\n    return topk_1\\n',\n","   'Id': 798,\n","   'Question': 'How to use API Reference, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': 'def forward(self, x):\\n    a = x + 1\\n    return x + self.attr_1\\n',\n","   'Id': 799,\n","   'Question': 'How to use torch.fx.Graph.eliminate_dead_code, give an example?',\n","   'context': ' Before dead code is eliminated,afroma = x + 1below has no users\\nand thus can be eliminated from the graph without having an effect.',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': 'def forward(self, x):\\n    return x + self.attr_1\\n',\n","   'Id': 800,\n","   'Question': 'How  Before dead code is eliminated,afroma = x + 1below has no users\\nand thus can be eliminated from the graph without having an effect.After dead code is eliminated,a = x + 1has been removed, and the rest\\nofforwardremains., give an example?',\n","   'context': ' Before dead code is eliminated,afroma = x + 1below has no users\\nand thus can be eliminated from the graph without having an effect.After dead code is eliminated,a = x + 1has been removed, and the rest\\nofforwardremains.',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': 'with g.inserting_after(n):\\n    ... # inserting after node n\\n... # insert point restored to what it was previously\\ng.inserting_after(n) #  set the insert point permanently\\n',\n","   'Id': 801,\n","   'Question': 'How to use torch.fx.Graph.inserting_after, give an example?',\n","   'context': ' Set the point at which create_node and companion methods will insert into the graph.\\nWhen used within a ‘with’ statement, this will temporary set the insert point and\\nthen restore it when the with statement exits:',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': 'with g.inserting_before(n):\\n    ... # inserting before node n\\n... # insert point restored to what it was previously\\ng.inserting_before(n) #  set the insert point permanently\\n',\n","   'Id': 802,\n","   'Question': 'How to use torch.fx.Graph.inserting_before, give an example?',\n","   'context': ' Set the point at which create_node and companion methods will insert into the graph.\\nWhen used within a ‘with’ statement, this will temporary set the insert point and\\nthen restore it when the with statement exits:',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': '# Copying all the nodes in `g` into `new_graph`\\ng : torch.fx.Graph = ...\\nnew_graph = torch.fx.graph()\\nvalue_remap = {}\\nfor node in g.nodes:\\n    value_remap[node] = new_graph.node_copy(node, lambda n : value_remap[n])\\n',\n","   'Id': 803,\n","   'Question': 'How to use torch.fx.Graph.node_copy, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': 'Before: p -> self\\n        bx -> x -> ax\\nAfter:  p -> x -> self\\n        bx -> ax\\n',\n","   'Id': 804,\n","   'Question': 'How to use torch.fx.Node.prepend, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': 'run()\\n    +-- run_node\\n        +-- placeholder()\\n        +-- get_attr()\\n        +-- call_function()\\n        +-- call_method()\\n        +-- call_module()\\n        +-- output()\\n',\n","   'Id': 805,\n","   'Question': 'How to use Methods in the Interpreter class can be overridden to customize\\nthe behavior of execution. The map of overrideable methods\\nin terms of call hierarchy:, give an example?',\n","   'context': ' Methods in the Interpreter class can be overridden to customize\\nthe behavior of execution. The map of overrideable methods\\nin terms of call hierarchy:',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': \"class NegSigmSwapInterpreter(Interpreter):\\n    def call_function(self, target : Target,\\n                      args : Tuple, kwargs : Dict) -> Any:\\n        if target == torch.sigmoid:\\n            return torch.neg(*args, **kwargs)\\n        return super().call_function(n)\\n\\n    def call_method(self, target : Target,\\n                    args : Tuple, kwargs : Dict) -> Any:\\n        if target == 'neg':\\n            call_self, *args_tail = args\\n            return call_self.sigmoid(*args_tail, **kwargs)\\n        return super().call_method(n)\\n\\ndef fn(x):\\n    return torch.sigmoid(x).neg()\\n\\ngm = torch.fx.symbolic_trace(fn)\\ninput = torch.randn(3, 4)\\nresult = NegSigmSwapInterpreter(gm).run(input)\\ntorch.testing.assert_allclose(result, torch.neg(input).sigmoid())\\n\",\n","   'Id': 806,\n","   'Question': 'How to use Suppose we want to swap all instances oftorch.negwithtorch.sigmoidand vice versa (including theirTensormethod equivalents). We could subclass Interpreter like so:, give an example?',\n","   'context': ' Suppose we want to swap all instances oftorch.negwithtorch.sigmoidand vice versa (including theirTensormethod equivalents). We could subclass Interpreter like so:',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': \"class NegSigmSwapXformer(Transformer):\\n    def call_function(self, target : 'Target', args : Tuple[Argument, ...], kwargs : Dict[str, Any]) -> Any:\\n        if target == torch.sigmoid:\\n            return torch.neg(*args, **kwargs)\\n        return super().call_function(n)\\n\\n    def call_method(self, target : 'Target', args : Tuple[Argument, ...], kwargs : Dict[str, Any]) -> Any:\\n        if target == 'neg':\\n            call_self, *args_tail = args\\n            return call_self.sigmoid(*args_tail, **kwargs)\\n        return super().call_method(n)\\n\\ndef fn(x):\\n    return torch.sigmoid(x).neg()\\n\\ngm = torch.fx.symbolic_trace(fn)\\n\\ntransformed : torch.nn.Module = NegSigmSwapXformer(gm).transform()\\ninput = torch.randn(3, 4)\\ntorch.testing.assert_allclose(transformed(input), torch.neg(input).sigmoid())\\n\",\n","   'Id': 807,\n","   'Question': 'How to use Suppose we want to swap all instances oftorch.negwithtorch.sigmoidand vice versa (including theirTensormethod equivalents). We could subclassTransformerlike so:, give an example?',\n","   'context': ' Suppose we want to swap all instances oftorch.negwithtorch.sigmoidand vice versa (including theirTensormethod equivalents). We could subclassTransformerlike so:',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': 'class Match(NamedTuple):\\n    # Node from which the match was found\\n    anchor: Node\\n    # Maps nodes in the pattern subgraph to nodes in the larger graph\\n    nodes_map: Dict[Node, Node]\\n',\n","   'Id': 808,\n","   'Question': 'How to use torch.fx.replace_pattern, give an example?',\n","   'context': ' A list ofMatchobjects representing the places\\nin the original graph thatpatternwas matched to. The list\\nis empty if there are no matches.Matchis defined as:',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': 'import torch\\nfrom torch.fx import symbolic_trace, subgraph_rewriter\\n\\nclass M(torch.nn.Module):\\n    def __init__(self):\\n        super().__init__()\\n\\n    def forward(self, x, w1, w2):\\n        m1 = torch.cat([w1, w2]).sum()\\n        m2 = torch.cat([w1, w2]).sum()\\n        return x + torch.max(m1) + torch.max(m2)\\n\\ndef pattern(w1, w2):\\n    return torch.cat([w1, w2]).sum()\\n\\ndef replacement(w1, w2):\\n    return torch.stack([w1, w2])\\n\\ntraced_module = symbolic_trace(M())\\n\\nsubgraph_rewriter.replace_pattern(traced_module, pattern, replacement)\\n',\n","   'Id': 809,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': 'def pattern(x, y):\\n    return torch.neg(x) + torch.relu(y)\\n',\n","   'Id': 810,\n","   'Question': 'How  When the pattern is matched, it will be removed from the larger\\nfunction and replaced byreplacement. If there are multiple\\nmatches forpatternin the larger function, each non-overlapping\\nmatch will be replaced. In the case of a match overlap, the first\\nfound match in the set of overlapping matches will be replaced.\\n(“First” here being defined as the first in a topological ordering\\nof the Nodes’ use-def relationships. In most cases, the first Node\\nis the parameter that appears directly afterself, while the\\nlast Node is whatever the function returns.)One important thing to note is that the parameters of thepatternCallable must be used in the Callable itself,\\nand the parameters of thereplacementCallable must match\\nthe pattern. The first rule is why, in the above code block, theforwardfunction has parametersx,w1,w2, but thepatternfunction only has parametersw1,w2.patterndoesn’t usex, so it shouldn’t specifyxas a parameter.\\nAs an example of the second rule, consider replacing, give an example?',\n","   'context': ' When the pattern is matched, it will be removed from the larger\\nfunction and replaced byreplacement. If there are multiple\\nmatches forpatternin the larger function, each non-overlapping\\nmatch will be replaced. In the case of a match overlap, the first\\nfound match in the set of overlapping matches will be replaced.\\n(“First” here being defined as the first in a topological ordering\\nof the Nodes’ use-def relationships. In most cases, the first Node\\nis the parameter that appears directly afterself, while the\\nlast Node is whatever the function returns.)One important thing to note is that the parameters of thepatternCallable must be used in the Callable itself,\\nand the parameters of thereplacementCallable must match\\nthe pattern. The first rule is why, in the above code block, theforwardfunction has parametersx,w1,w2, but thepatternfunction only has parametersw1,w2.patterndoesn’t usex, so it shouldn’t specifyxas a parameter.\\nAs an example of the second rule, consider replacing',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': 'def replacement(x, y):\\n    return torch.relu(x)\\n',\n","   'Id': 811,\n","   'Question': 'How  One important thing to note is that the parameters of thepatternCallable must be used in the Callable itself,\\nand the parameters of thereplacementCallable must match\\nthe pattern. The first rule is why, in the above code block, theforwardfunction has parametersx,w1,w2, but thepatternfunction only has parametersw1,w2.patterndoesn’t usex, so it shouldn’t specifyxas a parameter.\\nAs an example of the second rule, consider replacingwith, give an example?',\n","   'context': ' One important thing to note is that the parameters of thepatternCallable must be used in the Callable itself,\\nand the parameters of thereplacementCallable must match\\nthe pattern. The first rule is why, in the above code block, theforwardfunction has parametersx,w1,w2, but thepatternfunction only has parametersw1,w2.patterndoesn’t usex, so it shouldn’t specifyxas a parameter.\\nAs an example of the second rule, consider replacingwith',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': 'def forward(self, x, w1, w2):\\n    stack_1 = torch.stack([w1, w2])\\n    sum_1 = stack_1.sum()\\n    stack_2 = torch.stack([w1, w2])\\n    sum_2 = stack_2.sum()\\n    max_1 = torch.max(sum_1)\\n    add_1 = x + max_1\\n    max_2 = torch.max(sum_2)\\n    add_2 = add_1 + max_2\\n    return add_2\\n',\n","   'Id': 812,\n","   'Question': 'How  In this case,replacementneeds the same number of parameters\\naspattern(bothxandy), even though the parameteryisn’t used inreplacement.After callingsubgraph_rewriter.replace_pattern, the generated\\nPython code looks like this:, give an example?',\n","   'context': ' In this case,replacementneeds the same number of parameters\\naspattern(bothxandy), even though the parameteryisn’t used inreplacement.After callingsubgraph_rewriter.replace_pattern, the generated\\nPython code looks like this:',\n","   'source': 'https://pytorch.org/docs/stable/fx.html'},\n","  {'Answer': '>>> a = torch.arange(10).reshape(5,2)\\n>>> a\\ntensor([[0, 1],\\n        [2, 3],\\n        [4, 5],\\n        [6, 7],\\n        [8, 9]])\\n>>> torch.split(a, 2)\\n(tensor([[0, 1],\\n         [2, 3]]),\\n tensor([[4, 5],\\n         [6, 7]]),\\n tensor([[8, 9]]))\\n>>> torch.split(a, [1,4])\\n(tensor([[0, 1]]),\\n tensor([[2, 3],\\n         [4, 5],\\n         [6, 7],\\n         [8, 9]]))\\n',\n","   'Id': 813,\n","   'Question': 'How to use torch.split, give an example?',\n","   'context': ' Ifsplit_size_or_sectionsis a list, thentensorwill be split\\nintolen(split_size_or_sections)chunks with sizes indimaccording\\ntosplit_size_or_sections.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.split.html#torch.split'},\n","  {'Answer': '>>> a = torch.tensor([0.7, -1.2, 0., 2.3])\\n>>> a\\ntensor([ 0.7000, -1.2000,  0.0000,  2.3000])\\n>>> torch.sign(a)\\ntensor([ 1., -1.,  0.,  1.])\\n',\n","   'Id': 814,\n","   'Question': 'How to use torch.sign, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.sign.html#torch.sign'},\n","  {'Answer': \">>> from setuptools import setup\\n>>> from torch.utils.cpp_extension import BuildExtension, CppExtension\\n>>> setup(\\n        name='extension',\\n        ext_modules=[\\n            CppExtension(\\n                name='extension',\\n                sources=['extension.cpp'],\\n                extra_compile_args=['-g']),\\n        ],\\n        cmdclass={\\n            'build_ext': BuildExtension\\n        })\\n\",\n","   'Id': 815,\n","   'Question': 'How to use torch.utils.cpp_extension.CppExtension, give an example?',\n","   'context': ' All arguments are forwarded to thesetuptools.Extensionconstructor.',\n","   'source': 'https://pytorch.org/docs/stable/cpp_extension.html'},\n","  {'Answer': \">>> from setuptools import setup\\n>>> from torch.utils.cpp_extension import BuildExtension, CUDAExtension\\n>>> setup(\\n        name='cuda_extension',\\n        ext_modules=[\\n            CUDAExtension(\\n                    name='cuda_extension',\\n                    sources=['extension.cpp', 'extension_kernel.cu'],\\n                    extra_compile_args={'cxx': ['-g'],\\n                                        'nvcc': ['-O2']})\\n        ],\\n        cmdclass={\\n            'build_ext': BuildExtension\\n        })\\n\",\n","   'Id': 816,\n","   'Question': 'How to use torch.utils.cpp_extension.CUDAExtension, give an example?',\n","   'context': ' All arguments are forwarded to thesetuptools.Extensionconstructor.',\n","   'source': 'https://pytorch.org/docs/stable/cpp_extension.html'},\n","  {'Answer': \">>> from torch.utils.cpp_extension import load\\n>>> module = load(\\n        name='extension',\\n        sources=['extension.cpp', 'extension_kernel.cu'],\\n        extra_cflags=['-O2'],\\n        verbose=True)\\n\",\n","   'Id': 817,\n","   'Question': 'How to use torch.utils.cpp_extension.load, give an example?',\n","   'context': ' CUDA support with mixed compilation is provided. Simply pass CUDA source\\nfiles (.cuor.cuh) along with other sources. Such files will be\\ndetected and compiled with nvcc rather than the C++ compiler. This includes\\npassing the CUDA lib64 directory as a library directory, and linkingcudart. You can pass additional flags to nvcc viaextra_cuda_cflags, just like withextra_cflagsfor C++. Various\\nheuristics for finding the CUDA install directory are used, which usually\\nwork fine. If not, setting theCUDA_HOMEenvironment variable is the\\nsafest option.',\n","   'source': 'https://pytorch.org/docs/stable/cpp_extension.html'},\n","  {'Answer': \">>> from torch.utils.cpp_extension import load_inline\\n>>> source = \\\\'\\\\'\\\\'\\nat::Tensor sin_add(at::Tensor x, at::Tensor y) {\\n  return x.sin() + y.sin();\\n}\\n\\\\'\\\\'\\\\'\\n>>> module = load_inline(name='inline_extension',\\n                         cpp_sources=[source],\\n                         functions=['sin_add'])\\n\",\n","   'Id': 818,\n","   'Question': 'How to use torch.utils.cpp_extension.load_inline, give an example?',\n","   'context': ' Seeload()for a description of arguments omitted below.',\n","   'source': 'https://pytorch.org/docs/stable/cpp_extension.html'},\n","  {'Answer': '>>> x = torch.zeros(1, requires_grad=True)\\n>>> with torch.no_grad():\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> is_train = False\\n>>> with torch.set_grad_enabled(is_train):\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> torch.set_grad_enabled(True)  # this can also be used as a function\\n>>> y = x * 2\\n>>> y.requires_grad\\nTrue\\n\\n>>> torch.set_grad_enabled(False)\\n>>> y = x * 2\\n>>> y.requires_grad\\nFalse\\n',\n","   'Id': 819,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/torch.html#other-operations'},\n","  {'Answer': '>>> x = torch.tensor([1, 2, 3])\\n>>> x.repeat_interleave(2)\\ntensor([1, 1, 2, 2, 3, 3])\\n>>> y = torch.tensor([[1, 2], [3, 4]])\\n>>> torch.repeat_interleave(y, 2)\\ntensor([1, 1, 2, 2, 3, 3, 4, 4])\\n>>> torch.repeat_interleave(y, 3, dim=1)\\ntensor([[1, 1, 1, 2, 2, 2],\\n        [3, 3, 3, 4, 4, 4]])\\n>>> torch.repeat_interleave(y, torch.tensor([1, 2]), dim=0)\\ntensor([[1, 2],\\n        [3, 4],\\n        [3, 4]])\\n',\n","   'Id': 820,\n","   'Question': 'How to use torch.repeat_interleave, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave'},\n","  {'Answer': '>>> a = torch.randn(4)\\n>>> a\\ntensor([ 0.0202,  1.0985,  1.3506, -0.6056])\\n>>> torch.add(a, 20)\\ntensor([ 20.0202,  21.0985,  21.3506,  19.3944])\\n',\n","   'Id': 821,\n","   'Question': 'How to use torch.add, give an example?',\n","   'context': ' Ifinputis of type FloatTensor or DoubleTensor,othermust be\\na real number, otherwise it should be an integer.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.add.html#torch.add'},\n","  {'Answer': '>>> a = torch.randn(4)\\n>>> a\\ntensor([-0.9732, -0.3497,  0.6245,  0.4022])\\n>>> b = torch.randn(4, 1)\\n>>> b\\ntensor([[ 0.3743],\\n        [-1.7724],\\n        [-0.5811],\\n        [-0.8017]])\\n>>> torch.add(a, b, alpha=10)\\ntensor([[  2.7695,   3.3930,   4.3672,   4.1450],\\n        [-18.6971, -18.0736, -17.0994, -17.3216],\\n        [ -6.7845,  -6.1610,  -5.1868,  -5.4090],\\n        [ -8.9902,  -8.3667,  -7.3925,  -7.6147]])\\n',\n","   'Id': 822,\n","   'Question': 'How  Ifotheris of type FloatTensor or DoubleTensor,alphamust be\\na real number, otherwise it should be an integer., give an example?',\n","   'context': ' Ifotheris of type FloatTensor or DoubleTensor,alphamust be\\na real number, otherwise it should be an integer.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.add.html#torch.add'},\n","  {'Answer': '>>> mat1 = torch.eye(2)\\n>>> mat2 = torch.ones(2, 2)\\n>>> torch.kron(mat1, mat2)\\ntensor([[1., 1., 0., 0.],\\n        [1., 1., 0., 0.],\\n        [0., 0., 1., 1.],\\n        [0., 0., 1., 1.]])\\n\\n>>> mat1 = torch.eye(2)\\n>>> mat2 = torch.arange(1, 5).reshape(2, 2)\\n>>> torch.kron(mat1, mat2)\\ntensor([[1., 2., 0., 0.],\\n        [3., 4., 0., 0.],\\n        [0., 0., 1., 2.],\\n        [0., 0., 3., 4.]])\\n',\n","   'Id': 823,\n","   'Question': 'How to use torch.kron, give an example?',\n","   'context': ' Supports real-valued and complex-valued inputs.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.kron.html#torch.kron'},\n","  {'Answer': '>>> import numpy as np\\n>>> abs = torch.tensor([1, 2], dtype=torch.float64)\\n>>> angle = torch.tensor([np.pi / 2, 5 * np.pi / 4], dtype=torch.float64)\\n>>> z = torch.polar(abs, angle)\\n>>> z\\ntensor([(0.0000+1.0000j), (-1.4142-1.4142j)], dtype=torch.complex128)\\n',\n","   'Id': 824,\n","   'Question': 'How to use torch.polar, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.polar.html#torch.polar'},\n","  {'Answer': \">>> torch.use_deterministic_algorithms(True)\\n\\n# Forward mode nondeterministic error\\n>>> torch.randn(10).index_copy(0, torch.tensor([0]), torch.randn(1))\\n...\\nRuntimeError: index_copy does not have a deterministic implementation...\\n\\n# Backward mode nondeterministic error\\n>>> torch.randn(10, requires_grad=True, device='cuda').index_select(0, torch.tensor([0], device='cuda')).backward()\\n...\\nRuntimeError: index_add_cuda_ does not have a deterministic implementation...\\n\",\n","   'Id': 825,\n","   'Question': 'How to use torch.use_deterministic_algorithms, give an example?',\n","   'context': ' Note that deterministic operations tend to have worse performance than\\nnondeterministic operations.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms'},\n","  {'Answer': '>>> t = torch.randn(1, 3)\\n>>> t1 = torch.randn(3, 1)\\n>>> t2 = torch.randn(1, 3)\\n>>> torch.addcmul(t, t1, t2, value=0.1)\\ntensor([[-0.8635, -0.6391,  1.6174],\\n        [-0.7617, -0.5879,  1.7388],\\n        [-0.8353, -0.6249,  1.6511]])\\n',\n","   'Id': 826,\n","   'Question': 'How to use torch.addcmul, give an example?',\n","   'context': ' For inputs of typeFloatTensororDoubleTensor,valuemust be\\na real number, otherwise an integer.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.addcmul.html#torch.addcmul'},\n","  {'Answer': \">>> a = torch.tensor([9.7, float('nan'), 3.1, float('nan')])\\n>>> b = torch.tensor([-2.2, 0.5, float('nan'), float('nan')])\\n>>> torch.fmax(a, b)\\ntensor([9.7000, 0.5000, 3.1000,    nan])\\n\",\n","   'Id': 827,\n","   'Question': 'How to use torch.fmax, give an example?',\n","   'context': ' Supportsbroadcasting to a common shape,type promotion, and integer and floating-point inputs.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.fmax.html#torch.fmax'},\n","  {'Answer': '>>> x = torch.arange(8)\\n>>> torch.tensor_split(x, 3)\\n(tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7]))\\n\\n>>> x = torch.arange(7)\\n>>> torch.tensor_split(x, 3)\\n(tensor([0, 1, 2]), tensor([3, 4]), tensor([5, 6]))\\n>>> torch.tensor_split(x, (1, 6))\\n(tensor([0]), tensor([1, 2, 3, 4, 5]), tensor([6]))\\n\\n>>> x = torch.arange(14).reshape(2, 7)\\n>>> x\\ntensor([[ 0,  1,  2,  3,  4,  5,  6],\\n        [ 7,  8,  9, 10, 11, 12, 13]])\\n>>> torch.tensor_split(x, 3, dim=1)\\n(tensor([[0, 1, 2],\\n        [7, 8, 9]]),\\n tensor([[ 3,  4],\\n        [10, 11]]),\\n tensor([[ 5,  6],\\n        [12, 13]]))\\n>>> torch.tensor_split(x, (1, 6), dim=1)\\n(tensor([[0],\\n        [7]]),\\n tensor([[ 1,  2,  3,  4,  5],\\n        [ 8,  9, 10, 11, 12]]),\\n tensor([[ 6],\\n        [13]]))\\n',\n","   'Id': 828,\n","   'Question': 'How to use torch.tensor_split, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split'},\n","  {'Answer': '>>> a = torch.tensor([[-0.8166, -1.3802, -0.3560]])\\n>>> torch.std(a, unbiased=False)\\ntensor(0.4188)\\n',\n","   'Id': 829,\n","   'Question': 'How to use torch.std, give an example?',\n","   'context': ' IfunbiasedisTrue, Bessel’s correction will be used.\\nOtherwise, the sample deviation is calculated, without any correction.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.std.html#torch.std'},\n","  {'Answer': '>>> a = torch.randn(4)\\n>>> a\\ntensor([ 1.4309,  1.2706, -0.8562,  0.9796])\\n>>> torch.cos(a)\\ntensor([ 0.1395,  0.2957,  0.6553,  0.5574])\\n',\n","   'Id': 830,\n","   'Question': 'How to use torch.cos, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.cos.html#torch.cos'},\n","  {'Answer': '# original model\\n# all tensors and computations are in floating point\\nprevious_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32\\n                 /\\nlinear_weight_fp32\\n\\n# dynamically quantized model\\n# linear and LSTM weights are in int8\\nprevious_layer_fp32 -- linear_int8_w_fp32_inp -- activation_fp32 -- next_layer_fp32\\n                     /\\n   linear_weight_int8\\n',\n","   'Id': 831,\n","   'Question': 'How to use This is the simplest to apply form of quantization where the weights are\\nquantized ahead of time but the activations are dynamically quantized\\nduring inference. This is used for situations where the model execution time\\nis dominated by loading weights from memory rather than computing the matrix\\nmultiplications. This is true for for LSTM and Transformer type models with\\nsmall batch size.Diagram:, give an example?',\n","   'context': ' Diagram:',\n","   'source': 'https://pytorch.org/docs/stable/quantization.html'},\n","  {'Answer': 'import torch\\n\\n# define a floating point model\\nclass M(torch.nn.Module):\\n    def __init__(self):\\n        super(M, self).__init__()\\n        self.fc = torch.nn.Linear(4, 4)\\n\\n    def forward(self, x):\\n        x = self.fc(x)\\n        return x\\n\\n# create a model instance\\nmodel_fp32 = M()\\n# create a quantized model instance\\nmodel_int8 = torch.quantization.quantize_dynamic(\\n    model_fp32,  # the original model\\n    {torch.nn.Linear},  # a set of layers to dynamically quantize\\n    dtype=torch.qint8)  # the target dtype for quantized weights\\n\\n# run the model\\ninput_fp32 = torch.randn(4, 4, 4, 4)\\nres = model_int8(input_fp32)\\n',\n","   'Id': 832,\n","   'Question': 'How to use Diagram:API example:, give an example?',\n","   'context': ' Diagram:API example:',\n","   'source': 'https://pytorch.org/docs/stable/quantization.html'},\n","  {'Answer': '# original model\\n# all tensors and computations are in floating point\\nprevious_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32\\n                    /\\n    linear_weight_fp32\\n\\n# statically quantized model\\n# weights and activations are in int8\\nprevious_layer_int8 -- linear_with_activation_int8 -- next_layer_int8\\n                    /\\n  linear_weight_int8\\n',\n","   'Id': 833,\n","   'Question': 'How to use Static quantization quantizes the weights and activations of the model.  It\\nfuses activations into preceding layers where possible.  It requires\\ncalibration with a representative dataset to determine optimal quantization\\nparameters for activations. Post Training Quantization is typically used when\\nboth memory bandwidth and compute savings are important with CNNs being a\\ntypical use case.  Static quantization is also known as Post Training\\nQuantization or PTQ.Diagram:, give an example?',\n","   'context': ' Diagram:',\n","   'source': 'https://pytorch.org/docs/stable/quantization.html'},\n","  {'Answer': \"import torch\\n\\n# define a floating point model where some layers could be statically quantized\\nclass M(torch.nn.Module):\\n    def __init__(self):\\n        super(M, self).__init__()\\n        # QuantStub converts tensors from floating point to quantized\\n        self.quant = torch.quantization.QuantStub()\\n        self.conv = torch.nn.Conv2d(1, 1, 1)\\n        self.relu = torch.nn.ReLU()\\n        # DeQuantStub converts tensors from quantized to floating point\\n        self.dequant = torch.quantization.DeQuantStub()\\n\\n    def forward(self, x):\\n        # manually specify where tensors will be converted from floating\\n        # point to quantized in the quantized model\\n        x = self.quant(x)\\n        x = self.conv(x)\\n        x = self.relu(x)\\n        # manually specify where tensors will be converted from quantized\\n        # to floating point in the quantized model\\n        x = self.dequant(x)\\n        return x\\n\\n# create a model instance\\nmodel_fp32 = M()\\n\\n# model must be set to eval mode for static quantization logic to work\\nmodel_fp32.eval()\\n\\n# attach a global qconfig, which contains information about what kind\\n# of observers to attach. Use 'fbgemm' for server inference and\\n# 'qnnpack' for mobile inference. Other quantization configurations such\\n# as selecting symmetric or assymetric quantization and MinMax or L2Norm\\n# calibration techniques can be specified here.\\nmodel_fp32.qconfig = torch.quantization.get_default_qconfig('fbgemm')\\n\\n# Fuse the activations to preceding layers, where applicable.\\n# This needs to be done manually depending on the model architecture.\\n# Common fusions include `conv + relu` and `conv + batchnorm + relu`\\nmodel_fp32_fused = torch.quantization.fuse_modules(model_fp32, [['conv', 'relu']])\\n\\n# Prepare the model for static quantization. This inserts observers in\\n# the model that will observe activation tensors during calibration.\\nmodel_fp32_prepared = torch.quantization.prepare(model_fp32_fused)\\n\\n# calibrate the prepared model to determine quantization parameters for activations\\n# in a real world setting, the calibration would be done with a representative dataset\\ninput_fp32 = torch.randn(4, 1, 4, 4)\\nmodel_fp32_prepared(input_fp32)\\n\\n# Convert the observed model to a quantized model. This does several things:\\n# quantizes the weights, computes and stores the scale and bias value to be\\n# used with each activation tensor, and replaces key operators with quantized\\n# implementations.\\nmodel_int8 = torch.quantization.convert(model_fp32_prepared)\\n\\n# run the model, relevant calculations will happen in int8\\nres = model_int8(input_fp32)\\n\",\n","   'Id': 834,\n","   'Question': 'How to use Diagram:, give an example?',\n","   'context': ' Diagram:',\n","   'source': 'https://pytorch.org/docs/stable/quantization.html'},\n","  {'Answer': '# original model\\n# all tensors and computations are in floating point\\nprevious_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32\\n                      /\\n    linear_weight_fp32\\n\\n# model with fake_quants for modeling quantization numerics during training\\nprevious_layer_fp32 -- fq -- linear_fp32 -- activation_fp32 -- fq -- next_layer_fp32\\n                           /\\n   linear_weight_fp32 -- fq\\n\\n# quantized model\\n# weights and activations are in int8\\nprevious_layer_int8 -- linear_with_activation_int8 -- next_layer_int8\\n                     /\\n   linear_weight_int8\\n',\n","   'Id': 835,\n","   'Question': 'How to use Quantization Aware Training models the effects of quantization during training\\nallowing for higher accuracy compared to other quantization methods.  During\\ntraining, all calculations are done in floating point, with fake_quant modules\\nmodeling the effects of quantization by clamping and rounding to simulate the\\neffects of INT8.  After model conversion, weights and\\nactivations are quantized, and activations are fused into the preceding layer\\nwhere possible.  It is commonly used with CNNs and yields a higher accuracy\\ncompared to static quantization.  Quantization Aware Training is also known as\\nQAT.Diagram:, give an example?',\n","   'context': ' Diagram:',\n","   'source': 'https://pytorch.org/docs/stable/quantization.html'},\n","  {'Answer': \"import torch\\n\\n# define a floating point model where some layers could benefit from QAT\\nclass M(torch.nn.Module):\\n    def __init__(self):\\n        super(M, self).__init__()\\n        # QuantStub converts tensors from floating point to quantized\\n        self.quant = torch.quantization.QuantStub()\\n        self.conv = torch.nn.Conv2d(1, 1, 1)\\n        self.bn = torch.nn.BatchNorm2d(1)\\n        self.relu = torch.nn.ReLU()\\n        # DeQuantStub converts tensors from quantized to floating point\\n        self.dequant = torch.quantization.DeQuantStub()\\n\\n    def forward(self, x):\\n        x = self.quant(x)\\n        x = self.conv(x)\\n        x = self.bn(x)\\n        x = self.relu(x)\\n        x = self.dequant(x)\\n        return x\\n\\n# create a model instance\\nmodel_fp32 = M()\\n\\n# model must be set to train mode for QAT logic to work\\nmodel_fp32.train()\\n\\n# attach a global qconfig, which contains information about what kind\\n# of observers to attach. Use 'fbgemm' for server inference and\\n# 'qnnpack' for mobile inference. Other quantization configurations such\\n# as selecting symmetric or assymetric quantization and MinMax or L2Norm\\n# calibration techniques can be specified here.\\nmodel_fp32.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\\n\\n# fuse the activations to preceding layers, where applicable\\n# this needs to be done manually depending on the model architecture\\nmodel_fp32_fused = torch.quantization.fuse_modules(model_fp32,\\n    [['conv', 'bn', 'relu']])\\n\\n# Prepare the model for QAT. This inserts observers and fake_quants in\\n# the model that will observe weight and activation tensors during calibration.\\nmodel_fp32_prepared = torch.quantization.prepare_qat(model_fp32_fused)\\n\\n# run the training loop (not shown)\\ntraining_loop(model_fp32_prepared)\\n\\n# Convert the observed model to a quantized model. This does several things:\\n# quantizes the weights, computes and stores the scale and bias value to be\\n# used with each activation tensor, fuses modules where appropriate,\\n# and replaces key operators with quantized implementations.\\nmodel_fp32_prepared.eval()\\nmodel_int8 = torch.quantization.convert(model_fp32_prepared)\\n\\n# run the model, relevant calculations will happen in int8\\nres = model_int8(input_fp32)\\n\",\n","   'Id': 836,\n","   'Question': 'How  Diagram:, give an example?',\n","   'context': ' Diagram:',\n","   'source': 'https://pytorch.org/docs/stable/quantization.html'},\n","  {'Answer': 'import torch.quantization.quantize_fx as quantize_fx\\nimport copy\\n\\nmodel_fp = UserModel(...)\\n\\n#\\n# post training dynamic/weight_only quantization\\n#\\n\\n# we need to deepcopy if we still want to keep model_fp unchanged after quantization since quantization apis change the input model\\nmodel_to_quantize = copy.deepcopy(model_fp)\\nmodel_to_quantize.eval()\\nqconfig_dict = {\"\": torch.quantization.default_dynamic_qconfig}\\n# prepare\\nmodel_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_dict)\\n# no calibration needed when we only have dynamici/weight_only quantization\\n# quantize\\nmodel_quantized = quantize_fx.convert_fx(model_prepared)\\n\\n#\\n# post training static quantization\\n#\\n\\nmodel_to_quantize = copy.deepcopy(model_fp)\\nqconfig_dict = {\"\": torch.quantization.get_default_qconfig(\\'qnnpack\\')}\\nmodel_to_quantize.eval()\\n# prepare\\nmodel_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_dict)\\n# calibrate (not shown)\\n# quantize\\nmodel_quantized = quantize_fx.convert_fx(model_prepared)\\n\\n#\\n# quantization aware training for static quantization\\n#\\n\\nmodel_to_quantize = copy.deepcopy(model_fp)\\nqconfig_dict = {\"\": torch.quantization.get_default_qat_qconfig(\\'qnnpack\\')}\\nmodel_to_quantize.train()\\n# prepare\\nmodel_prepared = quantize_fx.prepare_qat_fx(model_to_qunatize, qconfig_dict)\\n# training loop (not shown)\\n# quantize\\nmodel_quantized = quantize_fx.convert_fx(model_prepared)\\n\\n#\\n# fusion\\n#\\nmodel_to_quantize = copy.deepcopy(model_fp)\\nmodel_fused = quantize_fx.fuse_fx(model_to_quantize)\\n',\n","   'Id': 837,\n","   'Question': 'How to use There are multiple quantization types in post training quantization (weight only, dynamic and static) and the configuration is done throughqconfig_dict(an argument of theprepare_fxfunction)., give an example?',\n","   'context': ' There are multiple quantization types in post training quantization (weight only, dynamic and static) and the configuration is done throughqconfig_dict(an argument of theprepare_fxfunction).',\n","   'source': 'https://pytorch.org/docs/stable/quantization.html'},\n","  {'Answer': \"RuntimeError: Could not run 'quantized::some_operator' with arguments from the 'CPU' backend...\\n\",\n","   'Id': 838,\n","   'Question': 'How to use If you see an error similar to:, give an example?',\n","   'context': ' If you see an error similar to:',\n","   'source': 'https://pytorch.org/docs/stable/quantization.html'},\n","  {'Answer': 'class M(torch.nn.Module):\\n    def __init__(self):\\n        super().__init__()\\n        self.quant = torch.quantization.QuantStub()\\n        self.conv = torch.nn.Conv2d(1, 1, 1)\\n\\n    def forward(self, x):\\n        # during the convert step, this will be replaced with a\\n        # `quantize_per_tensor` call\\n        x = self.quant(x)\\n        x = self.conv(x)\\n        return x\\n',\n","   'Id': 839,\n","   'Question': 'How to use If you see an error similar to:This means that you are trying to pass a non-quantized Tensor to a quantized\\nkernel. A common workaround is to usetorch.quantization.QuantStubto\\nquantize the tensor.  This needs to be done manually in Eager mode quantization.\\nAn e2e example:, give an example?',\n","   'context': ' This means that you are trying to pass a non-quantized Tensor to a quantized\\nkernel. A common workaround is to usetorch.quantization.QuantStubto\\nquantize the tensor.  This needs to be done manually in Eager mode quantization.\\nAn e2e example:',\n","   'source': 'https://pytorch.org/docs/stable/quantization.html'},\n","  {'Answer': \"RuntimeError: Could not run 'aten::thnn_conv2d_forward' with arguments from the 'QuantizedCPU' backend.\\n\",\n","   'Id': 840,\n","   'Question': 'How  If you see an error similar to:, give an example?',\n","   'context': ' If you see an error similar to:',\n","   'source': 'https://pytorch.org/docs/stable/quantization.html'},\n","  {'Answer': 'class M(torch.nn.Module):\\n    def __init__(self):\\n        super().__init__()\\n        self.quant = torch.quantization.QuantStub()\\n        self.conv1 = torch.nn.Conv2d(1, 1, 1)\\n        # this module will not be quantized (see `qconfig = None` logic below)\\n        self.conv2 = torch.nn.Conv2d(1, 1, 1)\\n        self.dequant = torch.quantization.DeQuantStub()\\n\\n    def forward(self, x):\\n        # during the convert step, this will be replaced with a\\n        # `quantize_per_tensor` call\\n        x = self.quant(x)\\n        x = self.conv1(x)\\n        # during the convert step, this will be replaced with a\\n        # `dequantize` call\\n        x = self.dequant(x)\\n        x = self.conv2(x)\\n        return x\\n\\nm = M()\\nm.qconfig = some_qconfig\\n# turn off quantization for conv2\\nm.conv2.qconfig = None\\n',\n","   'Id': 841,\n","   'Question': 'How to use If you see an error similar to:This means that you are trying to pass a quantized Tensor to a non-quantized\\nkernel. A common workaround is to usetorch.quantization.DeQuantStubto\\ndequantize the tensor.  This needs to be done manually in Eager mode quantization.\\nAn e2e example:, give an example?',\n","   'context': ' This means that you are trying to pass a quantized Tensor to a non-quantized\\nkernel. A common workaround is to usetorch.quantization.DeQuantStubto\\ndequantize the tensor.  This needs to be done manually in Eager mode quantization.\\nAn e2e example:',\n","   'source': 'https://pytorch.org/docs/stable/quantization.html'},\n","  {'Answer': '>>> x = torch.randn(4)\\n>>> x\\ntensor([ 0.0552,  0.9730,  0.3973, -1.0780])\\n>>> torch.fake_quantize_per_tensor_affine(x, 0.1, 0, 0, 255)\\ntensor([0.1000, 1.0000, 0.4000, 0.0000])\\n',\n","   'Id': 842,\n","   'Question': 'How to use torch.fake_quantize_per_tensor_affine, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.fake_quantize_per_tensor_affine.html#torch.fake_quantize_per_tensor_affine'},\n","  {'Answer': '>>> M = torch.randn(10, 3, 5)\\n>>> batch1 = torch.randn(10, 3, 4)\\n>>> batch2 = torch.randn(10, 4, 5)\\n>>> torch.baddbmm(M, batch1, batch2).size()\\ntorch.Size([10, 3, 5])\\n',\n","   'Id': 843,\n","   'Question': 'How to use torch.baddbmm, give an example?',\n","   'context': ' This operator supportsTensorFloat32.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.baddbmm.html#torch.baddbmm'},\n","  {'Answer': '>>> torch.ge(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\\ntensor([[True, True], [False, True]])\\n',\n","   'Id': 844,\n","   'Question': 'How to use torch.ge, give an example?',\n","   'context': ' The second argument can be a number or a tensor whose shape isbroadcastablewith the first argument.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.ge.html#torch.ge'},\n","  {'Answer': '>>> a = torch.rand(1, 2).bool()\\n>>> a\\ntensor([[False, True]], dtype=torch.bool)\\n>>> torch.all(a)\\ntensor(False, dtype=torch.bool)\\n>>> a = torch.arange(0, 3)\\n>>> a\\ntensor([0, 1, 2])\\n>>> torch.all(a)\\ntensor(False)\\n',\n","   'Id': 845,\n","   'Question': 'How to use torch.all, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.all.html#torch.all'},\n","  {'Answer': '>>> a = torch.rand(4, 2).bool()\\n>>> a\\ntensor([[True, True],\\n        [True, False],\\n        [True, True],\\n        [True, True]], dtype=torch.bool)\\n>>> torch.all(a, dim=1)\\ntensor([ True, False,  True,  True], dtype=torch.bool)\\n>>> torch.all(a, dim=0)\\ntensor([ True, False], dtype=torch.bool)\\n',\n","   'Id': 846,\n","   'Question': 'How  IfkeepdimisTrue, the output tensor is of the same size\\nasinputexcept in the dimensiondimwhere it is of size 1.\\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\\nthe output tensor having 1 fewer dimension thaninput., give an example?',\n","   'context': ' IfkeepdimisTrue, the output tensor is of the same size\\nasinputexcept in the dimensiondimwhere it is of size 1.\\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\\nthe output tensor having 1 fewer dimension thaninput.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.all.html#torch.all'},\n","  {'Answer': '>>> torch.dot(torch.tensor([2, 3]), torch.tensor([2, 1]))\\ntensor(7)\\n',\n","   'Id': 847,\n","   'Question': 'How to use torch.dot, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.dot.html#torch.dot'},\n","  {'Answer': \"import torch\\nimport torchvision\\nfrom torch.utils.tensorboard import SummaryWriter\\nfrom torchvision import datasets, transforms\\n\\n# Writer will output to ./runs/ directory by default\\nwriter = SummaryWriter()\\n\\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\\ntrainset = datasets.MNIST('mnist_train', train=True, download=True, transform=transform)\\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\\nmodel = torchvision.models.resnet50(False)\\n# Have ResNet model take in grayscale rather than RGB\\nmodel.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\\nimages, labels = next(iter(trainloader))\\n\\ngrid = torchvision.utils.make_grid(images)\\nwriter.add_image('images', grid, 0)\\nwriter.add_graph(model, images)\\nwriter.close()\\n\",\n","   'Id': 848,\n","   'Question': 'How to use Once you’ve installed TensorBoard, these utilities let you log PyTorch models\\nand metrics into a directory for visualization within the TensorBoard UI.\\nScalars, images, histograms, graphs, and embedding visualizations are all\\nsupported for PyTorch models and tensors as well as Caffe2 nets and blobs.The SummaryWriter class is your main entry to log data for consumption\\nand visualization by TensorBoard. For example:, give an example?',\n","   'context': ' Once you’ve installed TensorBoard, these utilities let you log PyTorch models\\nand metrics into a directory for visualization within the TensorBoard UI.\\nScalars, images, histograms, graphs, and embedding visualizations are all\\nsupported for PyTorch models and tensors as well as Caffe2 nets and blobs.The SummaryWriter class is your main entry to log data for consumption\\nand visualization by TensorBoard. For example:',\n","   'source': 'https://pytorch.org/docs/stable/tensorboard.html'},\n","  {'Answer': 'pip install tensorboard\\ntensorboard --logdir=runs\\n',\n","   'Id': 849,\n","   'Question': 'How to use The SummaryWriter class is your main entry to log data for consumption\\nand visualization by TensorBoard. For example:This can then be visualized with TensorBoard, which should be installable\\nand runnable with:, give an example?',\n","   'context': ' The SummaryWriter class is your main entry to log data for consumption\\nand visualization by TensorBoard. For example:This can then be visualized with TensorBoard, which should be installable\\nand runnable with:',\n","   'source': 'https://pytorch.org/docs/stable/tensorboard.html'},\n","  {'Answer': \"from torch.utils.tensorboard import SummaryWriter\\nimport numpy as np\\n\\nwriter = SummaryWriter()\\n\\nfor n_iter in range(100):\\n    writer.add_scalar('Loss/train', np.random.random(), n_iter)\\n    writer.add_scalar('Loss/test', np.random.random(), n_iter)\\n    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)\\n    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)\\n\",\n","   'Id': 850,\n","   'Question': 'How to use This can then be visualized with TensorBoard, which should be installable\\nand runnable with:Lots of information can be logged for one experiment. To avoid cluttering\\nthe UI and have better result clustering, we can group plots by naming them\\nhierarchically. For example, “Loss/train” and “Loss/test” will be grouped\\ntogether, while “Accuracy/train” and “Accuracy/test” will be grouped separately\\nin the TensorBoard interface., give an example?',\n","   'context': ' This can then be visualized with TensorBoard, which should be installable\\nand runnable with:Lots of information can be logged for one experiment. To avoid cluttering\\nthe UI and have better result clustering, we can group plots by naming them\\nhierarchically. For example, “Loss/train” and “Loss/test” will be grouped\\ntogether, while “Accuracy/train” and “Accuracy/test” will be grouped separately\\nin the TensorBoard interface.',\n","   'source': 'https://pytorch.org/docs/stable/tensorboard.html'},\n","  {'Answer': 'from torch.utils.tensorboard import SummaryWriter\\n\\n# create a summary writer with automatically generated folder name.\\nwriter = SummaryWriter()\\n# folder location: runs/May04_22-14-54_s-MacBook-Pro.local/\\n\\n# create a summary writer using the specified folder name.\\nwriter = SummaryWriter(\"my_experiment\")\\n# folder location: my_experiment\\n\\n# create a summary writer with comment appended.\\nwriter = SummaryWriter(comment=\"LR_0.1_BATCH_16\")\\n# folder location: runs/May04_22-14-54_s-MacBook-Pro.localLR_0.1_BATCH_16/\\n',\n","   'Id': 851,\n","   'Question': 'How to use torch.utils.tensorboard.writer.SummaryWriter.__init__, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/tensorboard.html'},\n","  {'Answer': \"from torch.utils.tensorboard import SummaryWriter\\nwriter = SummaryWriter()\\nx = range(100)\\nfor i in x:\\n    writer.add_scalar('y=2x', i * 2, i)\\nwriter.close()\\n\",\n","   'Id': 852,\n","   'Question': 'How to use torch.utils.tensorboard.writer.SummaryWriter.add_scalar, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/tensorboard.html'},\n","  {'Answer': \"from torch.utils.tensorboard import SummaryWriter\\nwriter = SummaryWriter()\\nr = 5\\nfor i in range(100):\\n    writer.add_scalars('run_14h', {'xsinx':i*np.sin(i/r),\\n                                    'xcosx':i*np.cos(i/r),\\n                                    'tanx': np.tan(i/r)}, i)\\nwriter.close()\\n# This call adds three values to the same scalar plot with the tag\\n# 'run_14h' in TensorBoard's scalar section.\\n\",\n","   'Id': 853,\n","   'Question': 'How to use torch.utils.tensorboard.writer.SummaryWriter.add_scalars, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/tensorboard.html'},\n","  {'Answer': \"from torch.utils.tensorboard import SummaryWriter\\nimport numpy as np\\nwriter = SummaryWriter()\\nfor i in range(10):\\n    x = np.random.random(1000)\\n    writer.add_histogram('distribution centers', x + i, i)\\nwriter.close()\\n\",\n","   'Id': 854,\n","   'Question': 'How to use torch.utils.tensorboard.writer.SummaryWriter.add_histogram, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/tensorboard.html'},\n","  {'Answer': \"from torch.utils.tensorboard import SummaryWriter\\nimport numpy as np\\nimg = np.zeros((3, 100, 100))\\nimg[0] = np.arange(0, 10000).reshape(100, 100) / 10000\\nimg[1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000\\n\\nimg_HWC = np.zeros((100, 100, 3))\\nimg_HWC[:, :, 0] = np.arange(0, 10000).reshape(100, 100) / 10000\\nimg_HWC[:, :, 1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000\\n\\nwriter = SummaryWriter()\\nwriter.add_image('my_image', img, 0)\\n\\n# If you have non-default dimension setting, set the dataformats argument.\\nwriter.add_image('my_image_HWC', img_HWC, 0, dataformats='HWC')\\nwriter.close()\\n\",\n","   'Id': 855,\n","   'Question': 'How to use torch.utils.tensorboard.writer.SummaryWriter.add_image, give an example?',\n","   'context': ' Note that this requires thepillowpackage.',\n","   'source': 'https://pytorch.org/docs/stable/tensorboard.html'},\n","  {'Answer': \"from torch.utils.tensorboard import SummaryWriter\\nimport numpy as np\\n\\nimg_batch = np.zeros((16, 3, 100, 100))\\nfor i in range(16):\\n    img_batch[i, 0] = np.arange(0, 10000).reshape(100, 100) / 10000 / 16 * i\\n    img_batch[i, 1] = (1 - np.arange(0, 10000).reshape(100, 100) / 10000) / 16 * i\\n\\nwriter = SummaryWriter()\\nwriter.add_images('my_image_batch', img_batch, 0)\\nwriter.close()\\n\",\n","   'Id': 856,\n","   'Question': 'How to use torch.utils.tensorboard.writer.SummaryWriter.add_images, give an example?',\n","   'context': ' Note that this requires thepillowpackage.',\n","   'source': 'https://pytorch.org/docs/stable/tensorboard.html'},\n","  {'Answer': \"writer.add_text('lstm', 'This is an lstm', 0)\\nwriter.add_text('rnn', 'This is an rnn', 10)\\n\",\n","   'Id': 857,\n","   'Question': 'How to use torch.utils.tensorboard.writer.SummaryWriter.add_text, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/tensorboard.html'},\n","  {'Answer': 'import keyword\\nimport torch\\nmeta = []\\nwhile len(meta)<100:\\n    meta = meta+keyword.kwlist # get some strings\\nmeta = meta[:100]\\n\\nfor i, v in enumerate(meta):\\n    meta[i] = v+str(i)\\n\\nlabel_img = torch.rand(100, 3, 10, 32)\\nfor i in range(100):\\n    label_img[i]*=i/100.0\\n\\nwriter.add_embedding(torch.randn(100, 5), metadata=meta, label_img=label_img)\\nwriter.add_embedding(torch.randn(100, 5), label_img=label_img)\\nwriter.add_embedding(torch.randn(100, 5), metadata=meta)\\n',\n","   'Id': 858,\n","   'Question': 'How to use torch.utils.tensorboard.writer.SummaryWriter.add_embedding, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/tensorboard.html'},\n","  {'Answer': \"from torch.utils.tensorboard import SummaryWriter\\nimport numpy as np\\nlabels = np.random.randint(2, size=100)  # binary label\\npredictions = np.random.rand(100)\\nwriter = SummaryWriter()\\nwriter.add_pr_curve('pr_curve', labels, predictions, 0)\\nwriter.close()\\n\",\n","   'Id': 859,\n","   'Question': 'How to use torch.utils.tensorboard.writer.SummaryWriter.add_pr_curve, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/tensorboard.html'},\n","  {'Answer': \"layout = {'Taiwan':{'twse':['Multiline',['twse/0050', 'twse/2330']]},\\n             'USA':{ 'dow':['Margin',   ['dow/aaa', 'dow/bbb', 'dow/ccc']],\\n                  'nasdaq':['Margin',   ['nasdaq/aaa', 'nasdaq/bbb', 'nasdaq/ccc']]}}\\n\\nwriter.add_custom_scalars(layout)\\n\",\n","   'Id': 860,\n","   'Question': 'How to use torch.utils.tensorboard.writer.SummaryWriter.add_custom_scalars, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/tensorboard.html'},\n","  {'Answer': \"from torch.utils.tensorboard import SummaryWriter\\nvertices_tensor = torch.as_tensor([\\n    [1, 1, 1],\\n    [-1, -1, 1],\\n    [1, -1, -1],\\n    [-1, 1, -1],\\n], dtype=torch.float).unsqueeze(0)\\ncolors_tensor = torch.as_tensor([\\n    [255, 0, 0],\\n    [0, 255, 0],\\n    [0, 0, 255],\\n    [255, 0, 255],\\n], dtype=torch.int).unsqueeze(0)\\nfaces_tensor = torch.as_tensor([\\n    [0, 2, 3],\\n    [0, 3, 1],\\n    [0, 1, 2],\\n    [1, 3, 2],\\n], dtype=torch.int).unsqueeze(0)\\n\\nwriter = SummaryWriter()\\nwriter.add_mesh('my_mesh', vertices=vertices_tensor, colors=colors_tensor, faces=faces_tensor)\\n\\nwriter.close()\\n\",\n","   'Id': 861,\n","   'Question': 'How to use torch.utils.tensorboard.writer.SummaryWriter.add_mesh, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/tensorboard.html'},\n","  {'Answer': \"from torch.utils.tensorboard import SummaryWriter\\nwith SummaryWriter() as w:\\n    for i in range(5):\\n        w.add_hparams({'lr': 0.1*i, 'bsize': i},\\n                      {'hparam/accuracy': 10*i, 'hparam/loss': 10*i})\\n\",\n","   'Id': 862,\n","   'Question': 'How to use torch.utils.tensorboard.writer.SummaryWriter.add_hparams, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/tensorboard.html'},\n","  {'Answer': '>>> torch.linspace(3, 10, steps=5)\\ntensor([  3.0000,   4.7500,   6.5000,   8.2500,  10.0000])\\n>>> torch.linspace(-10, 10, steps=5)\\ntensor([-10.,  -5.,   0.,   5.,  10.])\\n>>> torch.linspace(start=-10, end=10, steps=5)\\ntensor([-10.,  -5.,   0.,   5.,  10.])\\n>>> torch.linspace(start=-10, end=10, steps=1)\\ntensor([-10.])\\n',\n","   'Id': 863,\n","   'Question': 'How to use torch.linspace, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.linspace.html#torch.linspace'},\n","  {'Answer': '>>> a = torch.tensor([5, 10, 15])\\n>>> b = torch.tensor([3, 4, 5])\\n>>> torch.lcm(a, b)\\ntensor([15, 20, 15])\\n>>> c = torch.tensor([3])\\n>>> torch.lcm(a, c)\\ntensor([15, 30, 15])\\n',\n","   'Id': 864,\n","   'Question': 'How to use torch.lcm, give an example?',\n","   'context': ' Bothinputandothermust have integer types.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.lcm.html#torch.lcm'},\n","  {'Answer': 'for iterations...\\n    ...\\n    for param in model.parameters():\\n        param.grad = None\\n    loss.backward()\\n',\n","   'Id': 865,\n","   'Question': 'How to use The default behavior (letting.grads beNonebefore the firstbackward(), such that their layout is created according to 1 or 2,\\nand retained over time according to 3 or 4) is recommended for best performance.\\nCalls tomodel.zero_grad()oroptimizer.zero_grad()will not affect.gradlayouts.In fact, resetting all.grads toNonebefore each\\naccumulation phase, e.g.:, give an example?',\n","   'context': ' The default behavior (letting.grads beNonebefore the firstbackward(), such that their layout is created according to 1 or 2,\\nand retained over time according to 3 or 4) is recommended for best performance.\\nCalls tomodel.zero_grad()oroptimizer.zero_grad()will not affect.gradlayouts.In fact, resetting all.grads toNonebefore each\\naccumulation phase, e.g.:',\n","   'source': 'https://pytorch.org/docs/stable/autograd.html'},\n","  {'Answer': '>>> class Exp(Function):\\n>>>\\n>>>     @staticmethod\\n>>>     def forward(ctx, i):\\n>>>         result = i.exp()\\n>>>         ctx.save_for_backward(result)\\n>>>         return result\\n>>>\\n>>>     @staticmethod\\n>>>     def backward(ctx, grad_output):\\n>>>         result, = ctx.saved_tensors\\n>>>         return grad_output * result\\n>>>\\n>>> #Use it by calling the apply method:\\n>>> output = Exp.apply(input)\\n',\n","   'Id': 866,\n","   'Question': 'How to use torch.autograd.Function, give an example?',\n","   'context': ' Normally, the only way users interact with functions is by creating\\nsubclasses and defining new operations. This is a recommended way of\\nextending torch.autograd.',\n","   'source': 'https://pytorch.org/docs/stable/autograd.html'},\n","  {'Answer': '>>> x = torch.randn((1, 1), requires_grad=True)\\n>>> with torch.autograd.profiler.profile() as prof:\\n>>>     for _ in range(100):  # any normal python code, really!\\n>>>         y = x ** 2\\n>>          y.backward()\\n>>> # NOTE: some columns were removed for brevity\\n>>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\\n-----------------------------------  ---------------  ---------------  ---------------\\nName                                 Self CPU total   CPU time avg     Number of Calls\\n-----------------------------------  ---------------  ---------------  ---------------\\nmul                                  32.048ms         32.048ms         200\\npow                                  27.041ms         27.041ms         200\\nPowBackward0                         9.727ms          55.483ms         100\\ntorch::autograd::AccumulateGrad      9.148ms          9.148ms          100\\ntorch::autograd::GraphRoot           691.816us        691.816us        100\\n-----------------------------------  ---------------  ---------------  ---------------\\n',\n","   'Id': 867,\n","   'Question': 'How to use torch.autograd.profiler.profile, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/autograd.html'},\n","  {'Answer': 'nvprof --profile-from-start off -o trace_name.prof -- <regular command here>\\n',\n","   'Id': 868,\n","   'Question': 'How to use It is useful when running the program under nvprof:, give an example?',\n","   'context': ' It is useful when running the program under nvprof:',\n","   'source': 'https://pytorch.org/docs/stable/autograd.html'},\n","  {'Answer': '>>> with torch.cuda.profiler.profile():\\n...     model(x) # Warmup CUDA memory allocator and profiler\\n...     with torch.autograd.profiler.emit_nvtx():\\n...         model(x)\\n',\n","   'Id': 869,\n","   'Question': 'How to use torch.autograd.profiler.emit_nvtx, give an example?',\n","   'context': ' Unfortunately, there’s no way to force nvprof to flush the data it collected\\nto disk, so for CUDA profiling one has to use this context manager to annotate\\nnvprof traces and wait for the process to exit before inspecting them.\\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, ortorch.autograd.profiler.load_nvprof()can load the results for inspection\\ne.g. in Python REPL.',\n","   'source': 'https://pytorch.org/docs/stable/autograd.html'},\n","  {'Answer': '>>> import torch\\n>>> from torch import autograd\\n>>> class MyFunc(autograd.Function):\\n...     @staticmethod\\n...     def forward(ctx, inp):\\n...         return inp.clone()\\n...     @staticmethod\\n...     def backward(ctx, gO):\\n...         # Error during the backward pass\\n...         raise RuntimeError(\"Some error in backward\")\\n...         return gO.clone()\\n>>> def run_fn(a):\\n...     out = MyFunc.apply(a)\\n...     return out.sum()\\n>>> inp = torch.rand(10, 10, requires_grad=True)\\n>>> out = run_fn(inp)\\n>>> out.backward()\\n    Traceback (most recent call last):\\n      File \"<stdin>\", line 1, in <module>\\n      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\\n        torch.autograd.backward(self, gradient, retain_graph, create_graph)\\n      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\\n        allow_unreachable=True)  # allow_unreachable flag\\n      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\\n        return self._forward_cls.backward(self, *args)\\n      File \"<stdin>\", line 8, in backward\\n    RuntimeError: Some error in backward\\n>>> with autograd.detect_anomaly():\\n...     inp = torch.rand(10, 10, requires_grad=True)\\n...     out = run_fn(inp)\\n...     out.backward()\\n    Traceback of forward call that caused the error:\\n      File \"tmp.py\", line 53, in <module>\\n        out = run_fn(inp)\\n      File \"tmp.py\", line 44, in run_fn\\n        out = MyFunc.apply(a)\\n    Traceback (most recent call last):\\n      File \"<stdin>\", line 4, in <module>\\n      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\\n        torch.autograd.backward(self, gradient, retain_graph, create_graph)\\n      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\\n        allow_unreachable=True)  # allow_unreachable flag\\n      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\\n        return self._forward_cls.backward(self, *args)\\n      File \"<stdin>\", line 8, in backward\\n    RuntimeError: Some error in backward\\n',\n","   'Id': 870,\n","   'Question': 'How to use torch.autograd.detect_anomaly, give an example?',\n","   'context': ' This does two things:',\n","   'source': 'https://pytorch.org/docs/stable/autograd.html'},\n","  {'Answer': '>>> a = torch.randn(3, 4)\\n>>> b = torch.randn(4, 5)\\n>>> c = torch.randn(5, 6)\\n>>> d = torch.randn(6, 7)\\n>>> torch.chain_matmul(a, b, c, d)\\ntensor([[ -2.3375,  -3.9790,  -4.1119,  -6.6577,   9.5609, -11.5095,  -3.2614],\\n        [ 21.4038,   3.3378,  -8.4982,  -5.2457, -10.2561,  -2.4684,   2.7163],\\n        [ -0.9647,  -5.8917,  -2.3213,  -5.2284,  12.8615, -12.2816,  -2.5095]])\\n',\n","   'Id': 871,\n","   'Question': 'How to use torch.chain_matmul, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.chain_matmul.html#torch.chain_matmul'},\n","  {'Answer': '>>> a = torch.randn(4)\\n>>> a\\ntensor([ 0.8986, -0.7279,  1.1745,  0.2611])\\n>>> torch.tanh(a)\\ntensor([ 0.7156, -0.6218,  0.8257,  0.2553])\\n',\n","   'Id': 872,\n","   'Question': 'How to use torch.tanh, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.tanh.html#torch.tanh'},\n","  {'Answer': '>>> a = torch.randn(4)\\n>>> a\\ntensor([ 0.9041,  0.0196, -0.3108, -2.4423])\\n>>> torch.atan2(a, torch.randn(4))\\ntensor([ 0.9833,  0.0811, -1.9743, -1.4151])\\n',\n","   'Id': 873,\n","   'Question': 'How to use torch.atan2, give an example?',\n","   'context': ' The shapes ofinputandothermust bebroadcastable.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.atan2.html#torch.atan2'},\n","  {'Answer': '>>> x = torch.zeros(1, requires_grad=True)\\n>>> with torch.no_grad():\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> is_train = False\\n>>> with torch.set_grad_enabled(is_train):\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n\\n>>> torch.set_grad_enabled(True)  # this can also be used as a function\\n>>> y = x * 2\\n>>> y.requires_grad\\nTrue\\n\\n>>> torch.set_grad_enabled(False)\\n>>> y = x * 2\\n>>> y.requires_grad\\nFalse\\n',\n","   'Id': 874,\n","   'Question': 'How  , give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/torch.html#reduction-ops'},\n","  {'Answer': '>>> torch.ne(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\\ntensor([[False, True], [True, False]])\\n',\n","   'Id': 875,\n","   'Question': 'How to use torch.ne, give an example?',\n","   'context': ' The second argument can be a number or a tensor whose shape isbroadcastablewith the first argument.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.ne.html#torch.ne'},\n","  {'Answer': '>>> t = torch.randn(3, 4)\\n>>> t\\ntensor([[-0.1321,  0.4370, -1.2631, -1.1289],\\n        [-2.0527, -1.1250,  0.2275,  0.3077],\\n        [-0.0881, -0.1259, -0.5495,  1.0284]])\\n>>> torch.msort(t)\\ntensor([[-2.0527, -1.1250, -1.2631, -1.1289],\\n        [-0.1321, -0.1259, -0.5495,  0.3077],\\n        [-0.0881,  0.4370,  0.2275,  1.0284]])\\n',\n","   'Id': 876,\n","   'Question': 'How to use torch.msort, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.msort.html#torch.msort'},\n","  {'Answer': 'python -m torch.utils.bottleneck /path/to/source/script.py [args]\\n',\n","   'Id': 877,\n","   'Question': 'How to use torch.utils.bottleneckis a tool that can be used as an initial step for\\ndebugging bottlenecks in your program. It summarizes runs of your script with\\nthe Python profiler and PyTorch’s autograd profiler.Run it on the command line with, give an example?',\n","   'context': ' Run it on the command line with',\n","   'source': 'https://pytorch.org/docs/stable/bottleneck.html'},\n","  {'Answer': '>>> model = nn.Sequential(...)\\n>>> input_var = checkpoint_sequential(model, chunks, input_var)\\n',\n","   'Id': 878,\n","   'Question': 'How to use torch.utils.checkpoint.checkpoint_sequential, give an example?',\n","   'context': ' Seecheckpoint()on how checkpointing works.',\n","   'source': 'https://pytorch.org/docs/stable/checkpoint.html'},\n","  {'Answer': '>>> torch.unbind(torch.tensor([[1, 2, 3],\\n>>>                            [4, 5, 6],\\n>>>                            [7, 8, 9]]))\\n(tensor([1, 2, 3]), tensor([4, 5, 6]), tensor([7, 8, 9]))\\n',\n","   'Id': 879,\n","   'Question': 'How to use torch.unbind, give an example?',\n","   'context': ' Returns a tuple of all slices along a given dimension, already without it.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.unbind.html#torch.unbind'},\n","  {'Answer': '>>> a = torch.randn(3, 3)\\n>>> a = torch.mm(a, a.t()) + 1e-05 * torch.eye(3) # make symmetric positive definite\\n>>> u = torch.cholesky(a)\\n>>> a\\ntensor([[  0.9935,  -0.6353,   1.5806],\\n        [ -0.6353,   0.8769,  -1.7183],\\n        [  1.5806,  -1.7183,  10.6618]])\\n>>> torch.cholesky_inverse(u)\\ntensor([[ 1.9314,  1.2251, -0.0889],\\n        [ 1.2251,  2.4439,  0.2122],\\n        [-0.0889,  0.2122,  0.1412]])\\n>>> a.inverse()\\ntensor([[ 1.9314,  1.2251, -0.0889],\\n        [ 1.2251,  2.4439,  0.2122],\\n        [-0.0889,  0.2122,  0.1412]])\\n',\n","   'Id': 880,\n","   'Question': 'How to use torch.cholesky_inverse, give an example?',\n","   'context': ' IfupperisTrueor not provided,uuuis upper\\ntriangular such that the returned tensor is',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.cholesky_inverse.html#torch.cholesky_inverse'},\n","  {'Answer': \">>> A = torch.randn(2, 3, 3)\\n>>> A_LU, pivots = torch.lu(A)\\n>>> A_LU\\ntensor([[[ 1.3506,  2.5558, -0.0816],\\n         [ 0.1684,  1.1551,  0.1940],\\n         [ 0.1193,  0.6189, -0.5497]],\\n\\n        [[ 0.4526,  1.2526, -0.3285],\\n         [-0.7988,  0.7175, -0.9701],\\n         [ 0.2634, -0.9255, -0.3459]]])\\n>>> pivots\\ntensor([[ 3,  3,  3],\\n        [ 3,  3,  3]], dtype=torch.int32)\\n>>> A_LU, pivots, info = torch.lu(A, get_infos=True)\\n>>> if info.nonzero().size(0) == 0:\\n...   print('LU factorization succeeded for all samples!')\\nLU factorization succeeded for all samples!\\n\",\n","   'Id': 881,\n","   'Question': 'How to use torch.lu, give an example?',\n","   'context': ' ',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu'},\n","  {'Answer': '>>> a1 = torch.tensor([4.0])\\n>>> a2 = torch.tensor([3.0, 4.0, 5.0])\\n>>> a = torch.igammac(a1, a2)\\ntensor([0.3528, 0.5665, 0.7350])\\ntensor([0.3528, 0.5665, 0.7350])\\n>>> b = torch.igamma(a1, a2) + torch.igammac(a1, a2)\\ntensor([1., 1., 1.])\\n',\n","   'Id': 882,\n","   'Question': 'How to use torch.igamma, give an example?',\n","   'context': ' Supportsbroadcasting to a common shapeand float inputs.',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.igamma.html#torch.igamma'},\n","  {'Answer': \">>> torch.load('tensors.pt')\\n# Load all tensors onto the CPU\\n>>> torch.load('tensors.pt', map_location=torch.device('cpu'))\\n# Load all tensors onto the CPU, using a function\\n>>> torch.load('tensors.pt', map_location=lambda storage, loc: storage)\\n# Load all tensors onto GPU 1\\n>>> torch.load('tensors.pt', map_location=lambda storage, loc: storage.cuda(1))\\n# Map tensors from GPU 1 to GPU 0\\n>>> torch.load('tensors.pt', map_location={'cuda:1':'cuda:0'})\\n# Load tensor from io.BytesIO object\\n>>> with open('tensor.pt', 'rb') as f:\\n...     buffer = io.BytesIO(f.read())\\n>>> torch.load(buffer)\\n# Load a module with 'ascii' encoding for unpickling\\n>>> torch.load('module.pt', encoding='ascii')\\n\",\n","   'Id': 883,\n","   'Question': 'How to use torch.load, give an example?',\n","   'context': ' User extensions can register their own location tags and tagging and\\ndeserialization methods usingtorch.serialization.register_package().',\n","   'source': 'https://pytorch.org/docs/stable/generated/torch.load.html#torch.load'}],\n"," 883)"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-8n7nJTCaijj","executionInfo":{"status":"ok","timestamp":1628683293005,"user_tz":-330,"elapsed":150,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}},"outputId":"cf824f31-fa34-4fd1-fca4-e81181d007cf"},"source":["oneqtndict = {}\n","\n","for qa in qalist:\n","  oneqtndict[qa['Question']] = qa\n","len(oneqtndict)"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["750"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xebZOgYqaif3","executionInfo":{"status":"ok","timestamp":1628683293005,"user_tz":-330,"elapsed":63,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}},"outputId":"1df0f5c8-c852-473e-ebd0-a0910c697856"},"source":["urlqalist = {}\n","\n","for qa in qalist:\n","\n","  if qa['source'] in urlqalist:\n","    urlqalist[qa['source']].append(qa)\n","  else :\n","    urlqalist[qa['source']]= [qa]\n","len(urlqalist)"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["355"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"MmlsWg3baiXn","executionInfo":{"status":"ok","timestamp":1628683293006,"user_tz":-330,"elapsed":62,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}}},"source":["correction = []\n","for k,vs in urlqalist.items():\n","\n","  uniq = {}\n","  for v in vs:\n","    if v['Question'] in uniq:\n","      correction.append(v['Question']+'|'+v['source'])\n","    else:\n","      uniq[v['Question']] = v['source']\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hEBN4pPNaiTy","executionInfo":{"status":"ok","timestamp":1628683293006,"user_tz":-330,"elapsed":61,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}},"outputId":"e2c3f421-3260-45f8-9e40-c74540612e76"},"source":["len(correction)\n","statqstn = {}\n","for entry in correction:\n","  if entry in statqstn:\n","    statqstn[entry] =  statqstn[entry] + 1\n","  else:\n","    statqstn[entry] = 1\n","\n","len(statqstn)    "],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["15"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4IjjmzIiaiQX","executionInfo":{"status":"ok","timestamp":1628683293007,"user_tz":-330,"elapsed":60,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}},"outputId":"63eef13f-1e9b-4453-8e28-611434e5edce"},"source":["for k,v in statqstn.items():\n","  if v > 1:\n","    print(v ,k )\n","\n"],"execution_count":12,"outputs":[{"output_type":"stream","text":["7 How  Formax_abs_diffandmax_rel_diffthe type depends on thedtypeof the inputs., give an example?|https://pytorch.org/docs/stable/testing.html\n","2 How  is exported as:, give an example?|https://pytorch.org/docs/stable/onnx.html\n","9 How  , give an example?|https://pytorch.org/docs/stable/special.html#torch.special.exp2\n","9 How  , give an example?|https://pytorch.org/docs/stable/jit.html\n","9 How  , give an example?|https://pytorch.org/docs/stable/special.html#torch.special.erf\n","9 How  , give an example?|https://pytorch.org/docs/stable/special.html#torch.special.logit\n","4 How  , give an example?|https://pytorch.org/docs/stable/distributed.html\n","9 How  , give an example?|https://pytorch.org/docs/stable/special.html\n","9 How  , give an example?|https://pytorch.org/docs/stable/special.html#torch.special.expm1\n","9 How  , give an example?|https://pytorch.org/docs/stable/special.html#torch.special.erfc\n","9 How  , give an example?|https://pytorch.org/docs/stable/special.html#torch.special.expit\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GZ5t2AlfaiEZ","executionInfo":{"status":"ok","timestamp":1628683293007,"user_tz":-330,"elapsed":59,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}}},"source":[""],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"HnhUokYWhKhc","executionInfo":{"status":"ok","timestamp":1628683293008,"user_tz":-330,"elapsed":59,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}}},"source":["with open('onlypytorchcode.json', 'w') as file:\n","  file.write(json.dumps(qalist, indent=4))"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"V4-YKSZKI227","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628683293009,"user_tz":-330,"elapsed":60,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}},"outputId":"71cd1faf-29f7-4b5a-bd11-0c8a9be357ff"},"source":["countdict = {}\n","uniqqdict = {}\n","for qa in qalist:\n","  print(qa['source'])\n","  if qa['Question'] in uniqqdict:\n","    uniqqdict[qa['Question']][0] = uniqqdict[qa['Question']][0] + 1\n","  else :\n","    uniqqdict[qa['Question']] = [ 1 , qa['source']]\n","    \n","  if qa['source'] in countdict:\n","    countdict[qa['source']] = countdict[qa['source']] + 1\n","  else :\n","    countdict[qa['source']] = 1\n","\n"],"execution_count":14,"outputs":[{"output_type":"stream","text":["https://pytorch.org/docs/stable/generated/torch.atanh.html#torch.atanh\n","https://pytorch.org/docs/stable/generated/torch.logaddexp.html#torch.logaddexp\n","https://pytorch.org/docs/stable/testing.html\n","https://pytorch.org/docs/stable/testing.html\n","https://pytorch.org/docs/stable/testing.html\n","https://pytorch.org/docs/stable/testing.html\n","https://pytorch.org/docs/stable/testing.html\n","https://pytorch.org/docs/stable/testing.html\n","https://pytorch.org/docs/stable/testing.html\n","https://pytorch.org/docs/stable/testing.html\n","https://pytorch.org/docs/stable/testing.html\n","https://pytorch.org/docs/stable/generated/torch.diag.html#torch.diag\n","https://pytorch.org/docs/stable/generated/torch.diag.html#torch.diag\n","https://pytorch.org/docs/stable/torch.html#math-operations\n","https://pytorch.org/docs/stable/torch.overrides.html\n","https://pytorch.org/docs/stable/torch.overrides.html\n","https://pytorch.org/docs/stable/torch.overrides.html\n","https://pytorch.org/docs/stable/torch.overrides.html\n","https://pytorch.org/docs/stable/torch.overrides.html\n","https://pytorch.org/docs/stable/torch.overrides.html\n","https://pytorch.org/docs/stable/torch.overrides.html\n","https://pytorch.org/docs/stable/torch.overrides.html\n","https://pytorch.org/docs/stable/generated/torch.mvlgamma.html#torch.mvlgamma\n","https://pytorch.org/docs/stable/generated/torch.fake_quantize_per_channel_affine.html#torch.fake_quantize_per_channel_affine\n","https://pytorch.org/docs/stable/generated/torch.atleast_1d.html#torch.atleast_1d\n","https://pytorch.org/docs/stable/generated/torch.heaviside.html#torch.heaviside\n","https://pytorch.org/docs/stable/generated/torch.atleast_2d.html#torch.atleast_2d\n","https://pytorch.org/docs/stable/generated/torch.ldexp.html#torch.ldexp\n","https://pytorch.org/docs/stable/generated/torch.promote_types.html#torch.promote_types\n","https://pytorch.org/docs/stable/generated/torch.nextafter.html#torch.nextafter\n","https://pytorch.org/docs/stable/generated/torch.set_default_tensor_type.html#torch.set_default_tensor_type\n","https://pytorch.org/docs/stable/generated/torch.diagflat.html#torch.diagflat\n","https://pytorch.org/docs/stable/generated/torch.lu_unpack.html#torch.lu_unpack\n","https://pytorch.org/docs/stable/generated/torch.addmv.html#torch.addmv\n","https://pytorch.org/docs/stable/generated/torch.nanmedian.html#torch.nanmedian\n","https://pytorch.org/docs/stable/generated/torch.nanmedian.html#torch.nanmedian\n","https://pytorch.org/docs/stable/generated/torch.empty_strided.html#torch.empty_strided\n","https://pytorch.org/docs/stable/generated/torch.isnan.html#torch.isnan\n","https://pytorch.org/docs/stable/generated/torch.atleast_3d.html#torch.atleast_3d\n","https://pytorch.org/docs/stable/generated/torch.bitwise_xor.html#torch.bitwise_xor\n","https://pytorch.org/docs/stable/generated/torch.sinh.html#torch.sinh\n","https://pytorch.org/docs/stable/generated/torch.roll.html#torch.roll\n","https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile\n","https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk\n","https://pytorch.org/docs/stable/generated/torch.sqrt.html#torch.sqrt\n","https://pytorch.org/docs/stable/generated/torch.minimum.html#torch.minimum\n","https://pytorch.org/docs/stable/generated/torch.floor_divide.html#torch.floor_divide\n","https://pytorch.org/docs/stable/generated/torch.sgn.html#torch.sgn\n","https://pytorch.org/docs/stable/generated/torch.ones_like.html#torch.ones_like\n","https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad\n","https://pytorch.org/docs/stable/generated/torch.view_as_complex.html#torch.view_as_complex\n","https://pytorch.org/docs/stable/generated/torch.swapaxes.html#torch.swapaxes\n","https://pytorch.org/docs/stable/generated/torch.logical_xor.html#torch.logical_xor\n","https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype\n","https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype\n","https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype\n","https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype\n","https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype\n","https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype\n","https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype\n","https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype\n","https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype\n","https://pytorch.org/docs/stable/generated/torch.median.html#torch.median\n","https://pytorch.org/docs/stable/generated/torch.median.html#torch.median\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/onnx.html\n","https://pytorch.org/docs/stable/generated/torch.tril_indices.html#torch.tril_indices\n","https://pytorch.org/docs/stable/generated/torch.sub.html#torch.sub\n","https://pytorch.org/docs/stable/amp.html\n","https://pytorch.org/docs/stable/amp.html\n","https://pytorch.org/docs/stable/amp.html\n","https://pytorch.org/docs/stable/amp.html\n","https://pytorch.org/docs/stable/amp.html\n","https://pytorch.org/docs/stable/generated/torch.logdet.html#torch.logdet\n","https://pytorch.org/docs/stable/generated/torch.lt.html#torch.lt\n","https://pytorch.org/docs/stable/generated/torch.quantize_per_tensor.html#torch.quantize_per_tensor\n","https://pytorch.org/docs/stable/generated/torch.square.html#torch.square\n","https://pytorch.org/docs/stable/generated/torch.take.html#torch.take\n","https://pytorch.org/docs/stable/torch.html#torch.torch.default_generator\n","https://pytorch.org/docs/stable/generated/torch.normal.html#torch.normal\n","https://pytorch.org/docs/stable/generated/torch.normal.html#torch.normal\n","https://pytorch.org/docs/stable/generated/torch.normal.html#torch.normal\n","https://pytorch.org/docs/stable/generated/torch.normal.html#torch.normal\n","https://pytorch.org/docs/stable/generated/torch.std_mean.html#torch.std_mean\n","https://pytorch.org/docs/stable/generated/torch.swapdims.html#torch.swapdims\n","https://pytorch.org/docs/stable/generated/torch.dstack.html#torch.dstack\n","https://pytorch.org/docs/stable/special.html#torch.special.erfinv\n","https://pytorch.org/docs/stable/special.html#torch.special.erfinv\n","https://pytorch.org/docs/stable/special.html#torch.special.erfinv\n","https://pytorch.org/docs/stable/special.html#torch.special.erfinv\n","https://pytorch.org/docs/stable/special.html#torch.special.erfinv\n","https://pytorch.org/docs/stable/special.html#torch.special.erfinv\n","https://pytorch.org/docs/stable/special.html#torch.special.erfinv\n","https://pytorch.org/docs/stable/special.html#torch.special.erfinv\n","https://pytorch.org/docs/stable/special.html#torch.special.erfinv\n","https://pytorch.org/docs/stable/special.html#torch.special.erfinv\n","https://pytorch.org/docs/stable/special.html#torch.special.erfinv\n","https://pytorch.org/docs/stable/generated/torch.isneginf.html#torch.isneginf\n","https://pytorch.org/docs/stable/generated/torch.tensordot.html#torch.tensordot\n","https://pytorch.org/docs/stable/generated/torch.clamp.html#torch.clamp\n","https://pytorch.org/docs/stable/generated/torch.renorm.html#torch.renorm\n","https://pytorch.org/docs/stable/generated/torch.logcumsumexp.html#torch.logcumsumexp\n","https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat\n","https://pytorch.org/docs/stable/generated/torch.cdist.html#torch.cdist\n","https://pytorch.org/docs/stable/generated/torch.index_select.html#torch.index_select\n","https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc\n","https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc\n","https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc\n","https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc\n","https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc\n","https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc\n","https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc\n","https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc\n","https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc\n","https://pytorch.org/docs/stable/generated/torch.cholesky_solve.html#torch.cholesky_solve\n","https://pytorch.org/docs/stable/generated/torch.argmin.html#torch.argmin\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/distributions.html\n","https://pytorch.org/docs/stable/generated/torch.quasirandom.SobolEngine.html#torch.quasirandom.SobolEngine\n","https://pytorch.org/docs/stable/generated/torch.unique_consecutive.html#torch.unique_consecutive\n","https://pytorch.org/docs/stable/generated/torch.signbit.html#torch.signbit\n","https://pytorch.org/docs/stable/generated/torch.isinf.html#torch.isinf\n","https://pytorch.org/docs/stable/generated/torch.log1p.html#torch.log1p\n","https://pytorch.org/docs/stable/generated/torch.randperm.html#torch.randperm\n","https://pytorch.org/docs/stable/generated/torch.diff.html#torch.diff\n","https://pytorch.org/docs/stable/generated/torch.bernoulli.html#torch.bernoulli\n","https://pytorch.org/docs/stable/generated/torch.amin.html#torch.amin\n","https://pytorch.org/docs/stable/generated/torch.equal.html#torch.equal\n","https://pytorch.org/docs/stable/generated/torch.logical_not.html#torch.logical_not\n","https://pytorch.org/docs/stable/torch.html#spectral-ops\n","https://pytorch.org/docs/stable/generated/torch.acos.html#torch.acos\n","https://pytorch.org/docs/stable/torch.html#parallelism\n","https://pytorch.org/docs/stable/generated/torch.abs.html#torch.abs\n","https://pytorch.org/docs/stable/generated/torch.from_numpy.html#torch.from_numpy\n","https://pytorch.org/docs/stable/generated/torch.ravel.html#torch.ravel\n","https://pytorch.org/docs/stable/generated/torch.sinc.html#torch.sinc\n","https://pytorch.org/docs/stable/benchmark_utils.html\n","https://pytorch.org/docs/stable/benchmark_utils.html\n","https://pytorch.org/docs/stable/benchmark_utils.html\n","https://pytorch.org/docs/stable/generated/torch.save.html#torch.save\n","https://pytorch.org/docs/stable/generated/torch.set_grad_enabled.html#torch.set_grad_enabled\n","https://pytorch.org/docs/stable/generated/torch.combinations.html#torch.combinations\n","https://pytorch.org/docs/stable/generated/torch.mv.html#torch.mv\n","https://pytorch.org/docs/stable/generated/torch.amax.html#torch.amax\n","https://pytorch.org/docs/stable/generated/torch.numel.html#torch.numel\n","https://pytorch.org/docs/stable/generated/torch.logspace.html#torch.logspace\n","https://pytorch.org/docs/stable/generated/torch.acosh.html#torch.acosh\n","https://pytorch.org/docs/stable/package.html\n","https://pytorch.org/docs/stable/package.html\n","https://pytorch.org/docs/stable/package.html\n","https://pytorch.org/docs/stable/package.html\n","https://pytorch.org/docs/stable/package.html\n","https://pytorch.org/docs/stable/package.html\n","https://pytorch.org/docs/stable/package.html\n","https://pytorch.org/docs/stable/package.html\n","https://pytorch.org/docs/stable/package.html\n","https://pytorch.org/docs/stable/package.html\n","https://pytorch.org/docs/stable/package.html\n","https://pytorch.org/docs/stable/package.html\n","https://pytorch.org/docs/stable/package.html\n","https://pytorch.org/docs/stable/package.html\n","https://pytorch.org/docs/stable/package.html\n","https://pytorch.org/docs/stable/package.html\n","https://pytorch.org/docs/stable/package.html\n","https://pytorch.org/docs/stable/package.html\n","https://pytorch.org/docs/stable/package.html\n","https://pytorch.org/docs/stable/package.html\n","https://pytorch.org/docs/stable/package.html\n","https://pytorch.org/docs/stable/package.html\n","https://pytorch.org/docs/stable/package.html\n","https://pytorch.org/docs/stable/package.html\n","https://pytorch.org/docs/stable/package.html\n","https://pytorch.org/docs/stable/package.html\n","https://pytorch.org/docs/stable/package.html\n","https://pytorch.org/docs/stable/package.html\n","https://pytorch.org/docs/stable/package.html\n","https://pytorch.org/docs/stable/package.html\n","https://pytorch.org/docs/stable/package.html\n","https://pytorch.org/docs/stable/package.html\n","https://pytorch.org/docs/stable/package.html\n","https://pytorch.org/docs/stable/package.html\n","https://pytorch.org/docs/stable/generated/torch.cumsum.html#torch.cumsum\n","https://pytorch.org/docs/stable/generated/torch.linalg.matrix_power.html#torch.linalg.matrix_power\n","https://pytorch.org/docs/stable/generated/torch.linalg.matrix_power.html#torch.linalg.matrix_power\n","https://pytorch.org/docs/stable/torch.html#quasi-random-sampling\n","https://pytorch.org/docs/stable/profiler.html\n","https://pytorch.org/docs/stable/profiler.html\n","https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique\n","https://pytorch.org/docs/stable/generated/torch.asin.html#torch.asin\n","https://pytorch.org/docs/stable/optim.html\n","https://pytorch.org/docs/stable/optim.html\n","https://pytorch.org/docs/stable/optim.html\n","https://pytorch.org/docs/stable/optim.html\n","https://pytorch.org/docs/stable/optim.html\n","https://pytorch.org/docs/stable/optim.html\n","https://pytorch.org/docs/stable/optim.html\n","https://pytorch.org/docs/stable/optim.html\n","https://pytorch.org/docs/stable/optim.html\n","https://pytorch.org/docs/stable/optim.html\n","https://pytorch.org/docs/stable/optim.html\n","https://pytorch.org/docs/stable/optim.html\n","https://pytorch.org/docs/stable/optim.html\n","https://pytorch.org/docs/stable/generated/torch.nanquantile.html#torch.nanquantile\n","https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig\n","https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig\n","https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig\n","https://pytorch.org/docs/stable/generated/torch.gcd.html#torch.gcd\n","https://pytorch.org/docs/stable/generated/torch.matrix_exp.html#torch.matrix_exp\n","https://pytorch.org/docs/stable/hub.html\n","https://pytorch.org/docs/stable/hub.html\n","https://pytorch.org/docs/stable/hub.html\n","https://pytorch.org/docs/stable/hub.html\n","https://pytorch.org/docs/stable/hub.html\n","https://pytorch.org/docs/stable/hub.html\n","https://pytorch.org/docs/stable/hub.html\n","https://pytorch.org/docs/stable/hub.html\n","https://pytorch.org/docs/stable/generated/torch.log10.html#torch.log10\n","https://pytorch.org/docs/stable/generated/torch.fmin.html#torch.fmin\n","https://pytorch.org/docs/stable/generated/torch.outer.html#torch.outer\n","https://pytorch.org/docs/stable/generated/torch.tan.html#torch.tan\n","https://pytorch.org/docs/stable/generated/torch.addr.html#torch.addr\n","https://pytorch.org/docs/stable/generated/torch.cumprod.html#torch.cumprod\n","https://pytorch.org/docs/stable/generated/torch.inner.html#torch.inner\n","https://pytorch.org/docs/stable/generated/torch.isclose.html#torch.isclose\n","https://pytorch.org/docs/stable/generated/torch.rsqrt.html#torch.rsqrt\n","https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted\n","https://pytorch.org/docs/stable/generated/torch.block_diag.html#torch.block_diag\n","https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv\n","https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv\n","https://pytorch.org/docs/stable/generated/torch.bitwise_or.html#torch.bitwise_or\n","https://pytorch.org/docs/stable/generated/torch.igammac.html#torch.igammac\n","https://pytorch.org/docs/stable/generated/torch.randint.html#torch.randint\n","https://pytorch.org/docs/stable/generated/torch.masked_select.html#torch.masked_select\n","https://pytorch.org/docs/stable/torch.html#tensors\n","https://pytorch.org/docs/stable/generated/torch.mm.html#torch.mm\n","https://pytorch.org/docs/stable/generated/torch.movedim.html#torch.movedim\n","https://pytorch.org/docs/stable/generated/torch.addmm.html#torch.addmm\n","https://pytorch.org/docs/stable/torch.html#serialization\n","https://pytorch.org/docs/stable/generated/torch.prod.html#torch.prod\n","https://pytorch.org/docs/stable/generated/torch.prod.html#torch.prod\n","https://pytorch.org/docs/stable/generated/torch.where.html#torch.where\n","https://pytorch.org/docs/stable/generated/torch.empty_like.html#torch.empty_like\n","https://pytorch.org/docs/stable/generated/torch.bmm.html#torch.bmm\n","https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn\n","https://pytorch.org/docs/stable/generated/torch.poisson.html#torch.poisson\n","https://pytorch.org/docs/stable/generated/torch.vstack.html#torch.vstack\n","https://pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape\n","https://pytorch.org/docs/stable/generated/torch.bucketize.html#torch.bucketize\n","https://pytorch.org/docs/stable/generated/torch.copysign.html#torch.copysign\n","https://pytorch.org/docs/stable/generated/torch.kthvalue.html#torch.kthvalue\n","https://pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky\n","https://pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky\n","https://pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky\n","https://pytorch.org/docs/stable/generated/torch.histc.html#torch.histc\n","https://pytorch.org/docs/stable/torch.html#creation-ops\n","https://pytorch.org/docs/stable/torch.html#torch\n","https://pytorch.org/docs/stable/generated/torch.eq.html#torch.eq\n","https://pytorch.org/docs/stable/torch.html#utilities\n","https://pytorch.org/docs/stable/generated/torch.unsqueeze.html#torch.unsqueeze\n","https://pytorch.org/docs/stable/torch.html#random-sampling\n","https://pytorch.org/docs/stable/generated/torch.matrix_rank.html#torch.matrix_rank\n","https://pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax\n","https://pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax\n","https://pytorch.org/docs/stable/generated/torch.imag.html#torch.imag\n","https://pytorch.org/docs/stable/generated/torch.exp.html#torch.exp\n","https://pytorch.org/docs/stable/generated/torch.angle.html#torch.angle\n","https://pytorch.org/docs/stable/special.html#torch.special.exp2\n","https://pytorch.org/docs/stable/special.html#torch.special.exp2\n","https://pytorch.org/docs/stable/special.html#torch.special.exp2\n","https://pytorch.org/docs/stable/special.html#torch.special.exp2\n","https://pytorch.org/docs/stable/special.html#torch.special.exp2\n","https://pytorch.org/docs/stable/special.html#torch.special.exp2\n","https://pytorch.org/docs/stable/special.html#torch.special.exp2\n","https://pytorch.org/docs/stable/special.html#torch.special.exp2\n","https://pytorch.org/docs/stable/special.html#torch.special.exp2\n","https://pytorch.org/docs/stable/special.html#torch.special.exp2\n","https://pytorch.org/docs/stable/special.html#torch.special.exp2\n","https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_\n","https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_\n","https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_\n","https://pytorch.org/docs/stable/generated/torch.range.html#torch.range\n","https://pytorch.org/docs/stable/generated/torch.broadcast_to.html#torch.broadcast_to\n","https://pytorch.org/docs/stable/generated/torch.squeeze.html#torch.squeeze\n","https://pytorch.org/docs/stable/notes/randomness.html\n","https://pytorch.org/docs/stable/notes/randomness.html\n","https://pytorch.org/docs/stable/notes/randomness.html\n","https://pytorch.org/docs/stable/notes/randomness.html\n","https://pytorch.org/docs/stable/notes/randomness.html\n","https://pytorch.org/docs/stable/notes/randomness.html\n","https://pytorch.org/docs/stable/generated/torch.dsplit.html#torch.dsplit\n","https://pytorch.org/docs/stable/generated/torch.dsplit.html#torch.dsplit\n","https://pytorch.org/docs/stable/generated/torch.can_cast.html#torch.can_cast\n","https://pytorch.org/docs/stable/generated/torch.linalg.slogdet.html#torch.linalg.slogdet\n","https://pytorch.org/docs/stable/generated/torch.qr.html#torch.qr\n","https://pytorch.org/docs/stable/generated/torch.qr.html#torch.qr\n","https://pytorch.org/docs/stable/generated/torch.qr.html#torch.qr\n","https://pytorch.org/docs/stable/generated/torch.nan_to_num.html#torch.nan_to_num\n","https://pytorch.org/docs/stable/generated/torch.div.html#torch.div\n","https://pytorch.org/docs/stable/jit.html\n","https://pytorch.org/docs/stable/jit.html\n","https://pytorch.org/docs/stable/jit.html\n","https://pytorch.org/docs/stable/jit.html\n","https://pytorch.org/docs/stable/jit.html\n","https://pytorch.org/docs/stable/jit.html\n","https://pytorch.org/docs/stable/jit.html\n","https://pytorch.org/docs/stable/jit.html\n","https://pytorch.org/docs/stable/jit.html\n","https://pytorch.org/docs/stable/jit.html\n","https://pytorch.org/docs/stable/jit.html\n","https://pytorch.org/docs/stable/jit.html\n","https://pytorch.org/docs/stable/jit.html\n","https://pytorch.org/docs/stable/jit.html\n","https://pytorch.org/docs/stable/jit.html\n","https://pytorch.org/docs/stable/jit.html\n","https://pytorch.org/docs/stable/jit.html\n","https://pytorch.org/docs/stable/jit.html\n","https://pytorch.org/docs/stable/jit.html\n","https://pytorch.org/docs/stable/jit.html\n","https://pytorch.org/docs/stable/jit.html\n","https://pytorch.org/docs/stable/jit.html\n","https://pytorch.org/docs/stable/jit.html\n","https://pytorch.org/docs/stable/jit.html\n","https://pytorch.org/docs/stable/jit.html\n","https://pytorch.org/docs/stable/jit.html\n","https://pytorch.org/docs/stable/generated/torch.column_stack.html#torch.column_stack\n","https://pytorch.org/docs/stable/generated/torch.is_tensor.html\n","https://pytorch.org/docs/stable/generated/torch.set_flush_denormal.html#torch.set_flush_denormal\n","https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum\n","https://pytorch.org/docs/stable/generated/torch.lerp.html#torch.lerp\n","https://pytorch.org/docs/stable/generated/torch.maximum.html#torch.maximum\n","https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm\n","https://pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros\n","https://pytorch.org/docs/stable/generated/torch.any.html#torch.any\n","https://pytorch.org/docs/stable/generated/torch.any.html#torch.any\n","https://pytorch.org/docs/stable/generated/torch.lu_solve.html#torch.lu_solve\n","https://pytorch.org/docs/stable/generated/torch.argsort.html#torch.argsort\n","https://pytorch.org/docs/stable/generated/torch.cummax.html#torch.cummax\n","https://pytorch.org/docs/stable/generated/torch.gather.html#torch.gather\n","https://pytorch.org/docs/stable/generated/torch.gather.html#torch.gather\n","https://pytorch.org/docs/stable/generated/torch.isreal.html#torch.isreal\n","https://pytorch.org/docs/stable/nn.init.html\n","https://pytorch.org/docs/stable/nn.init.html\n","https://pytorch.org/docs/stable/nn.init.html\n","https://pytorch.org/docs/stable/nn.init.html\n","https://pytorch.org/docs/stable/nn.init.html\n","https://pytorch.org/docs/stable/nn.init.html\n","https://pytorch.org/docs/stable/nn.init.html\n","https://pytorch.org/docs/stable/nn.init.html\n","https://pytorch.org/docs/stable/nn.init.html\n","https://pytorch.org/docs/stable/nn.init.html\n","https://pytorch.org/docs/stable/nn.init.html\n","https://pytorch.org/docs/stable/nn.init.html\n","https://pytorch.org/docs/stable/nn.init.html\n","https://pytorch.org/docs/stable/nn.init.html\n","https://pytorch.org/docs/stable/generated/torch.addbmm.html#torch.addbmm\n","https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv\n","https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv\n","https://pytorch.org/docs/stable/generated/torch.result_type.html#torch.result_type\n","https://pytorch.org/docs/stable/generated/torch.set_default_dtype.html#torch.set_default_dtype\n","https://pytorch.org/docs/stable/generated/torch.cummin.html#torch.cummin\n","https://pytorch.org/docs/stable/generated/torch.mode.html#torch.mode\n","https://pytorch.org/docs/stable/generated/torch.is_tensor.html#torch.is_tensor\n","https://pytorch.org/docs/stable/distributed.optim.html\n","https://pytorch.org/docs/stable/generated/torch.arange.html#torch.arange\n","https://pytorch.org/docs/stable/generated/torch.hypot.html#torch.hypot\n","https://pytorch.org/docs/stable/torch.html#in-place-random-sampling\n","https://pytorch.org/docs/stable/generated/torch.as_strided.html#torch.as_strided\n","https://pytorch.org/docs/stable/generated/torch.transpose.html#torch.transpose\n","https://pytorch.org/docs/stable/generated/torch.cross.html#torch.cross\n","https://pytorch.org/docs/stable/generated/torch.rot90.html#torch.rot90\n","https://pytorch.org/docs/stable/torch.html#pointwise-ops\n","https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_\n","https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_\n","https://pytorch.org/docs/stable/generated/torch.triu.html#torch.triu\n","https://pytorch.org/docs/stable/generated/torch.polygamma.html#torch.polygamma\n","https://pytorch.org/docs/stable/generated/torch.var.html#torch.var\n","https://pytorch.org/docs/stable/special.html#torch.special.erf\n","https://pytorch.org/docs/stable/special.html#torch.special.erf\n","https://pytorch.org/docs/stable/special.html#torch.special.erf\n","https://pytorch.org/docs/stable/special.html#torch.special.erf\n","https://pytorch.org/docs/stable/special.html#torch.special.erf\n","https://pytorch.org/docs/stable/special.html#torch.special.erf\n","https://pytorch.org/docs/stable/special.html#torch.special.erf\n","https://pytorch.org/docs/stable/special.html#torch.special.erf\n","https://pytorch.org/docs/stable/special.html#torch.special.erf\n","https://pytorch.org/docs/stable/special.html#torch.special.erf\n","https://pytorch.org/docs/stable/special.html#torch.special.erf\n","https://pytorch.org/docs/stable/generated/torch.allclose.html#torch.allclose\n","https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n","https://pytorch.org/docs/stable/generated/torch.count_nonzero.html#torch.count_nonzero\n","https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid\n","https://pytorch.org/docs/stable/generated/torch.is_nonzero.html#torch.is_nonzero\n","https://pytorch.org/docs/stable/tensors.html\n","https://pytorch.org/docs/stable/tensors.html\n","https://pytorch.org/docs/stable/tensors.html\n","https://pytorch.org/docs/stable/tensors.html\n","https://pytorch.org/docs/stable/tensors.html\n","https://pytorch.org/docs/stable/sparse.html\n","https://pytorch.org/docs/stable/sparse.html\n","https://pytorch.org/docs/stable/sparse.html\n","https://pytorch.org/docs/stable/sparse.html\n","https://pytorch.org/docs/stable/sparse.html\n","https://pytorch.org/docs/stable/sparse.html\n","https://pytorch.org/docs/stable/sparse.html\n","https://pytorch.org/docs/stable/sparse.html\n","https://pytorch.org/docs/stable/sparse.html\n","https://pytorch.org/docs/stable/sparse.html\n","https://pytorch.org/docs/stable/sparse.html\n","https://pytorch.org/docs/stable/sparse.html\n","https://pytorch.org/docs/stable/sparse.html\n","https://pytorch.org/docs/stable/sparse.html\n","https://pytorch.org/docs/stable/sparse.html\n","https://pytorch.org/docs/stable/sparse.html\n","https://pytorch.org/docs/stable/sparse.html\n","https://pytorch.org/docs/stable/sparse.html\n","https://pytorch.org/docs/stable/sparse.html\n","https://pytorch.org/docs/stable/generated/torch.sin.html#torch.sin\n","https://pytorch.org/docs/stable/generated/torch.gradient.html#torch.gradient\n","https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount\n","https://pytorch.org/docs/stable/generated/torch.conj.html#torch.conj\n","https://pytorch.org/docs/stable/generated/torch.positive.html#torch.positive\n","https://pytorch.org/docs/stable/generated/torch.atan.html#torch.atan\n","https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor\n","https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices\n","https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder\n","https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye\n","https://pytorch.org/docs/stable/generated/torch.trunc.html#torch.trunc\n","https://pytorch.org/docs/stable/generated/torch.log2.html#torch.log2\n","https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor\n","https://pytorch.org/docs/stable/generated/torch.logical_or.html#torch.logical_or\n","https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve\n","https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve\n","https://pytorch.org/docs/stable/special.html#torch.special.logit\n","https://pytorch.org/docs/stable/special.html#torch.special.logit\n","https://pytorch.org/docs/stable/special.html#torch.special.logit\n","https://pytorch.org/docs/stable/special.html#torch.special.logit\n","https://pytorch.org/docs/stable/special.html#torch.special.logit\n","https://pytorch.org/docs/stable/special.html#torch.special.logit\n","https://pytorch.org/docs/stable/special.html#torch.special.logit\n","https://pytorch.org/docs/stable/special.html#torch.special.logit\n","https://pytorch.org/docs/stable/special.html#torch.special.logit\n","https://pytorch.org/docs/stable/special.html#torch.special.logit\n","https://pytorch.org/docs/stable/special.html#torch.special.logit\n","https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander\n","https://pytorch.org/docs/stable/generated/torch.frexp.html#torch.frexp\n","https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv\n","https://pytorch.org/docs/stable/generated/torch.moveaxis.html#torch.moveaxis\n","https://pytorch.org/docs/stable/generated/torch.lgamma.html#torch.lgamma\n","https://pytorch.org/docs/stable/futures.html\n","https://pytorch.org/docs/stable/futures.html\n","https://pytorch.org/docs/stable/futures.html\n","https://pytorch.org/docs/stable/futures.html\n","https://pytorch.org/docs/stable/futures.html\n","https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod\n","https://pytorch.org/docs/stable/generated/torch.dist.html#torch.dist\n","https://pytorch.org/docs/stable/generated/torch.empty.html#torch.empty\n","https://pytorch.org/docs/stable/generated/torch.nansum.html#torch.nansum\n","https://pytorch.org/docs/stable/generated/torch.nansum.html#torch.nansum\n","https://pytorch.org/docs/stable/generated/torch.diagonal.html#torch.diagonal\n","https://pytorch.org/docs/stable/generated/torch.le.html#torch.le\n","https://pytorch.org/docs/stable/torch.html#locally-disabling-gradient-computation\n","https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand\n","https://pytorch.org/docs/stable/generated/torch.take_along_dim.html#torch.take_along_dim\n","https://pytorch.org/docs/stable/generated/torch.isposinf.html#torch.isposinf\n","https://pytorch.org/docs/stable/generated/torch.logical_and.html#torch.logical_and\n","https://pytorch.org/docs/stable/generated/torch.neg.html#torch.neg\n","https://pytorch.org/docs/stable/generated/torch.trace.html#torch.trace\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/generated/torch.ceil.html#torch.ceil\n","https://pytorch.org/docs/stable/torch.html#comparison-ops\n","https://pytorch.org/docs/stable/generated/torch.linalg.det.html#torch.linalg.det\n","https://pytorch.org/docs/stable/generated/torch.nonzero.html#torch.nonzero\n","https://pytorch.org/docs/stable/generated/torch.reciprocal.html#torch.reciprocal\n","https://pytorch.org/docs/stable/torch.html#\n","https://pytorch.org/docs/stable/generated/torch.sort.html#torch.sort\n","https://pytorch.org/docs/stable/generated/torch.float_power.html#torch.float_power\n","https://pytorch.org/docs/stable/generated/torch.diag_embed.html#torch.diag_embed\n","https://pytorch.org/docs/stable/generated/torch.frac.html#torch.frac\n","https://pytorch.org/docs/stable/generated/torch.linalg.householder_product.html#torch.linalg.householder_product\n","https://pytorch.org/docs/stable/generated/torch.tril.html#torch.tril\n","https://pytorch.org/docs/stable/generated/torch.i0.html#torch.i0\n","https://pytorch.org/docs/stable/generated/torch.hstack.html#torch.hstack\n","https://pytorch.org/docs/stable/generated/torch.pow.html#torch.pow\n","https://pytorch.org/docs/stable/generated/torch.pow.html#torch.pow\n","https://pytorch.org/docs/stable/special.html\n","https://pytorch.org/docs/stable/special.html\n","https://pytorch.org/docs/stable/special.html\n","https://pytorch.org/docs/stable/special.html\n","https://pytorch.org/docs/stable/special.html\n","https://pytorch.org/docs/stable/special.html\n","https://pytorch.org/docs/stable/special.html\n","https://pytorch.org/docs/stable/special.html\n","https://pytorch.org/docs/stable/special.html\n","https://pytorch.org/docs/stable/special.html\n","https://pytorch.org/docs/stable/special.html\n","https://pytorch.org/docs/stable/generated/torch.floor.html#torch.floor\n","https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode\n","https://pytorch.org/docs/stable/generated/torch.as_tensor.html#torch.as_tensor\n","https://pytorch.org/docs/stable/generated/torch.trapz.html#torch.trapz\n","https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul\n","https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul\n","https://pytorch.org/docs/stable/generated/torch.logsumexp.html#torch.logsumexp\n","https://pytorch.org/docs/stable/generated/torch.max.html#torch.max\n","https://pytorch.org/docs/stable/generated/torch.max.html#torch.max\n","https://pytorch.org/docs/stable/model_zoo.html\n","https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean\n","https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean\n","https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile\n","https://pytorch.org/docs/stable/generated/torch.vdot.html#torch.vdot\n","https://pytorch.org/docs/stable/generated/torch.asinh.html#torch.asinh\n","https://pytorch.org/docs/stable/generated/torch.lstsq.html#torch.lstsq\n","https://pytorch.org/docs/stable/generated/torch.lstsq.html#torch.lstsq\n","https://pytorch.org/docs/stable/generated/torch.view_as_real.html#torch.view_as_real\n","https://pytorch.org/docs/stable/generated/torch.isfinite.html#torch.isfinite\n","https://pytorch.org/docs/stable/generated/torch.flipud.html#torch.flipud\n","https://pytorch.org/docs/stable/generated/torch.full.html#torch.full\n","https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator\n","https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator\n","https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator\n","https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator\n","https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator\n","https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator\n","https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator\n","https://pytorch.org/docs/stable/generated/torch.deg2rad.html#torch.deg2rad\n","https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean\n","https://pytorch.org/docs/stable/generated/torch.complex.html#torch.complex\n","https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve\n","https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones\n","https://pytorch.org/docs/stable/generated/torch.t.html#torch.t\n","https://pytorch.org/docs/stable/special.html#torch.special.expm1\n","https://pytorch.org/docs/stable/special.html#torch.special.expm1\n","https://pytorch.org/docs/stable/special.html#torch.special.expm1\n","https://pytorch.org/docs/stable/special.html#torch.special.expm1\n","https://pytorch.org/docs/stable/special.html#torch.special.expm1\n","https://pytorch.org/docs/stable/special.html#torch.special.expm1\n","https://pytorch.org/docs/stable/special.html#torch.special.expm1\n","https://pytorch.org/docs/stable/special.html#torch.special.expm1\n","https://pytorch.org/docs/stable/special.html#torch.special.expm1\n","https://pytorch.org/docs/stable/special.html#torch.special.expm1\n","https://pytorch.org/docs/stable/special.html#torch.special.expm1\n","https://pytorch.org/docs/stable/generated/torch.gt.html#torch.gt\n","https://pytorch.org/docs/stable/generated/torch.round.html#torch.round\n","https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank\n","https://pytorch.org/docs/stable/generated/torch.narrow.html#torch.narrow\n","https://pytorch.org/docs/stable/generated/torch.min.html#torch.min\n","https://pytorch.org/docs/stable/generated/torch.min.html#torch.min\n","https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial\n","https://pytorch.org/docs/stable/notes/modules.html\n","https://pytorch.org/docs/stable/notes/modules.html\n","https://pytorch.org/docs/stable/notes/modules.html\n","https://pytorch.org/docs/stable/notes/modules.html\n","https://pytorch.org/docs/stable/notes/modules.html\n","https://pytorch.org/docs/stable/notes/modules.html\n","https://pytorch.org/docs/stable/notes/modules.html\n","https://pytorch.org/docs/stable/notes/modules.html\n","https://pytorch.org/docs/stable/notes/modules.html\n","https://pytorch.org/docs/stable/notes/modules.html\n","https://pytorch.org/docs/stable/notes/modules.html\n","https://pytorch.org/docs/stable/notes/modules.html\n","https://pytorch.org/docs/stable/notes/modules.html\n","https://pytorch.org/docs/stable/notes/modules.html\n","https://pytorch.org/docs/stable/notes/modules.html\n","https://pytorch.org/docs/stable/notes/modules.html\n","https://pytorch.org/docs/stable/notes/modules.html\n","https://pytorch.org/docs/stable/tensors.html#torch.Tensor\n","https://pytorch.org/docs/stable/tensors.html#torch.Tensor\n","https://pytorch.org/docs/stable/tensors.html#torch.Tensor\n","https://pytorch.org/docs/stable/tensors.html#torch.Tensor\n","https://pytorch.org/docs/stable/tensors.html#torch.Tensor\n","https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip\n","https://pytorch.org/docs/stable/special.html#torch.special.erfc\n","https://pytorch.org/docs/stable/special.html#torch.special.erfc\n","https://pytorch.org/docs/stable/special.html#torch.special.erfc\n","https://pytorch.org/docs/stable/special.html#torch.special.erfc\n","https://pytorch.org/docs/stable/special.html#torch.special.erfc\n","https://pytorch.org/docs/stable/special.html#torch.special.erfc\n","https://pytorch.org/docs/stable/special.html#torch.special.erfc\n","https://pytorch.org/docs/stable/special.html#torch.special.erfc\n","https://pytorch.org/docs/stable/special.html#torch.special.erfc\n","https://pytorch.org/docs/stable/special.html#torch.special.erfc\n","https://pytorch.org/docs/stable/special.html#torch.special.erfc\n","https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd\n","https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd\n","https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd\n","https://pytorch.org/docs/stable/generated/torch.real.html#torch.real\n","https://pytorch.org/docs/stable/generated/torch.xlogy.html#torch.xlogy\n","https://pytorch.org/docs/stable/generated/torch.quantize_per_channel.html#torch.quantize_per_channel\n","https://pytorch.org/docs/stable/generated/torch.enable_grad.html#torch.enable_grad\n","https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig\n","https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig\n","https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig\n","https://pytorch.org/docs/stable/data.html\n","https://pytorch.org/docs/stable/data.html\n","https://pytorch.org/docs/stable/data.html\n","https://pytorch.org/docs/stable/data.html\n","https://pytorch.org/docs/stable/data.html\n","https://pytorch.org/docs/stable/data.html\n","https://pytorch.org/docs/stable/data.html\n","https://pytorch.org/docs/stable/data.html\n","https://pytorch.org/docs/stable/data.html\n","https://pytorch.org/docs/stable/data.html\n","https://pytorch.org/docs/stable/data.html\n","https://pytorch.org/docs/stable/data.html\n","https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum\n","https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum\n","https://pytorch.org/docs/stable/generated/torch.bitwise_not.html#torch.bitwise_not\n","https://pytorch.org/docs/stable/generated/torch.vsplit.html#torch.vsplit\n","https://pytorch.org/docs/stable/generated/torch.hsplit.html#torch.hsplit\n","https://pytorch.org/docs/stable/special.html#torch.special.expit\n","https://pytorch.org/docs/stable/special.html#torch.special.expit\n","https://pytorch.org/docs/stable/special.html#torch.special.expit\n","https://pytorch.org/docs/stable/special.html#torch.special.expit\n","https://pytorch.org/docs/stable/special.html#torch.special.expit\n","https://pytorch.org/docs/stable/special.html#torch.special.expit\n","https://pytorch.org/docs/stable/special.html#torch.special.expit\n","https://pytorch.org/docs/stable/special.html#torch.special.expit\n","https://pytorch.org/docs/stable/special.html#torch.special.expit\n","https://pytorch.org/docs/stable/special.html#torch.special.expit\n","https://pytorch.org/docs/stable/special.html#torch.special.expit\n","https://pytorch.org/docs/stable/generated/torch.fliplr.html#torch.fliplr\n","https://pytorch.org/docs/stable/generated/torch.cosh.html#torch.cosh\n","https://pytorch.org/docs/stable/generated/torch.rad2deg.html#torch.rad2deg\n","https://pytorch.org/docs/stable/generated/torch.broadcast_shapes.html#torch.broadcast_shapes\n","https://pytorch.org/docs/stable/generated/torch.get_default_dtype.html#torch.get_default_dtype\n","https://pytorch.org/docs/stable/generated/torch.cartesian_prod.html#torch.cartesian_prod\n","https://pytorch.org/docs/stable/torch.html#generators\n","https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like\n","https://pytorch.org/docs/stable/generated/torch.flatten.html#torch.flatten\n","https://pytorch.org/docs/stable/generated/torch.bitwise_and.html#torch.bitwise_and\n","https://pytorch.org/docs/stable/generated/torch.digamma.html#torch.digamma\n","https://pytorch.org/docs/stable/torch.html#indexing-slicing-joining-mutating-ops\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/fx.html\n","https://pytorch.org/docs/stable/generated/torch.split.html#torch.split\n","https://pytorch.org/docs/stable/generated/torch.sign.html#torch.sign\n","https://pytorch.org/docs/stable/cpp_extension.html\n","https://pytorch.org/docs/stable/cpp_extension.html\n","https://pytorch.org/docs/stable/cpp_extension.html\n","https://pytorch.org/docs/stable/cpp_extension.html\n","https://pytorch.org/docs/stable/torch.html#other-operations\n","https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave\n","https://pytorch.org/docs/stable/generated/torch.add.html#torch.add\n","https://pytorch.org/docs/stable/generated/torch.add.html#torch.add\n","https://pytorch.org/docs/stable/generated/torch.kron.html#torch.kron\n","https://pytorch.org/docs/stable/generated/torch.polar.html#torch.polar\n","https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms\n","https://pytorch.org/docs/stable/generated/torch.addcmul.html#torch.addcmul\n","https://pytorch.org/docs/stable/generated/torch.fmax.html#torch.fmax\n","https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split\n","https://pytorch.org/docs/stable/generated/torch.std.html#torch.std\n","https://pytorch.org/docs/stable/generated/torch.cos.html#torch.cos\n","https://pytorch.org/docs/stable/quantization.html\n","https://pytorch.org/docs/stable/quantization.html\n","https://pytorch.org/docs/stable/quantization.html\n","https://pytorch.org/docs/stable/quantization.html\n","https://pytorch.org/docs/stable/quantization.html\n","https://pytorch.org/docs/stable/quantization.html\n","https://pytorch.org/docs/stable/quantization.html\n","https://pytorch.org/docs/stable/quantization.html\n","https://pytorch.org/docs/stable/quantization.html\n","https://pytorch.org/docs/stable/quantization.html\n","https://pytorch.org/docs/stable/quantization.html\n","https://pytorch.org/docs/stable/generated/torch.fake_quantize_per_tensor_affine.html#torch.fake_quantize_per_tensor_affine\n","https://pytorch.org/docs/stable/generated/torch.baddbmm.html#torch.baddbmm\n","https://pytorch.org/docs/stable/generated/torch.ge.html#torch.ge\n","https://pytorch.org/docs/stable/generated/torch.all.html#torch.all\n","https://pytorch.org/docs/stable/generated/torch.all.html#torch.all\n","https://pytorch.org/docs/stable/generated/torch.dot.html#torch.dot\n","https://pytorch.org/docs/stable/tensorboard.html\n","https://pytorch.org/docs/stable/tensorboard.html\n","https://pytorch.org/docs/stable/tensorboard.html\n","https://pytorch.org/docs/stable/tensorboard.html\n","https://pytorch.org/docs/stable/tensorboard.html\n","https://pytorch.org/docs/stable/tensorboard.html\n","https://pytorch.org/docs/stable/tensorboard.html\n","https://pytorch.org/docs/stable/tensorboard.html\n","https://pytorch.org/docs/stable/tensorboard.html\n","https://pytorch.org/docs/stable/tensorboard.html\n","https://pytorch.org/docs/stable/tensorboard.html\n","https://pytorch.org/docs/stable/tensorboard.html\n","https://pytorch.org/docs/stable/tensorboard.html\n","https://pytorch.org/docs/stable/tensorboard.html\n","https://pytorch.org/docs/stable/tensorboard.html\n","https://pytorch.org/docs/stable/generated/torch.linspace.html#torch.linspace\n","https://pytorch.org/docs/stable/generated/torch.lcm.html#torch.lcm\n","https://pytorch.org/docs/stable/autograd.html\n","https://pytorch.org/docs/stable/autograd.html\n","https://pytorch.org/docs/stable/autograd.html\n","https://pytorch.org/docs/stable/autograd.html\n","https://pytorch.org/docs/stable/autograd.html\n","https://pytorch.org/docs/stable/autograd.html\n","https://pytorch.org/docs/stable/generated/torch.chain_matmul.html#torch.chain_matmul\n","https://pytorch.org/docs/stable/generated/torch.tanh.html#torch.tanh\n","https://pytorch.org/docs/stable/generated/torch.atan2.html#torch.atan2\n","https://pytorch.org/docs/stable/torch.html#reduction-ops\n","https://pytorch.org/docs/stable/generated/torch.ne.html#torch.ne\n","https://pytorch.org/docs/stable/generated/torch.msort.html#torch.msort\n","https://pytorch.org/docs/stable/bottleneck.html\n","https://pytorch.org/docs/stable/checkpoint.html\n","https://pytorch.org/docs/stable/generated/torch.unbind.html#torch.unbind\n","https://pytorch.org/docs/stable/generated/torch.cholesky_inverse.html#torch.cholesky_inverse\n","https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu\n","https://pytorch.org/docs/stable/generated/torch.igamma.html#torch.igamma\n","https://pytorch.org/docs/stable/generated/torch.load.html#torch.load\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FI6BZLZaBnAD","executionInfo":{"status":"ok","timestamp":1628683293009,"user_tz":-330,"elapsed":48,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}},"outputId":"88b123a3-5c0c-4f0d-8cea-7c572aad0fb2"},"source":["len(uniqqdict) , len(qalist)"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(750, 883)"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MoY6DkvnGpyI","executionInfo":{"status":"ok","timestamp":1628683293010,"user_tz":-330,"elapsed":30,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}},"outputId":"a0cee398-2633-4734-ba8b-4eabe3d7f93d"},"source":["i = 1\n","for  qa in qalist:\n","  if qa['source'] == 'https://pytorch.org/docs/stable/fx.html':\n","    print('-'*30,'\\n', i , qa['Question'] ,'\\n','*'*30, '\\n' , qa['context'])\n","    i += 1\n"],"execution_count":16,"outputs":[{"output_type":"stream","text":["------------------------------ \n"," 1 How to use This feature is under a Beta release and its API may change.FX is a toolkit for developers to use to transformnn.Moduleinstances. FX consists of three main components: asymbolic tracer,anintermediate representation, andPython code generation. A\n","demonstration of these components in action:, give an example? \n"," ****************************** \n","  FX is a toolkit for developers to use to transformnn.Moduleinstances. FX consists of three main components: asymbolic tracer,anintermediate representation, andPython code generation. A\n","demonstration of these components in action:\n","------------------------------ \n"," 2 How to use What is an FX transform? Essentially, it’s a function that looks like this., give an example? \n"," ****************************** \n","  What is an FX transform? Essentially, it’s a function that looks like this.\n","------------------------------ \n"," 3 How to use NoteIt is also possible to modify an existingGraphModuleinstead of\n","creating a new one, like so:, give an example? \n"," ****************************** \n","  It is also possible to modify an existingGraphModuleinstead of\n","creating a new one, like so:\n","------------------------------ \n"," 4 How to use Full treatment of the semantics of graphs can be found in theGraphdocumentation, but we are going to cover the basics here. AGraphis\n","a data structure that represents a method on aGraphModule. The\n","information that this requires is:All three of these concepts are represented withNodeinstances.\n","Let’s see what we mean by that with a short example:, give an example? \n"," ****************************** \n","  All three of these concepts are represented withNodeinstances.\n","Let’s see what we mean by that with a short example:\n","------------------------------ \n"," 5 How to use One approach to building this newGraphis to directly manipulate your old\n","one. To aid in this, we can simply take theGraphwe obtain from symbolic\n","tracing and modify it. For example, let’s say we desire to replacetorch.add()calls withtorch.mul()calls., give an example? \n"," ****************************** \n","  One approach to building this newGraphis to directly manipulate your old\n","one. To aid in this, we can simply take theGraphwe obtain from symbolic\n","tracing and modify it. For example, let’s say we desire to replacetorch.add()calls withtorch.mul()calls.\n","------------------------------ \n"," 6 How to use One approach to building this newGraphis to directly manipulate your old\n","one. To aid in this, we can simply take theGraphwe obtain from symbolic\n","tracing and modify it. For example, let’s say we desire to replacetorch.add()calls withtorch.mul()calls.We can also do more involvedGraphrewrites, such as\n","deleting or appending nodes. To aid in these transformations,\n","FX has utility functions for transforming the graph that can\n","be found in theGraphdocumentation. An\n","example of using these APIs to append atorch.relu()call\n","can be found below., give an example? \n"," ****************************** \n","  We can also do more involvedGraphrewrites, such as\n","deleting or appending nodes. To aid in these transformations,\n","FX has utility functions for transforming the graph that can\n","be found in theGraphdocumentation. An\n","example of using these APIs to append atorch.relu()call\n","can be found below.\n","------------------------------ \n"," 7 How to use Another way of manipulatingGraphs is by reusing theProxymachinery used in symbolic tracing. For example, let’s\n","imagine that we wanted to write a transformation that decomposed\n","PyTorch functions into smaller operations. It would transform everyF.relu(x)call into(x>0)*x. One possibility would be to\n","perform the requisite graph rewriting to insert the comparison and\n","multiplication after theF.relu, and then clean up the originalF.relu. However, we can automate this process by usingProxyobjects to automatically record operations into theGraph.To use this method, we write the operations that we want inserted as regular\n","PyTorch code and invoke that code withProxyobjects as arugments.\n","TheseProxyobjects will capture the operations that are performed\n","on them and append them to theGraph., give an example? \n"," ****************************** \n","  To use this method, we write the operations that we want inserted as regular\n","PyTorch code and invoke that code withProxyobjects as arugments.\n","TheseProxyobjects will capture the operations that are performed\n","on them and append them to theGraph.\n","------------------------------ \n"," 8 How to use A useful code organizational pattern in FX is to loop over all theNodes\n","in aGraphand execute them. This can be used for several things including\n","runtime analysis of values flowing through the graph or transformation of the code\n","via retracing withProxys. For example, suppose we want to run aGraphModuleand record thetorch.Tensorshape and dtype\n","properties on the nodes as we see them at runtime. That might look like:, give an example? \n"," ****************************** \n","  A useful code organizational pattern in FX is to loop over all theNodes\n","in aGraphand execute them. This can be used for several things including\n","runtime analysis of values flowing through the graph or transformation of the code\n","via retracing withProxys. For example, suppose we want to run aGraphModuleand record thetorch.Tensorshape and dtype\n","properties on the nodes as we see them at runtime. That might look like:\n","------------------------------ \n"," 9 How to use Because the output of most deep learning modules consists of floating\n","pointtorch.Tensorinstances, checking for equivalence between\n","the results of twotorch.nn.Moduleis not as straightforward\n","as doing a simple equality check. To motivate this, let’s use an\n","example:, give an example? \n"," ****************************** \n","  Because the output of most deep learning modules consists of floating\n","pointtorch.Tensorinstances, checking for equivalence between\n","the results of twotorch.nn.Moduleis not as straightforward\n","as doing a simple equality check. To motivate this, let’s use an\n","example:\n","------------------------------ \n"," 10 How to use Because the output of most deep learning modules consists of floating\n","pointtorch.Tensorinstances, checking for equivalence between\n","the results of twotorch.nn.Moduleis not as straightforward\n","as doing a simple equality check. To motivate this, let’s use an\n","example:Here, we’ve tried to check equality of the values of two deep learning\n","models with the==equality operator. However, this is not well-\n","defined both due to the issue of that operator returning a tensor\n","and not a bool, but also because comparison of floating point values\n","should use a margin of error (or epsilon) to account for the\n","non-commutativity of floating point operations (seeherefor more\n","details). We can usetorch.allclose()instead, which will give\n","us an approximate comparison taking into account a relative and\n","absolute tolerance threshold:, give an example? \n"," ****************************** \n","  Here, we’ve tried to check equality of the values of two deep learning\n","models with the==equality operator. However, this is not well-\n","defined both due to the issue of that operator returning a tensor\n","and not a bool, but also because comparison of floating point values\n","should use a margin of error (or epsilon) to account for the\n","non-commutativity of floating point operations (seeherefor more\n","details). We can usetorch.allclose()instead, which will give\n","us an approximate comparison taking into account a relative and\n","absolute tolerance threshold:\n","------------------------------ \n"," 11 How to use Invokepdbto step into the running program. Although the code that\n","represents theGraphis not in any source file, we can still step\n","into it manually usingpdbwhen the forward pass is invoked., give an example? \n"," ****************************** \n","  Invokepdbto step into the running program. Although the code that\n","represents theGraphis not in any source file, we can still step\n","into it manually usingpdbwhen the forward pass is invoked.\n","------------------------------ \n"," 12 How to use If you’d like to run the same code multiple times, then it can be\n","a bit tedious to step to the right code withpdb. In that case, one\n","approach is to simply copy-paste the generatedforwardpass into\n","your code and examine it from there., give an example? \n"," ****************************** \n","  If you’d like to run the same code multiple times, then it can be\n","a bit tedious to step to the right code withpdb. In that case, one\n","approach is to simply copy-paste the generatedforwardpass into\n","your code and examine it from there.\n","------------------------------ \n"," 13 How to use GraphModule.to_folder()is a method inGraphModulethat allows\n","you to dump out the generated FX code to a folder. Although copying the\n","forward pass into the code often suffices as inPrint the Generated Code,\n","it may be easier to examine modules and parameters usingto_folder., give an example? \n"," ****************************** \n","  GraphModule.to_folder()is a method inGraphModulethat allows\n","you to dump out the generated FX code to a folder. Although copying the\n","forward pass into the code often suffices as inPrint the Generated Code,\n","it may be easier to examine modules and parameters usingto_folder.\n","------------------------------ \n"," 14 How to use Now that we’ve identified that a transformation is creating incorrect\n","code, it’s time to debug the transformation itself. First, we’ll check\n","theLimitations of Symbolic Tracingsection in the documentation.\n","Once we verify that tracing is working as expected, the goal\n","becomes figuring out what went wrong during ourGraphModuletransformation. There may be a quick answer inWriting Transformations, but, if not, there are several ways to\n","examine our traced module:, give an example? \n"," ****************************** \n","  Now that we’ve identified that a transformation is creating incorrect\n","code, it’s time to debug the transformation itself. First, we’ll check\n","theLimitations of Symbolic Tracingsection in the documentation.\n","Once we verify that tracing is working as expected, the goal\n","becomes figuring out what went wrong during ourGraphModuletransformation. There may be a quick answer inWriting Transformations, but, if not, there are several ways to\n","examine our traced module:\n","------------------------------ \n"," 15 How to use Using the utility functions above, we can compare our traced Module\n","before and after we’ve applied our transformations. Sometimes, a\n","simple visual comparison is enough to trace down a bug. If it’s still\n","not clear what’s going wrong, a debugger likepdbcan be a good\n","next step.Going off of the example above, consider the following code:, give an example? \n"," ****************************** \n","  Using the utility functions above, we can compare our traced Module\n","before and after we’ve applied our transformations. Sometimes, a\n","simple visual comparison is enough to trace down a bug. If it’s still\n","not clear what’s going wrong, a debugger likepdbcan be a good\n","next step.Going off of the example above, consider the following code:\n","------------------------------ \n"," 16 How to use The main limitation of symbolic tracing is it does not currently supportdynamic control flow. That is, loops orifstatements where the\n","condition may depend on the input values of the program.For example, let’s examine the following program:, give an example? \n"," ****************************** \n","  For example, let’s examine the following program:\n","------------------------------ \n"," 17 How to use On the other hand, so-calledstatic control flowis supported. Static\n","control flow is loops orifstatements whose value cannot change\n","across invocations. Typically, in PyTorch programs, this control flow\n","arises for code making decisions about a model’s architecture based on\n","hyper-parameters. As a concrete example:, give an example? \n"," ****************************** \n","  On the other hand, so-calledstatic control flowis supported. Static\n","control flow is loops orifstatements whose value cannot change\n","across invocations. Typically, in PyTorch programs, this control flow\n","arises for code making decisions about a model’s architecture based on\n","hyper-parameters. As a concrete example:\n","------------------------------ \n"," 18 How to use The if-statementifself.do_activationdoes not depend on any\n","function inputs, thus it is static.do_activationcan be considered\n","to be a hyper-parameter, and the traces of different instances ofMyModulewith different values for that parameter have different\n","code. This is a valid pattern that is supported by symbolic tracing.Many instances of dynamic control flow are semantically static control\n","flow. These instances can be made to support symbolic tracing by\n","removing the data dependencies on input values, for example by moving\n","values toModuleattributes or by binding concrete values to arguments\n","during symbolic tracing:, give an example? \n"," ****************************** \n","  The if-statementifself.do_activationdoes not depend on any\n","function inputs, thus it is static.do_activationcan be considered\n","to be a hyper-parameter, and the traces of different instances ofMyModulewith different values for that parameter have different\n","code. This is a valid pattern that is supported by symbolic tracing.Many instances of dynamic control flow are semantically static control\n","flow. These instances can be made to support symbolic tracing by\n","removing the data dependencies on input values, for example by moving\n","values toModuleattributes or by binding concrete values to arguments\n","during symbolic tracing:\n","------------------------------ \n"," 19 How to use FX uses__torch_function__as the mechanism by which it intercepts\n","calls (see thetechnical\n","overviewfor more information about this). Some functions, such as builtin Python\n","functions or those in themathmodule, are not covered by__torch_function__, but we would still like to capture them in\n","symbolic tracing. For example:, give an example? \n"," ****************************** \n","  FX uses__torch_function__as the mechanism by which it intercepts\n","calls (see thetechnical\n","overviewfor more information about this). Some functions, such as builtin Python\n","functions or those in themathmodule, are not covered by__torch_function__, but we would still like to capture them in\n","symbolic tracing. For example:\n","------------------------------ \n"," 20 How to use FX uses__torch_function__as the mechanism by which it intercepts\n","calls (see thetechnical\n","overviewfor more information about this). Some functions, such as builtin Python\n","functions or those in themathmodule, are not covered by__torch_function__, but we would still like to capture them in\n","symbolic tracing. For example:The error tells us that the built-in functionlenis not supported.\n","We can make it so that functions like this are recorded in the trace as\n","direct calls using thewrap()API:, give an example? \n"," ****************************** \n","  The error tells us that the built-in functionlenis not supported.\n","We can make it so that functions like this are recorded in the trace as\n","direct calls using thewrap()API:\n","------------------------------ \n"," 21 How to use TheTracerclass is the class that underlies the\n","implementation ofsymbolic_trace. The behavior of tracing can be\n","customized by subclassing Tracer, like so:, give an example? \n"," ****************************** \n","  TheTracerclass is the class that underlies the\n","implementation ofsymbolic_trace. The behavior of tracing can be\n","customized by subclassing Tracer, like so:\n","------------------------------ \n"," 22 How to use Leaf Modules are the modules that appear as calls in the symbolic trace\n","rather than being traced through. The default set of leaf modules is the\n","set of standardtorch.nnmodule instances. For example:, give an example? \n"," ****************************** \n","  Leaf Modules are the modules that appear as calls in the symbolic trace\n","rather than being traced through. The default set of leaf modules is the\n","set of standardtorch.nnmodule instances. For example:\n","------------------------------ \n"," 23 How to use Miscellanea, give an example? \n"," ****************************** \n","  \n","------------------------------ \n"," 24 How to use torch.fx.symbolic_trace, give an example? \n"," ****************************** \n","  concrete_argsallows you to partially specialize your function, whether it’s to remove control flow or data structures.For example:\n","------------------------------ \n"," 25 How  Note that although you can still pass in different values ofb, they will be ignored.We can also useconcrete_argsto eliminate data-structure handling from\n","our function. This will use pytrees to flatten your input. To avoid\n","overspecializing, pass infx.PHfor values that shouldn’t be\n","specialized. For example:, give an example? \n"," ****************************** \n","  Note that although you can still pass in different values ofb, they will be ignored.We can also useconcrete_argsto eliminate data-structure handling from\n","our function. This will use pytrees to flatten your input. To avoid\n","overspecializing, pass infx.PHfor values that shouldn’t be\n","specialized. For example:\n","------------------------------ \n"," 26 How to use torch.fx.wrap, give an example? \n"," ****************************** \n","  This function can be called at module-level scope to register fn_or_name as a “leaf function”.\n","A “leaf function” will be preserved as a CallFunction node in the FX trace instead of being\n","traced through:\n","------------------------------ \n"," 27 How  This function can also equivalently be used as a decorator:, give an example? \n"," ****************************** \n","  This function can also equivalently be used as a decorator:\n","------------------------------ \n"," 28 How to use For example, the following code, give an example? \n"," ****************************** \n","  For example, the following code\n","------------------------------ \n"," 29 How to use Will produce the following Graph:, give an example? \n"," ****************************** \n","  For example, the following codeWill produce the following Graph:\n","------------------------------ \n"," 30 How to use API Reference, give an example? \n"," ****************************** \n","  \n","------------------------------ \n"," 31 How to use torch.fx.Graph.eliminate_dead_code, give an example? \n"," ****************************** \n","  Before dead code is eliminated,afroma = x + 1below has no users\n","and thus can be eliminated from the graph without having an effect.\n","------------------------------ \n"," 32 How  Before dead code is eliminated,afroma = x + 1below has no users\n","and thus can be eliminated from the graph without having an effect.After dead code is eliminated,a = x + 1has been removed, and the rest\n","offorwardremains., give an example? \n"," ****************************** \n","  Before dead code is eliminated,afroma = x + 1below has no users\n","and thus can be eliminated from the graph without having an effect.After dead code is eliminated,a = x + 1has been removed, and the rest\n","offorwardremains.\n","------------------------------ \n"," 33 How to use torch.fx.Graph.inserting_after, give an example? \n"," ****************************** \n","  Set the point at which create_node and companion methods will insert into the graph.\n","When used within a ‘with’ statement, this will temporary set the insert point and\n","then restore it when the with statement exits:\n","------------------------------ \n"," 34 How to use torch.fx.Graph.inserting_before, give an example? \n"," ****************************** \n","  Set the point at which create_node and companion methods will insert into the graph.\n","When used within a ‘with’ statement, this will temporary set the insert point and\n","then restore it when the with statement exits:\n","------------------------------ \n"," 35 How to use torch.fx.Graph.node_copy, give an example? \n"," ****************************** \n","  \n","------------------------------ \n"," 36 How to use torch.fx.Node.prepend, give an example? \n"," ****************************** \n","  \n","------------------------------ \n"," 37 How to use Methods in the Interpreter class can be overridden to customize\n","the behavior of execution. The map of overrideable methods\n","in terms of call hierarchy:, give an example? \n"," ****************************** \n","  Methods in the Interpreter class can be overridden to customize\n","the behavior of execution. The map of overrideable methods\n","in terms of call hierarchy:\n","------------------------------ \n"," 38 How to use Suppose we want to swap all instances oftorch.negwithtorch.sigmoidand vice versa (including theirTensormethod equivalents). We could subclass Interpreter like so:, give an example? \n"," ****************************** \n","  Suppose we want to swap all instances oftorch.negwithtorch.sigmoidand vice versa (including theirTensormethod equivalents). We could subclass Interpreter like so:\n","------------------------------ \n"," 39 How to use Suppose we want to swap all instances oftorch.negwithtorch.sigmoidand vice versa (including theirTensormethod equivalents). We could subclassTransformerlike so:, give an example? \n"," ****************************** \n","  Suppose we want to swap all instances oftorch.negwithtorch.sigmoidand vice versa (including theirTensormethod equivalents). We could subclassTransformerlike so:\n","------------------------------ \n"," 40 How to use torch.fx.replace_pattern, give an example? \n"," ****************************** \n","  A list ofMatchobjects representing the places\n","in the original graph thatpatternwas matched to. The list\n","is empty if there are no matches.Matchis defined as:\n","------------------------------ \n"," 41 How  , give an example? \n"," ****************************** \n","  \n","------------------------------ \n"," 42 How  When the pattern is matched, it will be removed from the larger\n","function and replaced byreplacement. If there are multiple\n","matches forpatternin the larger function, each non-overlapping\n","match will be replaced. In the case of a match overlap, the first\n","found match in the set of overlapping matches will be replaced.\n","(“First” here being defined as the first in a topological ordering\n","of the Nodes’ use-def relationships. In most cases, the first Node\n","is the parameter that appears directly afterself, while the\n","last Node is whatever the function returns.)One important thing to note is that the parameters of thepatternCallable must be used in the Callable itself,\n","and the parameters of thereplacementCallable must match\n","the pattern. The first rule is why, in the above code block, theforwardfunction has parametersx,w1,w2, but thepatternfunction only has parametersw1,w2.patterndoesn’t usex, so it shouldn’t specifyxas a parameter.\n","As an example of the second rule, consider replacing, give an example? \n"," ****************************** \n","  When the pattern is matched, it will be removed from the larger\n","function and replaced byreplacement. If there are multiple\n","matches forpatternin the larger function, each non-overlapping\n","match will be replaced. In the case of a match overlap, the first\n","found match in the set of overlapping matches will be replaced.\n","(“First” here being defined as the first in a topological ordering\n","of the Nodes’ use-def relationships. In most cases, the first Node\n","is the parameter that appears directly afterself, while the\n","last Node is whatever the function returns.)One important thing to note is that the parameters of thepatternCallable must be used in the Callable itself,\n","and the parameters of thereplacementCallable must match\n","the pattern. The first rule is why, in the above code block, theforwardfunction has parametersx,w1,w2, but thepatternfunction only has parametersw1,w2.patterndoesn’t usex, so it shouldn’t specifyxas a parameter.\n","As an example of the second rule, consider replacing\n","------------------------------ \n"," 43 How  One important thing to note is that the parameters of thepatternCallable must be used in the Callable itself,\n","and the parameters of thereplacementCallable must match\n","the pattern. The first rule is why, in the above code block, theforwardfunction has parametersx,w1,w2, but thepatternfunction only has parametersw1,w2.patterndoesn’t usex, so it shouldn’t specifyxas a parameter.\n","As an example of the second rule, consider replacingwith, give an example? \n"," ****************************** \n","  One important thing to note is that the parameters of thepatternCallable must be used in the Callable itself,\n","and the parameters of thereplacementCallable must match\n","the pattern. The first rule is why, in the above code block, theforwardfunction has parametersx,w1,w2, but thepatternfunction only has parametersw1,w2.patterndoesn’t usex, so it shouldn’t specifyxas a parameter.\n","As an example of the second rule, consider replacingwith\n","------------------------------ \n"," 44 How  In this case,replacementneeds the same number of parameters\n","aspattern(bothxandy), even though the parameteryisn’t used inreplacement.After callingsubgraph_rewriter.replace_pattern, the generated\n","Python code looks like this:, give an example? \n"," ****************************** \n","  In this case,replacementneeds the same number of parameters\n","aspattern(bothxandy), even though the parameteryisn’t used inreplacement.After callingsubgraph_rewriter.replace_pattern, the generated\n","Python code looks like this:\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BF38xcwxIS8S","executionInfo":{"status":"ok","timestamp":1628683293010,"user_tz":-330,"elapsed":28,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}},"outputId":"1509b6d9-71f4-4225-9777-0646b04aca64"},"source":["for k,v in uniqqdict.items():\n","  if v[0]> 1:\n","    print(v[0] , '--',k,v[1])"],"execution_count":17,"outputs":[{"output_type":"stream","text":["8 -- How  Formax_abs_diffandmax_rel_diffthe type depends on thedtypeof the inputs., give an example? https://pytorch.org/docs/stable/testing.html\n","3 -- How  Thetorch.deviceargument in functions can generally be substituted with a string.\n","This allows for fast prototyping of code., give an example? https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype\n","110 -- How  , give an example? https://pytorch.org/docs/stable/onnx.html\n","3 -- How  is exported as:, give an example? https://pytorch.org/docs/stable/onnx.html\n","2 -- How  The container format for atorch.packageis ZIP, so any tools that work with standard ZIP files should\n","work for exploring the contents. Some common ways to interact with ZIP files:, give an example? https://pytorch.org/docs/stable/package.html\n","2 -- How  Steps:, give an example? https://pytorch.org/docs/stable/package.html\n","3 -- How  IfkeepdimisTrue, the output tensor is of the same size\n","asinputexcept in the dimensiondimwhere it is of size 1.\n","Otherwise,dimis squeezed (seetorch.squeeze()), resulting in\n","the output tensor having 1 fewer dimension thaninput., give an example? https://pytorch.org/docs/stable/generated/torch.prod.html#torch.prod\n","7 -- How  Similar to SciPy’sscipy.special.xlog1py., give an example? https://pytorch.org/docs/stable/special.html#torch.special.exp2\n","3 -- How  IfkeepdimisTrue, the output tensor is of the same size\n","asinputexcept in the dimension(s)dimwhere it is of size 1.\n","Otherwise,dimis squeezed (seetorch.squeeze()), resulting in the\n","output tensor having 1 (orlen(dim)) fewer dimension(s)., give an example? https://pytorch.org/docs/stable/generated/torch.nansum.html#torch.nansum\n","2 -- How  Complex tensors are supported., give an example? https://pytorch.org/docs/stable/distributed.html\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3X8_NISpLTXe","executionInfo":{"status":"ok","timestamp":1628683293010,"user_tz":-330,"elapsed":23,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}},"outputId":"34b9a232-5629-4684-9ebe-a5e3ea977f42"},"source":["stotal = 0\n","urlcountdict = {}\n","for k,v in uniqqdict.items():\n","  stotal += v[0]\n","\n","  if v[1] in urlcountdict:\n","    urlcountdict[v[1]] = urlcountdict[v[1]] + v[0]\n","  else: \n","    urlcountdict[v[1]] = v[0]\n","\n","  if v[0] > 1:\n","    print(v[0], v[1])\n","\n","stotal"],"execution_count":18,"outputs":[{"output_type":"stream","text":["8 https://pytorch.org/docs/stable/testing.html\n","3 https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype\n","110 https://pytorch.org/docs/stable/onnx.html\n","3 https://pytorch.org/docs/stable/onnx.html\n","2 https://pytorch.org/docs/stable/package.html\n","2 https://pytorch.org/docs/stable/package.html\n","3 https://pytorch.org/docs/stable/generated/torch.prod.html#torch.prod\n","7 https://pytorch.org/docs/stable/special.html#torch.special.exp2\n","3 https://pytorch.org/docs/stable/generated/torch.nansum.html#torch.nansum\n","2 https://pytorch.org/docs/stable/distributed.html\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["883"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JeoY4KTfm9aE","executionInfo":{"status":"ok","timestamp":1628683293011,"user_tz":-330,"elapsed":21,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}},"outputId":"577261ae-fe6e-4ac3-a1e2-995d5bc59766"},"source":["for k , v in urlcountdict.items():\n","  if v > 1:\n","    print(k,v)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["https://pytorch.org/docs/stable/testing.html 9\n","https://pytorch.org/docs/stable/generated/torch.diag.html#torch.diag 2\n","https://pytorch.org/docs/stable/torch.overrides.html 8\n","https://pytorch.org/docs/stable/generated/torch.nanmedian.html#torch.nanmedian 2\n","https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype 11\n","https://pytorch.org/docs/stable/generated/torch.median.html#torch.median 2\n","https://pytorch.org/docs/stable/onnx.html 161\n","https://pytorch.org/docs/stable/amp.html 5\n","https://pytorch.org/docs/stable/generated/torch.normal.html#torch.normal 4\n","https://pytorch.org/docs/stable/special.html#torch.special.erfinv 11\n","https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc 6\n","https://pytorch.org/docs/stable/distributions.html 53\n","https://pytorch.org/docs/stable/benchmark_utils.html 3\n","https://pytorch.org/docs/stable/package.html 34\n","https://pytorch.org/docs/stable/generated/torch.linalg.matrix_power.html#torch.linalg.matrix_power 2\n","https://pytorch.org/docs/stable/profiler.html 2\n","https://pytorch.org/docs/stable/optim.html 13\n","https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig 3\n","https://pytorch.org/docs/stable/hub.html 8\n","https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv 2\n","https://pytorch.org/docs/stable/generated/torch.prod.html#torch.prod 4\n","https://pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky 3\n","https://pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax 2\n","https://pytorch.org/docs/stable/special.html#torch.special.exp2 7\n","https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_ 3\n","https://pytorch.org/docs/stable/notes/randomness.html 6\n","https://pytorch.org/docs/stable/generated/torch.qr.html#torch.qr 3\n","https://pytorch.org/docs/stable/jit.html 16\n","https://pytorch.org/docs/stable/generated/torch.gather.html#torch.gather 2\n","https://pytorch.org/docs/stable/nn.init.html 14\n","https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv 2\n","https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_ 2\n","https://pytorch.org/docs/stable/tensors.html 5\n","https://pytorch.org/docs/stable/sparse.html 19\n","https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve 2\n","https://pytorch.org/docs/stable/futures.html 5\n","https://pytorch.org/docs/stable/generated/torch.nansum.html#torch.nansum 4\n","https://pytorch.org/docs/stable/distributed.html 32\n","https://pytorch.org/docs/stable/generated/torch.pow.html#torch.pow 2\n","https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul 2\n","https://pytorch.org/docs/stable/generated/torch.max.html#torch.max 2\n","https://pytorch.org/docs/stable/generated/torch.lstsq.html#torch.lstsq 2\n","https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator 7\n","https://pytorch.org/docs/stable/generated/torch.min.html#torch.min 2\n","https://pytorch.org/docs/stable/notes/modules.html 17\n","https://pytorch.org/docs/stable/tensors.html#torch.Tensor 5\n","https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd 3\n","https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig 2\n","https://pytorch.org/docs/stable/data.html 11\n","https://pytorch.org/docs/stable/fx.html 43\n","https://pytorch.org/docs/stable/cpp_extension.html 4\n","https://pytorch.org/docs/stable/generated/torch.add.html#torch.add 2\n","https://pytorch.org/docs/stable/quantization.html 11\n","https://pytorch.org/docs/stable/tensorboard.html 15\n","https://pytorch.org/docs/stable/autograd.html 6\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bEBlN3ybNCwJ","executionInfo":{"status":"ok","timestamp":1628683293011,"user_tz":-330,"elapsed":19,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}},"outputId":"27927011-05a8-4aa6-ee78-a9000eacaa13"},"source":["countdict"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'https://pytorch.org/docs/stable/amp.html': 5,\n"," 'https://pytorch.org/docs/stable/autograd.html': 6,\n"," 'https://pytorch.org/docs/stable/benchmark_utils.html': 3,\n"," 'https://pytorch.org/docs/stable/bottleneck.html': 1,\n"," 'https://pytorch.org/docs/stable/checkpoint.html': 1,\n"," 'https://pytorch.org/docs/stable/cpp_extension.html': 4,\n"," 'https://pytorch.org/docs/stable/data.html': 12,\n"," 'https://pytorch.org/docs/stable/distributed.html': 37,\n"," 'https://pytorch.org/docs/stable/distributed.optim.html': 1,\n"," 'https://pytorch.org/docs/stable/distributions.html': 53,\n"," 'https://pytorch.org/docs/stable/futures.html': 5,\n"," 'https://pytorch.org/docs/stable/fx.html': 44,\n"," 'https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator': 7,\n"," 'https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_': 3,\n"," 'https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_': 2,\n"," 'https://pytorch.org/docs/stable/generated/torch.abs.html#torch.abs': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.acos.html#torch.acos': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.acosh.html#torch.acosh': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.add.html#torch.add': 2,\n"," 'https://pytorch.org/docs/stable/generated/torch.addbmm.html#torch.addbmm': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.addcmul.html#torch.addcmul': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.addmm.html#torch.addmm': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.addmv.html#torch.addmv': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.addr.html#torch.addr': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.all.html#torch.all': 2,\n"," 'https://pytorch.org/docs/stable/generated/torch.allclose.html#torch.allclose': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.amax.html#torch.amax': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.amin.html#torch.amin': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.angle.html#torch.angle': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.any.html#torch.any': 2,\n"," 'https://pytorch.org/docs/stable/generated/torch.arange.html#torch.arange': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax': 2,\n"," 'https://pytorch.org/docs/stable/generated/torch.argmin.html#torch.argmin': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.argsort.html#torch.argsort': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.as_strided.html#torch.as_strided': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.as_tensor.html#torch.as_tensor': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.asin.html#torch.asin': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.asinh.html#torch.asinh': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.atan.html#torch.atan': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.atan2.html#torch.atan2': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.atanh.html#torch.atanh': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.atleast_1d.html#torch.atleast_1d': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.atleast_2d.html#torch.atleast_2d': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.atleast_3d.html#torch.atleast_3d': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.baddbmm.html#torch.baddbmm': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.bernoulli.html#torch.bernoulli': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.bitwise_and.html#torch.bitwise_and': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.bitwise_not.html#torch.bitwise_not': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.bitwise_or.html#torch.bitwise_or': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.bitwise_xor.html#torch.bitwise_xor': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.block_diag.html#torch.block_diag': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.bmm.html#torch.bmm': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.broadcast_shapes.html#torch.broadcast_shapes': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.broadcast_to.html#torch.broadcast_to': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.bucketize.html#torch.bucketize': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.can_cast.html#torch.can_cast': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.cartesian_prod.html#torch.cartesian_prod': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.cdist.html#torch.cdist': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.ceil.html#torch.ceil': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.chain_matmul.html#torch.chain_matmul': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky': 3,\n"," 'https://pytorch.org/docs/stable/generated/torch.cholesky_inverse.html#torch.cholesky_inverse': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.cholesky_solve.html#torch.cholesky_solve': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.clamp.html#torch.clamp': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.column_stack.html#torch.column_stack': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.combinations.html#torch.combinations': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.complex.html#torch.complex': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.conj.html#torch.conj': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.copysign.html#torch.copysign': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.cos.html#torch.cos': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.cosh.html#torch.cosh': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.count_nonzero.html#torch.count_nonzero': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.cross.html#torch.cross': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.cummax.html#torch.cummax': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.cummin.html#torch.cummin': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.cumprod.html#torch.cumprod': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.cumsum.html#torch.cumsum': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.deg2rad.html#torch.deg2rad': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.diag.html#torch.diag': 2,\n"," 'https://pytorch.org/docs/stable/generated/torch.diag_embed.html#torch.diag_embed': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.diagflat.html#torch.diagflat': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.diagonal.html#torch.diagonal': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.diff.html#torch.diff': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.digamma.html#torch.digamma': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.dist.html#torch.dist': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.div.html#torch.div': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.dot.html#torch.dot': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.dsplit.html#torch.dsplit': 2,\n"," 'https://pytorch.org/docs/stable/generated/torch.dstack.html#torch.dstack': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig': 3,\n"," 'https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.empty.html#torch.empty': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.empty_like.html#torch.empty_like': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.empty_strided.html#torch.empty_strided': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.enable_grad.html#torch.enable_grad': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.eq.html#torch.eq': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.equal.html#torch.equal': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.exp.html#torch.exp': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.fake_quantize_per_channel_affine.html#torch.fake_quantize_per_channel_affine': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.fake_quantize_per_tensor_affine.html#torch.fake_quantize_per_tensor_affine': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.flatten.html#torch.flatten': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.fliplr.html#torch.fliplr': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.flipud.html#torch.flipud': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.float_power.html#torch.float_power': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.floor.html#torch.floor': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.floor_divide.html#torch.floor_divide': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.fmax.html#torch.fmax': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.fmin.html#torch.fmin': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.frac.html#torch.frac': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.frexp.html#torch.frexp': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.from_numpy.html#torch.from_numpy': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.full.html#torch.full': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.gather.html#torch.gather': 2,\n"," 'https://pytorch.org/docs/stable/generated/torch.gcd.html#torch.gcd': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.ge.html#torch.ge': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.get_default_dtype.html#torch.get_default_dtype': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.gradient.html#torch.gradient': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.gt.html#torch.gt': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.heaviside.html#torch.heaviside': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.histc.html#torch.histc': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.hsplit.html#torch.hsplit': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.hstack.html#torch.hstack': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.hypot.html#torch.hypot': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.i0.html#torch.i0': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.igamma.html#torch.igamma': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.igammac.html#torch.igammac': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.imag.html#torch.imag': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.index_select.html#torch.index_select': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.inner.html#torch.inner': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.is_nonzero.html#torch.is_nonzero': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.is_tensor.html': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.is_tensor.html#torch.is_tensor': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.isclose.html#torch.isclose': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.isfinite.html#torch.isfinite': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.isinf.html#torch.isinf': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.isnan.html#torch.isnan': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.isneginf.html#torch.isneginf': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.isposinf.html#torch.isposinf': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.isreal.html#torch.isreal': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.kron.html#torch.kron': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.kthvalue.html#torch.kthvalue': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.lcm.html#torch.lcm': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.ldexp.html#torch.ldexp': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.le.html#torch.le': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.lerp.html#torch.lerp': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.lgamma.html#torch.lgamma': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.linalg.det.html#torch.linalg.det': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.linalg.householder_product.html#torch.linalg.householder_product': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv': 2,\n"," 'https://pytorch.org/docs/stable/generated/torch.linalg.matrix_power.html#torch.linalg.matrix_power': 2,\n"," 'https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv': 2,\n"," 'https://pytorch.org/docs/stable/generated/torch.linalg.slogdet.html#torch.linalg.slogdet': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.linspace.html#torch.linspace': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.load.html#torch.load': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.log10.html#torch.log10': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.log1p.html#torch.log1p': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.log2.html#torch.log2': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.logaddexp.html#torch.logaddexp': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.logcumsumexp.html#torch.logcumsumexp': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.logdet.html#torch.logdet': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.logical_and.html#torch.logical_and': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.logical_not.html#torch.logical_not': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.logical_or.html#torch.logical_or': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.logical_xor.html#torch.logical_xor': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.logspace.html#torch.logspace': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.logsumexp.html#torch.logsumexp': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.lstsq.html#torch.lstsq': 2,\n"," 'https://pytorch.org/docs/stable/generated/torch.lt.html#torch.lt': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.lu_solve.html#torch.lu_solve': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.lu_unpack.html#torch.lu_unpack': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.masked_select.html#torch.masked_select': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.matrix_exp.html#torch.matrix_exp': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.matrix_rank.html#torch.matrix_rank': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.max.html#torch.max': 2,\n"," 'https://pytorch.org/docs/stable/generated/torch.maximum.html#torch.maximum': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean': 2,\n"," 'https://pytorch.org/docs/stable/generated/torch.median.html#torch.median': 2,\n"," 'https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.min.html#torch.min': 2,\n"," 'https://pytorch.org/docs/stable/generated/torch.minimum.html#torch.minimum': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.mm.html#torch.mm': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.mode.html#torch.mode': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.moveaxis.html#torch.moveaxis': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.movedim.html#torch.movedim': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.msort.html#torch.msort': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul': 2,\n"," 'https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.mv.html#torch.mv': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.mvlgamma.html#torch.mvlgamma': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.nan_to_num.html#torch.nan_to_num': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.nanmedian.html#torch.nanmedian': 2,\n"," 'https://pytorch.org/docs/stable/generated/torch.nanquantile.html#torch.nanquantile': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.nansum.html#torch.nansum': 2,\n"," 'https://pytorch.org/docs/stable/generated/torch.narrow.html#torch.narrow': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.ne.html#torch.ne': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.neg.html#torch.neg': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.nextafter.html#torch.nextafter': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.nonzero.html#torch.nonzero': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.normal.html#torch.normal': 4,\n"," 'https://pytorch.org/docs/stable/generated/torch.numel.html#torch.numel': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.ones_like.html#torch.ones_like': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.outer.html#torch.outer': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.poisson.html#torch.poisson': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.polar.html#torch.polar': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.polygamma.html#torch.polygamma': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.positive.html#torch.positive': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.pow.html#torch.pow': 2,\n"," 'https://pytorch.org/docs/stable/generated/torch.prod.html#torch.prod': 2,\n"," 'https://pytorch.org/docs/stable/generated/torch.promote_types.html#torch.promote_types': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.qr.html#torch.qr': 3,\n"," 'https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.quantize_per_channel.html#torch.quantize_per_channel': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.quantize_per_tensor.html#torch.quantize_per_tensor': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.quasirandom.SobolEngine.html#torch.quasirandom.SobolEngine': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.rad2deg.html#torch.rad2deg': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.randint.html#torch.randint': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.randperm.html#torch.randperm': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.range.html#torch.range': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.ravel.html#torch.ravel': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.real.html#torch.real': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.reciprocal.html#torch.reciprocal': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.renorm.html#torch.renorm': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.result_type.html#torch.result_type': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.roll.html#torch.roll': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.rot90.html#torch.rot90': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.round.html#torch.round': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.rsqrt.html#torch.rsqrt': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.save.html#torch.save': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.set_default_dtype.html#torch.set_default_dtype': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.set_default_tensor_type.html#torch.set_default_tensor_type': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.set_flush_denormal.html#torch.set_flush_denormal': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.set_grad_enabled.html#torch.set_grad_enabled': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.sgn.html#torch.sgn': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.sign.html#torch.sign': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.signbit.html#torch.signbit': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.sin.html#torch.sin': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.sinc.html#torch.sinc': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.sinh.html#torch.sinh': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve': 2,\n"," 'https://pytorch.org/docs/stable/generated/torch.sort.html#torch.sort': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.split.html#torch.split': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.sqrt.html#torch.sqrt': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.square.html#torch.square': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.squeeze.html#torch.squeeze': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.std.html#torch.std': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.std_mean.html#torch.std_mean': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.sub.html#torch.sub': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum': 2,\n"," 'https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd': 3,\n"," 'https://pytorch.org/docs/stable/generated/torch.swapaxes.html#torch.swapaxes': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.swapdims.html#torch.swapdims': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig': 3,\n"," 'https://pytorch.org/docs/stable/generated/torch.t.html#torch.t': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.take.html#torch.take': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.take_along_dim.html#torch.take_along_dim': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.tan.html#torch.tan': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.tanh.html#torch.tanh': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.tensordot.html#torch.tensordot': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.trace.html#torch.trace': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.transpose.html#torch.transpose': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.trapz.html#torch.trapz': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.tril.html#torch.tril': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.tril_indices.html#torch.tril_indices': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.triu.html#torch.triu': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.trunc.html#torch.trunc': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.unbind.html#torch.unbind': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.unique_consecutive.html#torch.unique_consecutive': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.unsqueeze.html#torch.unsqueeze': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.var.html#torch.var': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.vdot.html#torch.vdot': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.view_as_complex.html#torch.view_as_complex': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.view_as_real.html#torch.view_as_real': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.vsplit.html#torch.vsplit': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.vstack.html#torch.vstack': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.where.html#torch.where': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.xlogy.html#torch.xlogy': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros': 1,\n"," 'https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like': 1,\n"," 'https://pytorch.org/docs/stable/hub.html': 8,\n"," 'https://pytorch.org/docs/stable/jit.html': 26,\n"," 'https://pytorch.org/docs/stable/model_zoo.html': 1,\n"," 'https://pytorch.org/docs/stable/nn.init.html': 14,\n"," 'https://pytorch.org/docs/stable/notes/modules.html': 17,\n"," 'https://pytorch.org/docs/stable/notes/randomness.html': 6,\n"," 'https://pytorch.org/docs/stable/onnx.html': 52,\n"," 'https://pytorch.org/docs/stable/optim.html': 13,\n"," 'https://pytorch.org/docs/stable/package.html': 34,\n"," 'https://pytorch.org/docs/stable/profiler.html': 2,\n"," 'https://pytorch.org/docs/stable/quantization.html': 11,\n"," 'https://pytorch.org/docs/stable/sparse.html': 19,\n"," 'https://pytorch.org/docs/stable/special.html': 11,\n"," 'https://pytorch.org/docs/stable/special.html#torch.special.erf': 11,\n"," 'https://pytorch.org/docs/stable/special.html#torch.special.erfc': 11,\n"," 'https://pytorch.org/docs/stable/special.html#torch.special.erfinv': 11,\n"," 'https://pytorch.org/docs/stable/special.html#torch.special.exp2': 11,\n"," 'https://pytorch.org/docs/stable/special.html#torch.special.expit': 11,\n"," 'https://pytorch.org/docs/stable/special.html#torch.special.expm1': 11,\n"," 'https://pytorch.org/docs/stable/special.html#torch.special.logit': 11,\n"," 'https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype': 9,\n"," 'https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc': 9,\n"," 'https://pytorch.org/docs/stable/tensorboard.html': 15,\n"," 'https://pytorch.org/docs/stable/tensors.html': 5,\n"," 'https://pytorch.org/docs/stable/tensors.html#torch.Tensor': 5,\n"," 'https://pytorch.org/docs/stable/testing.html': 9,\n"," 'https://pytorch.org/docs/stable/torch.html#': 1,\n"," 'https://pytorch.org/docs/stable/torch.html#comparison-ops': 1,\n"," 'https://pytorch.org/docs/stable/torch.html#creation-ops': 1,\n"," 'https://pytorch.org/docs/stable/torch.html#generators': 1,\n"," 'https://pytorch.org/docs/stable/torch.html#in-place-random-sampling': 1,\n"," 'https://pytorch.org/docs/stable/torch.html#indexing-slicing-joining-mutating-ops': 1,\n"," 'https://pytorch.org/docs/stable/torch.html#locally-disabling-gradient-computation': 1,\n"," 'https://pytorch.org/docs/stable/torch.html#math-operations': 1,\n"," 'https://pytorch.org/docs/stable/torch.html#other-operations': 1,\n"," 'https://pytorch.org/docs/stable/torch.html#parallelism': 1,\n"," 'https://pytorch.org/docs/stable/torch.html#pointwise-ops': 1,\n"," 'https://pytorch.org/docs/stable/torch.html#quasi-random-sampling': 1,\n"," 'https://pytorch.org/docs/stable/torch.html#random-sampling': 1,\n"," 'https://pytorch.org/docs/stable/torch.html#reduction-ops': 1,\n"," 'https://pytorch.org/docs/stable/torch.html#serialization': 1,\n"," 'https://pytorch.org/docs/stable/torch.html#spectral-ops': 1,\n"," 'https://pytorch.org/docs/stable/torch.html#tensors': 1,\n"," 'https://pytorch.org/docs/stable/torch.html#torch': 1,\n"," 'https://pytorch.org/docs/stable/torch.html#torch.torch.default_generator': 1,\n"," 'https://pytorch.org/docs/stable/torch.html#utilities': 1,\n"," 'https://pytorch.org/docs/stable/torch.overrides.html': 8}"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"7A65Crpl9fWV","executionInfo":{"status":"ok","timestamp":1628683344764,"user_tz":-330,"elapsed":51767,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}}},"source":["valhglt = {}\n","for url in teamurllist[start_idx:end_idx]:\n","  #url = 'https://pytorch.org/docs/stable/autograd.html'\n","  # 'https://pytorch.org/docs/stable/generated/torch.load.html#torch.load'\n","  #print(url)\n","  try:\n","    html = req.urlopen(url).read()\n","  except:\n","    continue\n","  soup = BeautifulSoup(html)\n","\n","\n","  pdict = {}\n","\n","  \n","  divs = soup.find_all('div' ,  class_=\"highlight\") #class_=\"section\") #\n","  \n","\n","  if divs:\n","    valhglt[url] = len(divs)\n","\n"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TaXtqKKS-E8-","executionInfo":{"status":"ok","timestamp":1628683344768,"user_tz":-330,"elapsed":14,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}},"outputId":"6cf3283d-3409-499b-d275-161b79a29b2e"},"source":["len(valhglt) , len(countdict)"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(359, 355)"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ppcxSLhm-aJN","executionInfo":{"status":"ok","timestamp":1628683344768,"user_tz":-330,"elapsed":12,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}},"outputId":"d57424e0-41b1-4757-c1ca-613a933ccba0"},"source":["for k, v  in valhglt.items():\n","  if k not in countdict:\n","    print(k)\n","  else:\n","    assert countdict[k] == v"],"execution_count":23,"outputs":[{"output_type":"stream","text":["https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md\n","https://pytorch.org/features\n","https://github.com/rtfd/sphinx_rtd_theme\n","https://github.com/pytorch/pytorch\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"17_jPT6qneYN","executionInfo":{"status":"ok","timestamp":1628683344769,"user_tz":-330,"elapsed":11,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}}},"source":["def findHighlightDiv(hgdiv):\n","\n","  hglts = hgdiv.find_next_siblings('div')\n","\n","  if  hglts:\n","    for hglt in hglts:\n","      try:\n","        pr = hglt.find('pre').text\n","\n","        print(pr)\n","      except:\n","        findHighlightDiv(hglt)\n","\n"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"7TLPsDapnW8H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628683345576,"user_tz":-330,"elapsed":817,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}},"outputId":"aca8e157-23e4-4f4e-faed-76022356106a"},"source":["for url in teamurllist[0:1]:\n","  url = 'https://pytorch.org/docs/stable/autograd.html'\n","  # 'https://pytorch.org/docs/stable/generated/torch.load.html#torch.load'\n","  #print(url)\n","  try:\n","    html = req.urlopen(url).read()\n","  except:\n","    continue\n","  soup = BeautifulSoup(html)\n","\n","\n","  pdict = {}\n","\n","  \n","  divs = soup.find_all('div' , class_=\"section\") # class_=\"highlight\") \n","  \n","\n","  if divs:\n","\n","    for div in divs:\n","      findHighlightDiv(div)\n","      \n"],"execution_count":25,"outputs":[{"output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> class Exp(Function):\n",">>>\n",">>>     @staticmethod\n",">>>     def forward(ctx, i):\n",">>>         result = i.exp()\n",">>>         ctx.save_for_backward(result)\n",">>>         return result\n",">>>\n",">>>     @staticmethod\n",">>>     def backward(ctx, grad_output):\n",">>>         result, = ctx.saved_tensors\n",">>>         return grad_output * result\n",">>>\n",">>> #Use it by calling the apply method:\n",">>> output = Exp.apply(input)\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> class Exp(Function):\n",">>>\n",">>>     @staticmethod\n",">>>     def forward(ctx, i):\n",">>>         result = i.exp()\n",">>>         ctx.save_for_backward(result)\n",">>>         return result\n",">>>\n",">>>     @staticmethod\n",">>>     def backward(ctx, grad_output):\n",">>>         result, = ctx.saved_tensors\n",">>>         return grad_output * result\n",">>>\n",">>> #Use it by calling the apply method:\n",">>> output = Exp.apply(input)\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> class Exp(Function):\n",">>>\n",">>>     @staticmethod\n",">>>     def forward(ctx, i):\n",">>>         result = i.exp()\n",">>>         ctx.save_for_backward(result)\n",">>>         return result\n",">>>\n",">>>     @staticmethod\n",">>>     def backward(ctx, grad_output):\n",">>>         result, = ctx.saved_tensors\n",">>>         return grad_output * result\n",">>>\n",">>> #Use it by calling the apply method:\n",">>> output = Exp.apply(input)\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> class Exp(Function):\n",">>>\n",">>>     @staticmethod\n",">>>     def forward(ctx, i):\n",">>>         result = i.exp()\n",">>>         ctx.save_for_backward(result)\n",">>>         return result\n",">>>\n",">>>     @staticmethod\n",">>>     def backward(ctx, grad_output):\n",">>>         result, = ctx.saved_tensors\n",">>>         return grad_output * result\n",">>>\n",">>> #Use it by calling the apply method:\n",">>> output = Exp.apply(input)\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> class Exp(Function):\n",">>>\n",">>>     @staticmethod\n",">>>     def forward(ctx, i):\n",">>>         result = i.exp()\n",">>>         ctx.save_for_backward(result)\n",">>>         return result\n",">>>\n",">>>     @staticmethod\n",">>>     def backward(ctx, grad_output):\n",">>>         result, = ctx.saved_tensors\n",">>>         return grad_output * result\n",">>>\n",">>> #Use it by calling the apply method:\n",">>> output = Exp.apply(input)\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> class Exp(Function):\n",">>>\n",">>>     @staticmethod\n",">>>     def forward(ctx, i):\n",">>>         result = i.exp()\n",">>>         ctx.save_for_backward(result)\n",">>>         return result\n",">>>\n",">>>     @staticmethod\n",">>>     def backward(ctx, grad_output):\n",">>>         result, = ctx.saved_tensors\n",">>>         return grad_output * result\n",">>>\n",">>> #Use it by calling the apply method:\n",">>> output = Exp.apply(input)\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> class Exp(Function):\n",">>>\n",">>>     @staticmethod\n",">>>     def forward(ctx, i):\n",">>>         result = i.exp()\n",">>>         ctx.save_for_backward(result)\n",">>>         return result\n",">>>\n",">>>     @staticmethod\n",">>>     def backward(ctx, grad_output):\n",">>>         result, = ctx.saved_tensors\n",">>>         return grad_output * result\n",">>>\n",">>> #Use it by calling the apply method:\n",">>> output = Exp.apply(input)\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> class Exp(Function):\n",">>>\n",">>>     @staticmethod\n",">>>     def forward(ctx, i):\n",">>>         result = i.exp()\n",">>>         ctx.save_for_backward(result)\n",">>>         return result\n",">>>\n",">>>     @staticmethod\n",">>>     def backward(ctx, grad_output):\n",">>>         result, = ctx.saved_tensors\n",">>>         return grad_output * result\n",">>>\n",">>> #Use it by calling the apply method:\n",">>> output = Exp.apply(input)\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> class Exp(Function):\n",">>>\n",">>>     @staticmethod\n",">>>     def forward(ctx, i):\n",">>>         result = i.exp()\n",">>>         ctx.save_for_backward(result)\n",">>>         return result\n",">>>\n",">>>     @staticmethod\n",">>>     def backward(ctx, grad_output):\n",">>>         result, = ctx.saved_tensors\n",">>>         return grad_output * result\n",">>>\n",">>> #Use it by calling the apply method:\n",">>> output = Exp.apply(input)\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> class Exp(Function):\n",">>>\n",">>>     @staticmethod\n",">>>     def forward(ctx, i):\n",">>>         result = i.exp()\n",">>>         ctx.save_for_backward(result)\n",">>>         return result\n",">>>\n",">>>     @staticmethod\n",">>>     def backward(ctx, grad_output):\n",">>>         result, = ctx.saved_tensors\n",">>>         return grad_output * result\n",">>>\n",">>> #Use it by calling the apply method:\n",">>> output = Exp.apply(input)\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> class Exp(Function):\n",">>>\n",">>>     @staticmethod\n",">>>     def forward(ctx, i):\n",">>>         result = i.exp()\n",">>>         ctx.save_for_backward(result)\n",">>>         return result\n",">>>\n",">>>     @staticmethod\n",">>>     def backward(ctx, grad_output):\n",">>>         result, = ctx.saved_tensors\n",">>>         return grad_output * result\n",">>>\n",">>> #Use it by calling the apply method:\n",">>> output = Exp.apply(input)\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> class Exp(Function):\n",">>>\n",">>>     @staticmethod\n",">>>     def forward(ctx, i):\n",">>>         result = i.exp()\n",">>>         ctx.save_for_backward(result)\n",">>>         return result\n",">>>\n",">>>     @staticmethod\n",">>>     def backward(ctx, grad_output):\n",">>>         result, = ctx.saved_tensors\n",">>>         return grad_output * result\n",">>>\n",">>> #Use it by calling the apply method:\n",">>> output = Exp.apply(input)\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> class Exp(Function):\n",">>>\n",">>>     @staticmethod\n",">>>     def forward(ctx, i):\n",">>>         result = i.exp()\n",">>>         ctx.save_for_backward(result)\n",">>>         return result\n",">>>\n",">>>     @staticmethod\n",">>>     def backward(ctx, grad_output):\n",">>>         result, = ctx.saved_tensors\n",">>>         return grad_output * result\n",">>>\n",">>> #Use it by calling the apply method:\n",">>> output = Exp.apply(input)\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> class Exp(Function):\n",">>>\n",">>>     @staticmethod\n",">>>     def forward(ctx, i):\n",">>>         result = i.exp()\n",">>>         ctx.save_for_backward(result)\n",">>>         return result\n",">>>\n",">>>     @staticmethod\n",">>>     def backward(ctx, grad_output):\n",">>>         result, = ctx.saved_tensors\n",">>>         return grad_output * result\n",">>>\n",">>> #Use it by calling the apply method:\n",">>> output = Exp.apply(input)\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> class Exp(Function):\n",">>>\n",">>>     @staticmethod\n",">>>     def forward(ctx, i):\n",">>>         result = i.exp()\n",">>>         ctx.save_for_backward(result)\n",">>>         return result\n",">>>\n",">>>     @staticmethod\n",">>>     def backward(ctx, grad_output):\n",">>>         result, = ctx.saved_tensors\n",">>>         return grad_output * result\n",">>>\n",">>> #Use it by calling the apply method:\n",">>> output = Exp.apply(input)\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> class Exp(Function):\n",">>>\n",">>>     @staticmethod\n",">>>     def forward(ctx, i):\n",">>>         result = i.exp()\n",">>>         ctx.save_for_backward(result)\n",">>>         return result\n",">>>\n",">>>     @staticmethod\n",">>>     def backward(ctx, grad_output):\n",">>>         result, = ctx.saved_tensors\n",">>>         return grad_output * result\n",">>>\n",">>> #Use it by calling the apply method:\n",">>> output = Exp.apply(input)\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> x = torch.randn((1, 1), requires_grad=True)\n",">>> with torch.autograd.profiler.profile() as prof:\n",">>>     for _ in range(100):  # any normal python code, really!\n",">>>         y = x ** 2\n",">>          y.backward()\n",">>> # NOTE: some columns were removed for brevity\n",">>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","-----------------------------------  ---------------  ---------------  ---------------\n","Name                                 Self CPU total   CPU time avg     Number of Calls\n","-----------------------------------  ---------------  ---------------  ---------------\n","mul                                  32.048ms         32.048ms         200\n","pow                                  27.041ms         27.041ms         200\n","PowBackward0                         9.727ms          55.483ms         100\n","torch::autograd::AccumulateGrad      9.148ms          9.148ms          100\n","torch::autograd::GraphRoot           691.816us        691.816us        100\n","-----------------------------------  ---------------  ---------------  ---------------\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n",">>> import torch\n",">>> from torch import autograd\n",">>> class MyFunc(autograd.Function):\n","...     @staticmethod\n","...     def forward(ctx, inp):\n","...         return inp.clone()\n","...     @staticmethod\n","...     def backward(ctx, gO):\n","...         # Error during the backward pass\n","...         raise RuntimeError(\"Some error in backward\")\n","...         return gO.clone()\n",">>> def run_fn(a):\n","...     out = MyFunc.apply(a)\n","...     return out.sum()\n",">>> inp = torch.rand(10, 10, requires_grad=True)\n",">>> out = run_fn(inp)\n",">>> out.backward()\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 1, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n",">>> with autograd.detect_anomaly():\n","...     inp = torch.rand(10, 10, requires_grad=True)\n","...     out = run_fn(inp)\n","...     out.backward()\n","    Traceback of forward call that caused the error:\n","      File \"tmp.py\", line 53, in <module>\n","        out = run_fn(inp)\n","      File \"tmp.py\", line 44, in run_fn\n","        out = MyFunc.apply(a)\n","    Traceback (most recent call last):\n","      File \"<stdin>\", line 4, in <module>\n","      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n","        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n","        allow_unreachable=True)  # allow_unreachable flag\n","      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n","        return self._forward_cls.backward(self, *args)\n","      File \"<stdin>\", line 8, in backward\n","    RuntimeError: Some error in backward\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bTpfPIe3RUQ4"},"source":["### testbed"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SzyAqAGG4UrZ","executionInfo":{"status":"ok","timestamp":1628696483421,"user_tz":-330,"elapsed":1686,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}},"outputId":"e64aa557-c5ca-4f48-8bcb-d425dddcbf54"},"source":["testuniqdict = {}\n","qalisttest = []\n","for url in teamurllist[0:1]:\n","  url = 'https://pytorch.org/docs/stable/distributed.html'\n","  #  'https://pytorch.org/docs/stable/special.html#torch.special.expit'\n","  #  'https://pytorch.org/docs/stable/special.html#torch.special.expm1' \n","  #'https://pytorch.org/docs/stable/special.html'\n","  #'https://pytorch.org/docs/stable/special.html#torch.special.logit'\n","  \"\"\"9 How  , give an example?|https://pytorch.org/docs/stable/special.html#torch.special.exp2\n","  9 How  , give an example?|https://pytorch.org/docs/stable/jit.html\n","  9 How  , give an example?|https://pytorch.org/docs/stable/special.html#torch.special.erf\n","  9 How  , give an example?|https://pytorch.org/docs/stable/special.html#torch.special.logit\n","  4 How  , give an example?|https://pytorch.org/docs/stable/distributed.html\n","  9 How  , give an example?|https://pytorch.org/docs/stable/special.html\n","  9 How  , give an example?|https://pytorch.org/docs/stable/special.html#torch.special.expm1\n","  9 How  , give an example?|https://pytorch.org/docs/stable/special.html#torch.special.erfc\n","  9 How  , give an example?|https://pytorch.org/docs/stable/special.html#torch.special.expit\"\"\"\n","  #'https://pytorch.org/docs/stable/special.html#torch.special.erf'\n","  #'https://pytorch.org/docs/stable/special.html#torch.special.exp2'\n","  # 'https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc' \n","  #'https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc'\n","  # 'https://pytorch.org/docs/stable/special.html#torch.special.exp2'\n","  #  'https://pytorch.org/docs/stable/optim.html'\n","  #'https://pytorch.org/docs/stable/fx.html' \n","  #'https://pytorch.org/docs/stable/distributions.html'\n","  # 'https://pytorch.org/docs/stable/tensorboard.html'\n","  #'https://pytorch.org/docs/stable/special.html#torch.special.erfinv'\n","  # 'https://pytorch.org/docs/stable/autograd.html'\n","  # 'https://pytorch.org/docs/stable/generated/torch.load.html#torch.load'\n","  #print(url)\n","  try:\n","    html = req.urlopen(url).read()\n","  except:\n","    continue\n","  soup = BeautifulSoup(html)\n","\n","\n","  pdict = {}\n","\n","  \n","  divs = soup.find_all('div' ,  class_=\"highlight\") #class_=\"section\") #\n","  \n","\n","  if divs:\n","\n","    for div in divs:\n","      \n","      \n","      \n","      sctn = div.find_parent('div', class_=\"section\")\n","\n","      if sctn:\n","\n","        whichhdr = False\n","        hdrtxt = \"\"\n","        #print('1')\n","        try:\n","          #print('2')\n","          hdr = sctn.find('h1')\n","        \n","          hdrtxt = hdr.get_text().strip()\n","          hdrtxt = re.sub(r\"[^a-zA-Z0-9]+\", ' ', hdrtxt).strip()\n","          #qstnsubject = qstnsubject + hdrtxt + '?'\n","          #print('2a')\n","        except:\n","          try:\n","            #print('3')\n","            hdr = sctn.find('h2')\n","            hdrtxt = hdr.get_text().strip()\n","            hdrtxt = re.sub(r\"[^a-zA-Z0-9]+\", ' ', hdrtxt).strip()\n","            #qstnsubject = qstnsubject + hdrtxt + '?'\n","            #print('3a')\n","          except:\n","            try:\n","              #print('4')\n","              hdr = sctn.find('h3')\n","              hdrtxt = hdr.get_text().strip()\n","              hdrtxt = re.sub(r\"[^a-zA-Z0-9]+\", ' ', hdrtxt).strip()\n","              #qstnsubject = qstnsubject + hdrtxt + '?'\n","              #print('4a')\n","            except:\n","              #print('5')\n","              1 == 1\n","            \n","\n","        #print(div.get('class')[0])\n","      \n","        #if hglts:\n","\n","\n","\n","\n","        #for hglt in hglts:\n","        if 1 == 1:\n","\n","          #qstnsubject = 'How to use ' + hdrtxt + '?'\n","\n","          prnt = div.find_parent('div',  {'class':['highlight-default', 'highlight-python']})\n","\n","          ctxt = \" \"\n","          funcname = \"\"\n","          \n","          if prnt:\n","            #print('before dts')\n","            dl = prnt.find_parent('dl' ,class_=\"function\")\n","            dlm = prnt.find_parent('dl' ,class_=\"method\")\n","            dlc = prnt.find_parent('dl' ,class_=\"class\")\n","            dv = prnt.find_parent('div', {'class':['admonition note', 'section']})\n","            \n","            dvsbp = prnt.find_previous('div', class_=\"admonition warning\")\n","\n","            if dl:\n","              #print('in dl')\n","              dt = dl.find_next('dt')\n","\n","              if dt:\n","                funcname = dt.get('id')\n","                #print('dl dt ',dt.get('id'))\n","            elif dlm:\n","              #print('in dl')\n","              dt = dlm.find_next('dt')\n","\n","              if dt:\n","                funcname = dt.get('id')\n","                #print('dlmdt ',dt.get('id'))\n","              elif prnt.find_previous_sibling('p') :\n","\n","                pc = prnt.find_previous_sibling('p')\n","\n","                if pc:\n","                  funcname = pc.get_text(strip = True)\n","            elif dlc:\n","              #print('in dlc')\n","              pc = prnt.find_previous_sibling('p')\n","\n","              if pc:\n","                funcname = pc.get_text(strip = True)\n","\n","                if re.match(r'Example',funcname ):\n","                  dlct = dlc.find_next('dt')\n","                  funcname = dlct.get('id')\n","    \n","                #print('dlcdt ',funcname)\n","            elif dv :\n","              print('dv')\n","              #pc = prnt.find_previous_sibling('p')\n","              dpps = prnt.find_previous_siblings('p')\n","\n","              \"\"\"if pc:\n","                funcname = pc.get_text(strip = True)\n","              print(funcname)\"\"\"\n","\n","              dvsb = prnt.find_previous_sibling('div', class_=\"admonition warning\")\n","\n","              if dvsb :\n","                print('dvsb')\n","\n","                psbs = dvsb.find_all_next('p')\n","\n","                if psbs:\n","                  print('psb')\n","                  \n","                  scope = 1\n","                  plen = len(dpps)\n","                  #print('p len ', plen)\n","                  if  plen >= 2:\n","                    scope = 2\n","  \n","                  \n","                  for psb in psbs[:scope]:\n","                    funcname = funcname + psb.get_text(strip=True)\n","                  print('*'*30, '\\n','warning' , funcname , '\\n' , '*'*24)\n","\n","\n","              elif dpps:\n","                scope = 1\n","                plen = len(dpps)\n","                #print('p len ', plen)\n","                if  plen >= 2:\n","                  scope = 2\n","            \n","                for pp in reversed(dpps[:scope]):\n","                  content = pp.get_text(strip=True)\n","                  if not re.search(r'Example',content):\n","                    funcname = funcname + content\n","                #print('from dpps ' , funcname)\n","\n","                if len(funcname) == 0:\n","                  dlcs = prnt.find_previous_sibling('dl' ,  class_='simple')\n","\n","                  if dlcs:\n","\n","                    dtcs = dlcs.find_next('dt')\n","                    funcname = dtcs.get_text(strip=True)\n","\n","                  #print('dpps fnc ',funcname)\n","\n","            elif dvsbp :\n","              print('dvsbp')\n","\n","            #print('7')\n","            \"\"\"pps = prnt.find_previous_siblings('p')\n","\n","            pp1 = prnt.find_previous_sibling('p')\n","\n","            if pp1:\n","              print('pp1' , pp1.get_text())\n","\n","              pp2 = pp1.find_previous_sibling('p')\n","\n","              if pp2:\n","                print('pp2' , pp2.get_text())\"\"\"\n","\n","\n","\n","            if pps:\n","              scope = 1\n","              plen = len(pps)\n","              #print('p len ', plen)\n","              if plen >= 2:\n","                scope = 2\n","            \n","              \"\"\"m = 0\n","              for pp in pps:\n","                print(m , ' p text' , pp.get_text(strip=True))\n","                m += 1\"\"\"\n","            \n","              for pp in reversed(pps[:scope]):\n","                content = pp.get_text(strip=True)\n","                if not re.search(r'Example',content):\n","                  ctxt = ctxt + content\n","\n","                if re.search(r'Note',ctxt):\n","                  ctxt = re.sub(r'Note','',ctxt)\n","              #print('from p ' , ctxt)\n","\n","          localctxt = ctxt\n","\n","          if len(funcname) > 0:\n","            ctxt = funcname\n","          elif len(hdrtxt) > 0 : \n","            ctxt = hdrtxt\n","\n","          if len(ctxt) < 0:\n","            print('url' , url)\n","          qstnsubject = 'How to use ' + ctxt +  ', give an example?'\n","\n","          qadict = {}\n","          try:\n","\n","            pr = div.find('pre').text\n","\n","\n","            qadict['Answer'] = pr\n","\n","            if len(qadict['Answer']) > 0: \n","              # and len(qadict['context'])\n","              idn += 1\n","\n","              if qstnsubject in testuniqdict:\n","                qstnsubject = 'How ' + localctxt +  ', give an example?'\n","                \n","              testuniqdict[qstnsubject] = url\n","\n","              qadict['Question'] = qstnsubject\n","              \n","              #print('6 QA - ',qstnsubject)\n","\n","              qadict['Id'] = idn\n","              qadict['source'] = url\n","              qadict['context'] = localctxt\n","              qalisttest.append(qadict)\n","          except:\n","            print('no hglts')\n","            continue\n"],"execution_count":136,"outputs":[{"output_type":"stream","text":["dv\n","dv\n","dvsb\n","psb\n","****************************** \n"," warning WarningThis method will always create the file and try its best to clean up and remove\n","the file at the end of the program. In other words, each initialization with\n","the file init method will need a brand new empty file in order for the initialization\n","to succeed. If the same file used by the previous initialization (which happens not\n","to get cleaned up) is used again, this is unexpected behavior and can often cause\n","deadlocks and failures. Therefore, even though this method will try its best to clean up\n","the file, if the auto-delete happens to be unsuccessful, it is your responsibility\n","to ensure that the file is removed at the end of the training to prevent the same\n","file to be reused again during the next time. This is especially important\n","if you plan to callinit_process_group()multiple times on the same file name.\n","In other words, if the file is not removed/cleaned up and you callinit_process_group()again on that file, failures are expected.\n","The rule of thumb here is that, make sure that the file is non-existent or\n","empty every timeinit_process_group()is called. \n"," ************************\n","dv\n","dv\n","dv\n","dv\n","dv\n","dv\n","dv\n","dv\n","dv\n","dv\n","dv\n","dv\n","dv\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9McsUnvtIoJB"},"source":["### test result"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lb_5-KOK4UgK","executionInfo":{"status":"ok","timestamp":1628696483422,"user_tz":-330,"elapsed":42,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}},"outputId":"fff96e3b-f57b-41d8-a17b-06e4b1a14068"},"source":["qalisttest , len(qalisttest)"],"execution_count":137,"outputs":[{"output_type":"execute_result","data":{"text/plain":["([{'Answer': \"import torch.distributed as dist\\n\\n# Use address of one of the machines\\ndist.init_process_group(backend, init_method='tcp://10.1.1.20:23456',\\n                        rank=args.rank, world_size=4)\\n\",\n","   'Id': 1451,\n","   'Question': 'How to use There are two ways to initialize using TCP, both requiring a network address\\nreachable from all processes and a desiredworld_size. The first way\\nrequires specifying an address that belongs to the rank 0 process. This\\ninitialization method requires that all processes have manually specified ranks.Note that multicast address is not supported anymore in the latest distributed\\npackage.group_nameis deprecated as well., give an example?',\n","   'context': ' 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': \"import torch.distributed as dist\\n\\n# rank should always be specified\\ndist.init_process_group(backend, init_method='file:///mnt/nfs/sharedfile',\\n                        world_size=4, rank=args.rank)\\n\",\n","   'Id': 1452,\n","   'Question': 'How to use WarningThis method will always create the file and try its best to clean up and remove\\nthe file at the end of the program. In other words, each initialization with\\nthe file init method will need a brand new empty file in order for the initialization\\nto succeed. If the same file used by the previous initialization (which happens not\\nto get cleaned up) is used again, this is unexpected behavior and can often cause\\ndeadlocks and failures. Therefore, even though this method will try its best to clean up\\nthe file, if the auto-delete happens to be unsuccessful, it is your responsibility\\nto ensure that the file is removed at the end of the training to prevent the same\\nfile to be reused again during the next time. This is especially important\\nif you plan to callinit_process_group()multiple times on the same file name.\\nIn other words, if the file is not removed/cleaned up and you callinit_process_group()again on that file, failures are expected.\\nThe rule of thumb here is that, make sure that the file is non-existent or\\nempty every timeinit_process_group()is called., give an example?',\n","   'context': ' 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> import torch.distributed as dist\\n>>> from datetime import timedelta\\n>>> # Run on process 1 (server)\\n>>> server_store = dist.TCPStore(\"127.0.0.1\", 1234, 2, True, timedelta(seconds=30))\\n>>> # Run on process 2 (client)\\n>>> client_store = dist.TCPStore(\"127.0.0.1\", 1234, 2, False)\\n>>> # Use any of the store methods from either the client or server after initialization\\n>>> server_store.set(\"first_key\", \"first_value\")\\n>>> client_store.get(\"first_key\")\\n',\n","   'Id': 1453,\n","   'Question': 'How to use Distributed Key Value Store, give an example?',\n","   'context': ' 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> import torch.distributed as dist\\n>>> store = dist.HashStore()\\n>>> # store can be used from other threads\\n>>> # Use any of the store methods after initialization\\n>>> store.set(\"first_key\", \"first_value\")\\n',\n","   'Id': 1454,\n","   'Question': 'How  3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it., give an example?',\n","   'context': ' 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> import torch.distributed as dist\\n>>> store1 = dist.FileStore(\"/tmp/filestore\", 2)\\n>>> store2 = dist.FileStore(\"/tmp/filestore\", 2)\\n>>> # Use any of the store methods from either the client or server after initialization\\n>>> store1.set(\"first_key\", \"first_value\")\\n>>> store2.get(\"first_key\")\\n',\n","   'Id': 1455,\n","   'Question': 'How  3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it., give an example?',\n","   'context': ' 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> import torch.distributed as dist\\n>>> from datetime import timedelta\\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\\n>>> store.set(\"first_key\", \"first_value\")\\n>>> # Should return \"first_value\"\\n>>> store.get(\"first_key\")\\n',\n","   'Id': 1456,\n","   'Question': 'How to use torch.distributed.Store.set, give an example?',\n","   'context': ' 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> import torch.distributed as dist\\n>>> from datetime import timedelta\\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\\n>>> store.set(\"first_key\", \"first_value\")\\n>>> # Should return \"first_value\"\\n>>> store.get(\"first_key\")\\n',\n","   'Id': 1457,\n","   'Question': 'How to use torch.distributed.Store.get, give an example?',\n","   'context': ' 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> import torch.distributed as dist\\n>>> from datetime import timedelta\\n>>> # Using TCPStore as an example, other store types can also be used\\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\\n>>> store.add(\"first_key\", 1)\\n>>> store.add(\"first_key\", 6)\\n>>> # Should return 7\\n>>> store.get(\"first_key\")\\n',\n","   'Id': 1458,\n","   'Question': 'How to use torch.distributed.Store.add, give an example?',\n","   'context': ' 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> import torch.distributed as dist\\n>>> from datetime import timedelta\\n>>> # Using TCPStore as an example, other store types can also be used\\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\\n>>> # This will throw an exception after 30 seconds\\n>>> store.wait([\"bad_key\"])\\n',\n","   'Id': 1459,\n","   'Question': 'How to use torch.distributed.Store.wait, give an example?',\n","   'context': ' 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> import torch.distributed as dist\\n>>> from datetime import timedelta\\n>>> # Using TCPStore as an example, other store types can also be used\\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\\n>>> # This will throw an exception after 10 seconds\\n>>> store.wait([\"bad_key\"], timedelta(seconds=10))\\n',\n","   'Id': 1460,\n","   'Question': 'How  3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it., give an example?',\n","   'context': ' 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> import torch.distributed as dist\\n>>> from datetime import timedelta\\n>>> # Using TCPStore as an example, other store types can also be used\\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\\n>>> store.set(\"first_key\", \"first_value\")\\n>>> # This should return 2\\n>>> store.num_keys()\\n',\n","   'Id': 1461,\n","   'Question': 'How to use torch.distributed.Store.num_keys, give an example?',\n","   'context': ' 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> import torch.distributed as dist\\n>>> from datetime import timedelta\\n>>> # Using TCPStore as an example, HashStore can also be used\\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\\n>>> store.set(\"first_key\")\\n>>> # This should return true\\n>>> store.delete_key(\"first_key\")\\n>>> # This should return false\\n>>> store.delete_key(\"bad_key\")\\n',\n","   'Id': 1462,\n","   'Question': 'How to use torch.distributed.Store.delete_key, give an example?',\n","   'context': ' 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> import torch.distributed as dist\\n>>> from datetime import timedelta\\n>>> # Using TCPStore as an example, other store types can also be used\\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\\n>>> store.set_timeout(timedelta(seconds=10))\\n>>> # This will throw an exception after 10 seconds\\n>>> store.wait([\"bad_key\"])\\n',\n","   'Id': 1463,\n","   'Question': 'How to use torch.distributed.Store.set_timeout, give an example?',\n","   'context': ' 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '# Code runs on each rank.\\ndist.init_process_group(\"nccl\", rank=rank, world_size=2)\\noutput = torch.tensor([rank]).cuda(rank)\\ns = torch.cuda.Stream()\\nhandle = dist.all_reduce(output, async_op=True)\\n# Wait ensures the operation is enqueued, but not necessarily complete.\\nhandle.wait()\\n# Using result on non-default stream.\\nwith torch.cuda.stream(s):\\n    s.wait_stream(torch.cuda.default_stream())\\n    output.add_(100)\\nif rank == 0:\\n    # if the explicit call to wait_stream was omitted, the output below will be\\n    # non-deterministically 1 or 101, depending on whether the allreduce overwrote\\n    # the value after the add completed.\\n    print(output)\\n',\n","   'Id': 1464,\n","   'Question': 'How to use The following code can serve as a reference regarding semantics for CUDA operations when using distributed collectives.\\nIt shows the explicit need to synchronize when using collective outputs on different CUDA streams:, give an example?',\n","   'context': ' 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> # Note: Process group initialization omitted on each rank.\\n>>> import torch.distributed as dist\\n>>> if dist.get_rank() == 0:\\n>>>     # Assumes world_size of 3.\\n>>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\\n>>> else:\\n>>>     objects = [None, None, None]\\n>>> dist.broadcast_object_list(objects, src=0)\\n>>> broadcast_objects\\n[\\'foo\\', 12, {1: 2}]\\n',\n","   'Id': 1465,\n","   'Question': 'How to use torch.distributed.broadcast_object_list, give an example?',\n","   'context': ' 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> # All tensors below are of torch.int64 type.\\n>>> # We have 2 process groups, 2 ranks.\\n>>> tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank\\n>>> tensor\\ntensor([1, 2]) # Rank 0\\ntensor([3, 4]) # Rank 1\\n>>> dist.all_reduce(tensor, op=ReduceOp.SUM)\\n>>> tensor\\ntensor([4, 6]) # Rank 0\\ntensor([4, 6]) # Rank 1\\n',\n","   'Id': 1466,\n","   'Question': 'How to use torch.distributed.all_reduce, give an example?',\n","   'context': ' 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> # All tensors below are of torch.cfloat type.\\n>>> # We have 2 process groups, 2 ranks.\\n>>> tensor = torch.tensor([1+1j, 2+2j], dtype=torch.cfloat) + 2 * rank * (1+1j)\\n>>> tensor\\ntensor([1.+1.j, 2.+2.j]) # Rank 0\\ntensor([3.+3.j, 4.+4.j]) # Rank 1\\n>>> dist.all_reduce(tensor, op=ReduceOp.SUM)\\n>>> tensor\\ntensor([4.+4.j, 6.+6.j]) # Rank 0\\ntensor([4.+4.j, 6.+6.j]) # Rank 1\\n',\n","   'Id': 1467,\n","   'Question': 'How  3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it., give an example?',\n","   'context': ' 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> # All tensors below are of torch.int64 dtype.\\n>>> # We have 2 process groups, 2 ranks.\\n>>> tensor_list = [torch.zero(2, dtype=torch.int64) for _ in range(2)]\\n>>> tensor_list\\n[tensor([0, 0]), tensor([0, 0])] # Rank 0 and 1\\n>>> tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank\\n>>> tensor\\ntensor([1, 2]) # Rank 0\\ntensor([3, 4]) # Rank 1\\n>>> dist.all_gather(tensor_list, tensor)\\n>>> tensor_list\\n[tensor([1, 2]), tensor([3, 4])] # Rank 0\\n[tensor([1, 2]), tensor([3, 4])] # Rank 1\\n',\n","   'Id': 1468,\n","   'Question': 'How to use torch.distributed.all_gather, give an example?',\n","   'context': ' 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> # All tensors below are of torch.cfloat dtype.\\n>>> # We have 2 process groups, 2 ranks.\\n>>> tensor_list = [torch.zero(2, dtype=torch.cfloat) for _ in range(2)]\\n>>> tensor_list\\n[tensor([0.+0.j, 0.+0.j]), tensor([0.+0.j, 0.+0.j])] # Rank 0 and 1\\n>>> tensor = torch.tensor([1+1j, 2+2j], dtype=torch.cfloat) + 2 * rank * (1+1j)\\n>>> tensor\\ntensor([1.+1.j, 2.+2.j]) # Rank 0\\ntensor([3.+3.j, 4.+4.j]) # Rank 1\\n>>> dist.all_gather(tensor_list, tensor)\\n>>> tensor_list\\n[tensor([1.+1.j, 2.+2.j]), tensor([3.+3.j, 4.+4.j])] # Rank 0\\n[tensor([1.+1.j, 2.+2.j]), tensor([3.+3.j, 4.+4.j])] # Rank 1\\n',\n","   'Id': 1469,\n","   'Question': 'How  3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it., give an example?',\n","   'context': ' 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> # Note: Process group initialization omitted on each rank.\\n>>> import torch.distributed as dist\\n>>> # Assumes world_size of 3.\\n>>> gather_objects = [\"foo\", 12, {1: 2}] # any picklable object\\n>>> output = [None for _ in gather_objects]\\n>>> dist.all_gather_object(output, gather_objects[dist.get_rank()])\\n>>> output\\n[\\'foo\\', 12, {1: 2}]\\n',\n","   'Id': 1470,\n","   'Question': 'How to use torch.distributed.all_gather_object, give an example?',\n","   'context': ' 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> # Note: Process group initialization omitted on each rank.\\n>>> import torch.distributed as dist\\n>>> # Assumes world_size of 3.\\n>>> gather_objects = [\"foo\", 12, {1: 2}] # any picklable object\\n>>> output = [None for _ in gather_objects]\\n>>> dist.gather_object(\\n        gather_objects[dist.get_rank()],\\n        output if dist.get_rank() == 0 else None,\\n        dst=0\\n    )\\n>>> # On rank 0\\n>>> output\\n[\\'foo\\', 12, {1: 2}]\\n',\n","   'Id': 1471,\n","   'Question': 'How to use torch.distributed.gather_object, give an example?',\n","   'context': ' 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> # Note: Process group initialization omitted on each rank.\\n>>> import torch.distributed as dist\\n>>> if dist.get_rank() == 0:\\n>>>     # Assumes world_size of 3.\\n>>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\\n>>> else:\\n>>>     # Can be any list on non-src ranks, elements are not used.\\n>>>     objects = [None, None, None]\\n>>> output_list = [None]\\n>>> dist.scatter_object_list(output_list, objects, src=0)\\n>>> # Rank i gets objects[i]. For example, on rank 2:\\n>>> output_list\\n[{1: 2}]\\n',\n","   'Id': 1472,\n","   'Question': 'How to use torch.distributed.scatter_object_list, give an example?',\n","   'context': ' 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> input = torch.arange(4) + rank * 4\\n>>> input = list(input.chunk(4))\\n>>> input\\n[tensor([0]), tensor([1]), tensor([2]), tensor([3])]     # Rank 0\\n[tensor([4]), tensor([5]), tensor([6]), tensor([7])]     # Rank 1\\n[tensor([8]), tensor([9]), tensor([10]), tensor([11])]   # Rank 2\\n[tensor([12]), tensor([13]), tensor([14]), tensor([15])] # Rank 3\\n>>> output = list(torch.empty([4], dtype=torch.int64).chunk(4))\\n>>> dist.all_to_all(output, input)\\n>>> output\\n[tensor([0]), tensor([4]), tensor([8]), tensor([12])]    # Rank 0\\n[tensor([1]), tensor([5]), tensor([9]), tensor([13])]    # Rank 1\\n[tensor([2]), tensor([6]), tensor([10]), tensor([14])]   # Rank 2\\n[tensor([3]), tensor([7]), tensor([11]), tensor([15])]   # Rank 3\\n',\n","   'Id': 1473,\n","   'Question': 'How to use torch.distributed.all_to_all, give an example?',\n","   'context': ' 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> # Essentially, it is similar to following operation:\\n>>> scatter_list = input\\n>>> gather_list  = output\\n>>> for i in range(world_size):\\n>>>   dist.scatter(gather_list[i], scatter_list if i == rank else [], src = i)\\n',\n","   'Id': 1474,\n","   'Question': 'How  3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it., give an example?',\n","   'context': ' 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> input\\ntensor([0, 1, 2, 3, 4, 5])                                       # Rank 0\\ntensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1\\ntensor([20, 21, 22, 23, 24])                                     # Rank 2\\ntensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3\\n>>> input_splits\\n[2, 2, 1, 1]                                                     # Rank 0\\n[3, 2, 2, 2]                                                     # Rank 1\\n[2, 1, 1, 1]                                                     # Rank 2\\n[2, 2, 2, 1]                                                     # Rank 3\\n>>> output_splits\\n[2, 3, 2, 2]                                                     # Rank 0\\n[2, 2, 1, 2]                                                     # Rank 1\\n[1, 2, 1, 2]                                                     # Rank 2\\n[1, 2, 1, 1]                                                     # Rank 3\\n>>> input = list(input.split(input_splits))\\n>>> input\\n[tensor([0, 1]), tensor([2, 3]), tensor([4]), tensor([5])]                   # Rank 0\\n[tensor([10, 11, 12]), tensor([13, 14]), tensor([15, 16]), tensor([17, 18])] # Rank 1\\n[tensor([20, 21]), tensor([22]), tensor([23]), tensor([24])]                 # Rank 2\\n[tensor([30, 31]), tensor([32, 33]), tensor([34, 35]), tensor([36])]         # Rank 3\\n>>> output = ...\\n>>> dist.all_to_all(output, input)\\n>>> output\\n[tensor([0, 1]), tensor([10, 11, 12]), tensor([20, 21]), tensor([30, 31])]   # Rank 0\\n[tensor([2, 3]), tensor([13, 14]), tensor([22]), tensor([32, 33])]           # Rank 1\\n[tensor([4]), tensor([15, 16]), tensor([23]), tensor([34, 35])]              # Rank 2\\n[tensor([5]), tensor([17, 18]), tensor([24]), tensor([36])]                  # Rank 3\\n',\n","   'Id': 1475,\n","   'Question': 'How  3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it., give an example?',\n","   'context': ' 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': 'import torch\\nimport torch.distributed as dist\\nwith torch.profiler():\\n    tensor = torch.randn(20, 10)\\n    dist.all_reduce(tensor)\\n',\n","   'Id': 1476,\n","   'Question': 'How to use Note that you can usetorch.profiler(recommended, only available after 1.8.1)  ortorch.autograd.profilerto profile collective communication and point-to-point communication APIs mentioned here. All out-of-the-box backends (gloo,nccl,mpi) are supported and collective communication usage will be rendered as expected in profiling output/traces. Profiling your code is the same as any regular torch operator:, give an example?',\n","   'context': ' 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': 'import torch\\nimport torch.distributed as dist\\n\\ndist.init_process_group(backend=\"nccl\",\\n                        init_method=\"file:///distributed_test\",\\n                        world_size=2,\\n                        rank=0)\\ntensor_list = []\\nfor dev_idx in range(torch.cuda.device_count()):\\n    tensor_list.append(torch.FloatTensor([1]).cuda(dev_idx))\\n\\ndist.all_reduce_multigpu(tensor_list)\\n',\n","   'Id': 1477,\n","   'Question': 'How to use For example, if the system we use for distributed training has 2 nodes, each\\nof which has 8 GPUs. On each of the 16 GPUs, there is a tensor that we would\\nlike to all-reduce. The following code can serve as a reference:Code running on Node 0, give an example?',\n","   'context': ' 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': 'import torch\\nimport torch.distributed as dist\\n\\ndist.init_process_group(backend=\"nccl\",\\n                        init_method=\"file:///distributed_test\",\\n                        world_size=2,\\n                        rank=1)\\ntensor_list = []\\nfor dev_idx in range(torch.cuda.device_count()):\\n    tensor_list.append(torch.FloatTensor([1]).cuda(dev_idx))\\n\\ndist.all_reduce_multigpu(tensor_list)\\n',\n","   'Id': 1478,\n","   'Question': 'How to use Code running on Node 0Code running on Node 1, give an example?',\n","   'context': ' 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\\n           YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other\\n           arguments of your training script)\\n',\n","   'Id': 1479,\n","   'Question': 'How to use In both cases of single-node distributed training or multi-node distributed\\ntraining, this utility will launch the given number of processes per node\\n(--nproc_per_node). If used for GPU training, this number needs to be less\\nor equal to the number of GPUs on the current system (nproc_per_node),\\nand each process will be operating on a single GPU fromGPU 0 to\\nGPU (nproc_per_node - 1).How to use this module:, give an example?',\n","   'context': ' 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\\n           --nnodes=2 --node_rank=0 --master_addr=\"192.168.1.1\"\\n           --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3\\n           and all other arguments of your training script)\\n',\n","   'Id': 1480,\n","   'Question': 'How to use How to use this module:Node 1:(IP: 192.168.1.1, and has a free port: 1234), give an example?',\n","   'context': ' 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\\n           --nnodes=2 --node_rank=1 --master_addr=\"192.168.1.1\"\\n           --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3\\n           and all other arguments of your training script)\\n',\n","   'Id': 1481,\n","   'Question': 'How to use Node 1:(IP: 192.168.1.1, and has a free port: 1234)Node 2:, give an example?',\n","   'context': ' 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> python -m torch.distributed.launch --help\\n',\n","   'Id': 1482,\n","   'Question': 'How  3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it., give an example?',\n","   'context': ' 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> import argparse\\n>>> parser = argparse.ArgumentParser()\\n>>> parser.add_argument(\"--local_rank\", type=int)\\n>>> args = parser.parse_args()\\n',\n","   'Id': 1483,\n","   'Question': 'How to use 2. In your training program, you must parse the command-line argument:--local_rank=LOCAL_PROCESS_RANK, which will be provided by this module.\\nIf your training program uses GPUs, you should ensure that your code only\\nruns on the GPU device of LOCAL_PROCESS_RANK. This can be done by:Parsing the local_rank argument, give an example?',\n","   'context': ' 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> torch.cuda.set_device(args.local_rank)  # before your code runs\\n',\n","   'Id': 1484,\n","   'Question': 'How to use Parsing the local_rank argumentSet your device to local rank using either, give an example?',\n","   'context': ' 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': '>>> with torch.cuda.device(args.local_rank):\\n>>>    # your code to run\\n',\n","   'Id': 1485,\n","   'Question': 'How to use Set your device to local rank using eitheror, give an example?',\n","   'context': ' 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': \"torch.distributed.init_process_group(backend='YOUR BACKEND',\\n                                     init_method='env://')\\n\",\n","   'Id': 1486,\n","   'Question': 'How to use or3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module., give an example?',\n","   'context': ' 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'},\n","  {'Answer': 'model = torch.nn.parallel.DistributedDataParallel(model,\\n                                                  device_ids=[args.local_rank],\\n                                                  output_device=args.local_rank)\\n',\n","   'Id': 1487,\n","   'Question': 'How to use 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it., give an example?',\n","   'context': ' 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it.',\n","   'source': 'https://pytorch.org/docs/stable/distributed.html'}],\n"," 37)"]},"metadata":{"tags":[]},"execution_count":137}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6QZMd5-2x8u4","executionInfo":{"status":"ok","timestamp":1628696483422,"user_tz":-330,"elapsed":27,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}},"outputId":"fafdac6c-407a-41b8-9f0a-e259b2a821b2"},"source":["countdict1 = {}\n","uniqqdict1 = {}\n","for qa in qalisttest:\n","  print(qa['source'])\n","  if qa['Question'] in uniqqdict1:\n","    uniqqdict1[qa['Question']][0] = uniqqdict1[qa['Question']][0] + 1\n","  else :\n","    uniqqdict1[qa['Question']] = [ 1 , qa['source']]\n","    \n","  if qa['source'] in countdict1:\n","    countdict1[qa['source']] = countdict1[qa['source']] + 1\n","  else :\n","    countdict1[qa['source']] = 1\n","\n"],"execution_count":138,"outputs":[{"output_type":"stream","text":["https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n","https://pytorch.org/docs/stable/distributed.html\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mWnJNA_Nyg2W","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628696483423,"user_tz":-330,"elapsed":14,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}},"outputId":"2624818c-daa2-47f7-8f27-0f5423b5e5c4"},"source":["uniqqdict1 , len(uniqqdict1)"],"execution_count":139,"outputs":[{"output_type":"execute_result","data":{"text/plain":["({'How  3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it., give an example?': [8,\n","   'https://pytorch.org/docs/stable/distributed.html'],\n","  'How to use 2. In your training program, you must parse the command-line argument:--local_rank=LOCAL_PROCESS_RANK, which will be provided by this module.\\nIf your training program uses GPUs, you should ensure that your code only\\nruns on the GPU device of LOCAL_PROCESS_RANK. This can be done by:Parsing the local_rank argument, give an example?': [1,\n","   'https://pytorch.org/docs/stable/distributed.html'],\n","  'How to use 3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\\nhere is how to configure it., give an example?': [1,\n","   'https://pytorch.org/docs/stable/distributed.html'],\n","  'How to use Code running on Node 0Code running on Node 1, give an example?': [1,\n","   'https://pytorch.org/docs/stable/distributed.html'],\n","  'How to use Distributed Key Value Store, give an example?': [1,\n","   'https://pytorch.org/docs/stable/distributed.html'],\n","  'How to use For example, if the system we use for distributed training has 2 nodes, each\\nof which has 8 GPUs. On each of the 16 GPUs, there is a tensor that we would\\nlike to all-reduce. The following code can serve as a reference:Code running on Node 0, give an example?': [1,\n","   'https://pytorch.org/docs/stable/distributed.html'],\n","  'How to use How to use this module:Node 1:(IP: 192.168.1.1, and has a free port: 1234), give an example?': [1,\n","   'https://pytorch.org/docs/stable/distributed.html'],\n","  'How to use In both cases of single-node distributed training or multi-node distributed\\ntraining, this utility will launch the given number of processes per node\\n(--nproc_per_node). If used for GPU training, this number needs to be less\\nor equal to the number of GPUs on the current system (nproc_per_node),\\nand each process will be operating on a single GPU fromGPU 0 to\\nGPU (nproc_per_node - 1).How to use this module:, give an example?': [1,\n","   'https://pytorch.org/docs/stable/distributed.html'],\n","  'How to use Node 1:(IP: 192.168.1.1, and has a free port: 1234)Node 2:, give an example?': [1,\n","   'https://pytorch.org/docs/stable/distributed.html'],\n","  'How to use Note that you can usetorch.profiler(recommended, only available after 1.8.1)  ortorch.autograd.profilerto profile collective communication and point-to-point communication APIs mentioned here. All out-of-the-box backends (gloo,nccl,mpi) are supported and collective communication usage will be rendered as expected in profiling output/traces. Profiling your code is the same as any regular torch operator:, give an example?': [1,\n","   'https://pytorch.org/docs/stable/distributed.html'],\n","  'How to use Parsing the local_rank argumentSet your device to local rank using either, give an example?': [1,\n","   'https://pytorch.org/docs/stable/distributed.html'],\n","  'How to use Set your device to local rank using eitheror, give an example?': [1,\n","   'https://pytorch.org/docs/stable/distributed.html'],\n","  'How to use The following code can serve as a reference regarding semantics for CUDA operations when using distributed collectives.\\nIt shows the explicit need to synchronize when using collective outputs on different CUDA streams:, give an example?': [1,\n","   'https://pytorch.org/docs/stable/distributed.html'],\n","  'How to use There are two ways to initialize using TCP, both requiring a network address\\nreachable from all processes and a desiredworld_size. The first way\\nrequires specifying an address that belongs to the rank 0 process. This\\ninitialization method requires that all processes have manually specified ranks.Note that multicast address is not supported anymore in the latest distributed\\npackage.group_nameis deprecated as well., give an example?': [1,\n","   'https://pytorch.org/docs/stable/distributed.html'],\n","  'How to use WarningThis method will always create the file and try its best to clean up and remove\\nthe file at the end of the program. In other words, each initialization with\\nthe file init method will need a brand new empty file in order for the initialization\\nto succeed. If the same file used by the previous initialization (which happens not\\nto get cleaned up) is used again, this is unexpected behavior and can often cause\\ndeadlocks and failures. Therefore, even though this method will try its best to clean up\\nthe file, if the auto-delete happens to be unsuccessful, it is your responsibility\\nto ensure that the file is removed at the end of the training to prevent the same\\nfile to be reused again during the next time. This is especially important\\nif you plan to callinit_process_group()multiple times on the same file name.\\nIn other words, if the file is not removed/cleaned up and you callinit_process_group()again on that file, failures are expected.\\nThe rule of thumb here is that, make sure that the file is non-existent or\\nempty every timeinit_process_group()is called., give an example?': [1,\n","   'https://pytorch.org/docs/stable/distributed.html'],\n","  'How to use or3. In your training program, you are supposed to call the following function\\nat the beginning to start the distributed backend. You need to make sure that\\nthe init_method usesenv://, which is the only supportedinit_methodby this module., give an example?': [1,\n","   'https://pytorch.org/docs/stable/distributed.html'],\n","  'How to use torch.distributed.Store.add, give an example?': [1,\n","   'https://pytorch.org/docs/stable/distributed.html'],\n","  'How to use torch.distributed.Store.delete_key, give an example?': [1,\n","   'https://pytorch.org/docs/stable/distributed.html'],\n","  'How to use torch.distributed.Store.get, give an example?': [1,\n","   'https://pytorch.org/docs/stable/distributed.html'],\n","  'How to use torch.distributed.Store.num_keys, give an example?': [1,\n","   'https://pytorch.org/docs/stable/distributed.html'],\n","  'How to use torch.distributed.Store.set, give an example?': [1,\n","   'https://pytorch.org/docs/stable/distributed.html'],\n","  'How to use torch.distributed.Store.set_timeout, give an example?': [1,\n","   'https://pytorch.org/docs/stable/distributed.html'],\n","  'How to use torch.distributed.Store.wait, give an example?': [1,\n","   'https://pytorch.org/docs/stable/distributed.html'],\n","  'How to use torch.distributed.all_gather, give an example?': [1,\n","   'https://pytorch.org/docs/stable/distributed.html'],\n","  'How to use torch.distributed.all_gather_object, give an example?': [1,\n","   'https://pytorch.org/docs/stable/distributed.html'],\n","  'How to use torch.distributed.all_reduce, give an example?': [1,\n","   'https://pytorch.org/docs/stable/distributed.html'],\n","  'How to use torch.distributed.all_to_all, give an example?': [1,\n","   'https://pytorch.org/docs/stable/distributed.html'],\n","  'How to use torch.distributed.broadcast_object_list, give an example?': [1,\n","   'https://pytorch.org/docs/stable/distributed.html'],\n","  'How to use torch.distributed.gather_object, give an example?': [1,\n","   'https://pytorch.org/docs/stable/distributed.html'],\n","  'How to use torch.distributed.scatter_object_list, give an example?': [1,\n","   'https://pytorch.org/docs/stable/distributed.html']},\n"," 30)"]},"metadata":{"tags":[]},"execution_count":139}]},{"cell_type":"code","metadata":{"id":"zo6Q7dbjCNwE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628696483424,"user_tz":-330,"elapsed":10,"user":{"displayName":"Deepak H","photoUrl":"","userId":"14450656196594695398"}},"outputId":"c42b1124-7c53-48d3-e622-cf7283f0c4de"},"source":["for k, v in uniqqdict1.items():\n","  if v[0] > 1:\n","    print(v[0] , k , v[1])"],"execution_count":140,"outputs":[{"output_type":"stream","text":["8 How  3. In your training program, you are supposed to call the following function\n","at the beginning to start the distributed backend. You need to make sure that\n","the init_method usesenv://, which is the only supportedinit_methodby this module.4. In your training program, you can either use regular distributed functions\n","or usetorch.nn.parallel.DistributedDataParallel()module. If your\n","training program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\n","here is how to configure it., give an example? https://pytorch.org/docs/stable/distributed.html\n"],"name":"stdout"}]}]}